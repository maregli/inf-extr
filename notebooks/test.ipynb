{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flash_attn import flash_attn_qkvpacked_func, flash_attn_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "from src import paths\n",
    "\n",
    "from src.utils import get_default_pydantic_model, SideEffect, SideEffectList\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from bert_score import BERTScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = BERTScorer(model_type=os.path.join(paths.MODEL_PATH, \"medbert-512\"), num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [\"This is a test\"]*3\n",
    "prediction = [\"This is a train\", \"This is a trial\", \"This is a test\"]\n",
    "\n",
    "P, R, F1 = scorer.score(prediction, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P[torch.argmax(F1).item()].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(F1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer.score([\" \"], [\" \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "[\", \".join(tup) for tup in list(permutations(prediction))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "default_model = get_default_pydantic_model(\"side_effects\")\n",
    "model = BertModel.from_pretrained(os.path.join(paths.MODEL_PATH, \"medbert-512\")).to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(os.path.join(paths.MODEL_PATH, \"medbert-512\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_group(grouped_df: pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess a group of rows (grouped by obs index) from the dataframe. The preprocessing consists of:\n",
    "    - Fusing all the unique side effects per medication with \",\" (so only one row per medication is left)\n",
    "    - Splitting the medications by \"/\" if necessary\n",
    "\n",
    "    Args:\n",
    "        grouped_df (pd.DataFrame): A group of rows from the dataframe.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with the group preprocessed.\n",
    "    \"\"\"\n",
    "\n",
    "    df = []\n",
    "    \n",
    "    # group by medication name\n",
    "    for med, group in grouped_df.groupby(\"medication\"):\n",
    "       # Fuse all the unique side effects with \",\"\n",
    "        side_effects = group[\"side_effect\"].unique()\n",
    "        side_effects = \"[SEP]\".join(side_effects)\n",
    "        text = group[\"text\"].iloc[0]\n",
    "        original_text = group[\"original_text\"].iloc[0]\n",
    "        index = group[\"index\"].iloc[0]\n",
    "\n",
    "        new_row = {\"medication\": med, \"side_effect\": side_effects, \"text\": text, \"original_text\": original_text, \"index\": index, \"successful\": group[\"successful\"].iloc[0]}\n",
    "        # Append the new row to the dataframe\n",
    "        df.append(new_row)\n",
    "\n",
    "    return pd.DataFrame(df)\n",
    "\n",
    "def prepare_results(path: str)->pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Prepare the results from the model to be compared with the labels. The preprocessing consists of:\n",
    "    - Fixing the model answers, replacing them with the default model answer if they are invalid (in regards to JSON structure).\n",
    "    - Grouping the results by index and preprocessing the group. (See preprocess_group() function for more details.)\n",
    "    - Lowercasing and converting everything to string.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the results file.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with the results preprocessed.\n",
    "    \"\"\"\n",
    "    results = torch.load(path)\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    df[\"successful\"] = True\n",
    "\n",
    "    # Fix model_answers\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            # Does model answer have key \"side_effects\"?\n",
    "            answer = json.loads(row[\"model_answers\"])\n",
    "\n",
    "            # Is answer a valid SideEffectList?\n",
    "            SideEffectList(**answer)\n",
    "\n",
    "            # Is the element an empty list?\n",
    "            if len(answer[\"side_effects\"]) == 0:\n",
    "                raise ValueError(\"Empty list\")\n",
    "            \n",
    "            # Are all the elements valid SideEffect?\n",
    "            for med in answer[\"side_effects\"]:\n",
    "                SideEffect(**med)\n",
    "            \n",
    "            df.at[idx, \"model_answers\"] = json.dumps(answer)\n",
    "\n",
    "            \n",
    "        except:\n",
    "            df.at[idx, \"model_answers\"] = default_model.model_dump_json()\n",
    "            # print(f\"Error at index {idx}\")\n",
    "            df.at[idx, \"successful\"] = False\n",
    "    \n",
    "    dfs = []\n",
    "    for idx, row in df.iterrows():\n",
    "        row = row.to_dict()\n",
    "        answer = json.loads(row[\"model_answers\"])\n",
    "        medications = answer[\"side_effects\"]\n",
    "        for med in medications:\n",
    "            med[\"text\"] = row.get(\"text\", \"\")\n",
    "            med[\"original_text\"] = row.get(\"original_text\", \"\")\n",
    "            med[\"index\"] = row.get(\"index\", \"\")\n",
    "            med[\"successful\"] = row.get(\"successful\", \"\")\n",
    "            dfs.append((med))\n",
    "\n",
    "    res = pd.DataFrame(dfs)\n",
    "\n",
    "    res = res.sort_values(by=\"index\")\n",
    "\n",
    "    # Group by index and preprocess\n",
    "    _dfs = []\n",
    "\n",
    "    for index, group in res.groupby(\"index\"):\n",
    "        _dfs.append(preprocess_group(group))\n",
    "\n",
    "    res = pd.concat(_dfs)\n",
    "\n",
    "    # Convert everything to string and lowercase\n",
    "    res = res.map(lambda x: str(x).lower())\n",
    "\n",
    "    return res\n",
    "\n",
    "def prepare_labels(path: str)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare the labels to be compared with the results. The preprocessing consists of:\n",
    "    - Lowercasing and converting everything to string.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the labels file.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        pd.DataFrame: A dataframe with the labels preprocessed.\n",
    "    \"\"\"\n",
    "    labels = pd.read_excel(path) \n",
    "\n",
    "    # Convert everything to string and lowercase\n",
    "    labels = labels.map(lambda x: str(x).lower())\n",
    "\n",
    "    return labels\n",
    "\n",
    "def calculate_precision_recall_f1(ground_truth:list[str], predicted:list[str])->tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculate precision and recall from two lists of strings, allowing for partial string matches.\n",
    "    A string does not have to be exactly the same to be considered a match, it is enough if one string is a substring of the other.\n",
    "    This function is used to evaluate the medication names.\n",
    "\n",
    "    Args:\n",
    "        ground_truth (list[str]): List of ground truth medication names\n",
    "        predicted (list[str]): List of predicted medication names\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, float, float]: A tuple with the precision, recall and f1 score.\n",
    "    \"\"\"\n",
    "    # Convert lists to sets for easier comparison\n",
    "    ground_truth_set = set(ground_truth)\n",
    "    predicted_set = set(predicted)\n",
    "\n",
    "    # Initialize counters\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "\n",
    "    # Calculate True Positives and False Positives\n",
    "    for pred_med in predicted_set:\n",
    "        found_match = False\n",
    "        for truth_med in ground_truth_set:\n",
    "            if pred_med in truth_med or truth_med in pred_med:\n",
    "                true_positives += 1\n",
    "                found_match = True\n",
    "                break\n",
    "        if not found_match:\n",
    "            false_positives += 1\n",
    "\n",
    "    # Calculate False Negatives\n",
    "    false_negatives = len(ground_truth_set) - true_positives\n",
    "\n",
    "    # Calculate Precision\n",
    "    if true_positives + false_positives == 0:\n",
    "        precision = 0  # Handle division by zero\n",
    "    else:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "\n",
    "    # Calculate Recall\n",
    "    if true_positives + false_negatives == 0:\n",
    "        recall = 0  # Handle division by zero\n",
    "    else:\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    # Calculate F1 Score\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def calculate_bert_score(ground_truth:str, predicted:str, scorer: BERTScorer)->tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculate the BERT score for between all combinations of side effect concatenations and the ground truth.\n",
    "\n",
    "    Args:\n",
    "        ground_truth (str): The ground truth side effect.\n",
    "        predicted (str): The predicted side effect.\n",
    "        scorer (BERTScorer): The BERTScorer object to score the similarity between the side effects.\n",
    "    \"\"\"\n",
    "    predicted_se = predicted.split(\"[SEP]\")\n",
    "    combinations = [\", \".join(tup) for tup in list(permutations(predicted_se))]\n",
    "    ground_truth_multiple = [ground_truth]*len(combinations)\n",
    "\n",
    "    P, R, F1 = scorer.score(combinations, ground_truth_multiple)\n",
    "    max_id = torch.argmax(F1).item()\n",
    "\n",
    "    return P[max_id].item(), R[max_id].item(), F1[max_id].item()\n",
    "\n",
    "def string_match(ground_truth:list[dict], predicted:list[dict], scorer: BERTScorer)->tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculate if the strings match (based on cosine similarity) from two lists of dictionaries.\n",
    "    \"\"\"\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "\n",
    "    remaining_gt = ground_truth.copy()\n",
    "    matched_gt_indices = set()\n",
    "\n",
    "    for pred in predicted:\n",
    "        for gt_index, gt in enumerate(remaining_gt):\n",
    "            if gt_index in matched_gt_indices:\n",
    "                # Skip already matched ground truth items\n",
    "                continue\n",
    "            \n",
    "            if pred[\"medication\"] in gt[\"medication\"] or gt[\"medication\"] in pred[\"medication\"]:\n",
    "                # Calculate BERT score for side effects\n",
    "                with open(paths.RESULTS_PATH/\"side-effects/similarity.txt\", \"a\") as f:\n",
    "                    f.write(json.dumps({\"gt\": gt[\"side_effect\"], \"pred\": pred[\"side_effect\"]}) + \"\\n\")\n",
    "                P, R, F1 = calculate_bert_score(gt[\"side_effect\"], pred[\"side_effect\"], scorer)\n",
    "                precision.append(P)\n",
    "                recall.append(R)\n",
    "                f1.append(F1)\n",
    "                matched_gt_indices.add(gt_index)\n",
    "                break\n",
    "    \n",
    "    # Add 0 for all unmatched items\n",
    "    unmatched = len(remaining_gt) - len(matched_gt_indices) + len(predicted) - len(matched_gt_indices)\n",
    "\n",
    "    precision += [0]*unmatched\n",
    "    recall += [0]*unmatched\n",
    "    f1 += [0]*unmatched\n",
    "\n",
    "    return np.mean(precision), np.mean(recall), np.mean(f1)\n",
    "\n",
    "def create_metrics_df(labels_path:str, results_path:str, *args, **kwargs)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a dataframe with the metrics for each index. The metrics are:\n",
    "    - Precision, Recall and F1 score for medication names\n",
    "    - Precision, Recall and F1 score for side effects\n",
    "\n",
    "    Args:\n",
    "        labels_path (str): Path to the labels file.\n",
    "        results_path (str): Path to the results file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with the metrics for each index.\n",
    "    \"\"\"\n",
    "    \n",
    "    labels = prepare_labels(labels_path)\n",
    "    res = prepare_results(results_path)\n",
    "    scorer = kwargs.get(\"scorer\", None)\n",
    "\n",
    "    dfs = []\n",
    "    for index, group in res.groupby(\"index\"):\n",
    "        text = group[\"text\"].iloc[0]\n",
    "        original_text = group[\"original_text\"].iloc[0]\n",
    "        index = group[\"index\"].iloc[0]\n",
    "        preds_med = group[\"medication\"].to_list()\n",
    "        preds_se = group[\"side_effect\"].to_list()\n",
    "        labels_med = labels[labels[\"index\"] == index][\"medication\"].to_list()\n",
    "        labels_se = labels[labels[\"index\"] == index][\"side_effect\"].to_list()\n",
    "\n",
    "        # Side effects\n",
    "        ground_truth = labels[labels[\"index\"] == index].to_dict(orient=\"records\")\n",
    "        predicted = group.to_dict(orient=\"records\")\n",
    "        assert model is not None and tokenizer is not None, \"Model and tokenizer must be provided as keyword arguments for side effects evaluation.\"\n",
    "        side_effects_precision, side_effects_recall, side_effects_f1 = string_match(ground_truth, predicted, scorer)\n",
    "        # Medication\n",
    "        ground_truth = labels[labels[\"index\"] == index][\"medication\"].to_list()\n",
    "        predicted = group[\"medication\"].to_list()\n",
    "        medication_precision, medication_recall, medication_f1 = calculate_precision_recall_f1(ground_truth, predicted)\n",
    "\n",
    "        \n",
    "        if group[\"successful\"].iloc[0] == \"false\":\n",
    "            successful = False\n",
    "        else:\n",
    "            successful = True\n",
    "\n",
    "        dfs.append({\"text\": text, \n",
    "                    \"original_text\": original_text, \n",
    "                    \"index\": index, \n",
    "                    \"preds_med\": preds_med,\n",
    "                    \"labels_med\": labels_med,\n",
    "                    \"preds_se\": preds_se,\n",
    "                    \"labels_se\": labels_se,\n",
    "                    \"successful\": successful,\n",
    "                    \"side_effects_precision\": side_effects_precision, \"side_effects_recall\": side_effects_recall, \"side_effects_f1\": side_effects_f1, \"medication_precision\": medication_precision, \"medication_recall\": medication_recall, \"medication_f1\": medication_f1})\n",
    "\n",
    "    return pd.DataFrame(dfs)\n",
    "\n",
    "def create_agg_metrics_df(labels_path:str, results_path:str, *args, **kwargs)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a dataframe with the aggregated metrics for the whole dataset. The metrics are:\n",
    "    - Precision, Recall and F1 score for medication names\n",
    "    - Precision, Recall and F1 score for side effects\n",
    "\n",
    "    Args:\n",
    "        labels_path (str): Path to the labels file.\n",
    "        results_path (str): Path to the results file.\n",
    "        scorer (BERTScorer): BERTScorer object to score the similarity between the side effects.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with the aggregated metrics for the whole dataset.\n",
    "    \"\"\"\n",
    "    metrics = create_metrics_df(labels_path, results_path, *args, **kwargs)\n",
    "\n",
    "    # Calculate the average of the metrics\n",
    "    agg_metrics = metrics.agg({\"side_effects_precision\": \"mean\", \"side_effects_recall\": \"mean\", \"side_effects_f1\": \"mean\", \"medication_precision\": \"mean\", \"medication_recall\": \"mean\", \"medication_f1\": \"mean\", \"successful\": \"sum\"})\n",
    "\n",
    "    return agg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(paths:list[str], labels:str, *args, **kwargs)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarize the results from the model. The summary consists of:\n",
    "    - Precision, Recall and F1 score for medication names\n",
    "    - Precision, Recall and F1 score for side effects\n",
    "    - Aggregated metrics for the whole dataset\n",
    "\n",
    "    Args:\n",
    "        paths (list): List of paths to the results files.\n",
    "        labels (str): Path to the labels file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with the summary of the results.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for path in paths:\n",
    "        metrics = create_agg_metrics_df(labels, path, *args, **kwargs)\n",
    "        path = str(path)\n",
    "        if path.endswith(\"rag.pt\"):\n",
    "            metrics[\"approach\"] = \"S2A-1\"\n",
    "        elif path.endswith(\"s2a.pt\"):\n",
    "            metrics[\"approach\"] = \"S2A-2\"\n",
    "        else:\n",
    "            metrics[\"approach\"] = \"Base\"\n",
    "        \n",
    "        if \"few_shot_vanilla\" in path:\n",
    "            metrics[\"strategy\"] = \"Few-Shot Base\"\n",
    "        elif \"few_shot_instruction\" in path:\n",
    "            metrics[\"strategy\"] = \"Few-Shot Instruction\"\n",
    "        elif \"zero_shot_vanilla\" in path:\n",
    "            metrics[\"strategy\"] = \"Zero-Shot Base\"\n",
    "        elif \"zero_shot_instruction\" in path:\n",
    "            metrics[\"strategy\"] = \"Zero-Shot Instruction\"\n",
    "        else:\n",
    "            ValueError(\"Unknown strategy\")\n",
    "        dfs.append(metrics)\n",
    "    return pd.DataFrame(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_vanilla_s2a.pt\", scorer=scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_vanilla.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot instruction\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_instruction.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_vanilla_10_examples.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_10_examples.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_vanilla_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot instruction\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_instruction_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_vanilla_10_examples_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_10_examples_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_vanilla_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot instruction\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_instruction_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_vanilla_10_examples_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Instruction\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_10_examples_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latex_metrics(metrics:pd.DataFrame)->str:\n",
    "    \"\"\"\n",
    "    Convert the metrics dataframe to a latex table.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for strat, group in metrics.groupby(\"strategy\"):\n",
    "        cols = [col for col in metrics.columns if col not in [\"strategy\", \"approach\"]]\n",
    "        df_base = group[group[\"approach\"] == \"Base\"][cols].reset_index(drop=True)\n",
    "        df_base.columns = [f\"{col}_base\" for col in cols]\n",
    "        df_rag = group[group[\"approach\"] == \"S2A-1\"][cols].reset_index(drop=True)\n",
    "        df_rag.columns = [f\"{col}_rag\" for col in cols]\n",
    "        df_s2a = group[group[\"approach\"] == \"S2A-2\"][cols].reset_index(drop=True)\n",
    "        df_s2a.columns = [f\"{col}_s2a\" for col in cols]\n",
    "        df = pd.concat([df_base, df_rag, df_s2a], axis=1)\n",
    "        df[\"strategy\"] = strat\n",
    "        dfs.append(df)\n",
    "\n",
    "    return pd.concat(dfs, axis=0)\n",
    "\n",
    "def p_r_f_data(results: pd.DataFrame):\n",
    "    thesis13b = latex_metrics(results)\n",
    "    thesis13b_precision = thesis13b[[\"strategy\"] + [col for col in thesis13b.columns if \"precision\" in col]]\n",
    "    thesis13b_recall = thesis13b[[\"strategy\"] + [col for col in thesis13b.columns if \"recall\" in col]]\n",
    "    thesis13b_f1 = thesis13b[[\"strategy\"] + [col for col in thesis13b.columns if \"f1\" in col]]\n",
    "    return thesis13b_precision, thesis13b_recall, thesis13b_f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_vanilla.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_instruction.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_vanilla_10_examples.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_10_examples.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_vanilla_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_instruction_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_vanilla_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_vanilla_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_instruction_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_vanilla_10_examples_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_10_examples_s2a.pt\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = summarize(file_paths, paths.RESULTS_PATH/\"side-effects/labels.xlsx\", scorer=scorer)\n",
    "results = results.round(2)\n",
    "# results.to_csv(paths.RESULTS_PATH/\"side-effects/summary13b.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesis13b_precision,thesis13b_recall, thesis13b_f1 =  p_r_f_data(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thesis13b = latex_metrics(results)\n",
    "# thesis13b_precision = thesis13b[[\"strategy\"] + [col for col in thesis13b.columns if \"precision\" in col]]\n",
    "# thesis13b_recall = thesis13b[[\"strategy\"] + [col for col in thesis13b.columns if \"recall\" in col]]\n",
    "# thesis13b_f1 = thesis13b[[\"strategy\"] + [col for col in thesis13b.columns if \"f1\" in col]]\n",
    "\n",
    "# thesis13b_precision.to_csv(paths.RESULTS_PATH/\"side-effects/thesis13b_precision.csv\", index=False)\n",
    "# thesis13b_recall.to_csv(paths.RESULTS_PATH/\"side-effects/thesis13b_recall.csv\", index=False)\n",
    "# thesis13b_f1.to_csv(paths.RESULTS_PATH/\"side-effects/thesis13b_f1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model seems to have difficulties if there is no medication mentioned with side effects. The tuned model does not have this problem I think, show example\n",
    "- also split medications that contain \"/\" into two lists\n",
    "- concatenate the side effects for the same medications with \",\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_vanilla.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot instruction\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_instruction.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_vanilla_10_examples.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_instruction_10_examples.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_vanilla_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot instruction\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_instruction_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_vanilla_10_examples_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_instruction_10_examples_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_vanilla_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot instruction\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_instruction_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_vanilla_10_examples_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_instruction_10_examples_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths7b = [paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_vanilla.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_instruction.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_vanilla_10_examples.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_instruction_10_examples.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_vanilla_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_instruction_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_vanilla_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_instruction_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_vanilla_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_instruction_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_vanilla_10_examples_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_instruction_10_examples_s2a.pt\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results7b = summarize(file_paths7b, paths.RESULTS_PATH/\"side-effects/labels.xlsx\", model=model, tokenizer=tokenizer, threshold = 0.9)\n",
    "results7b = results7b.round(2)\n",
    "# results7b.to_csv(paths.RESULTS_PATH/\"side-effects/summary7b.csv\", index=False)\n",
    "# results7b.to_csv(paths.THESIS_PATH/\"se_summary7b.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesis7b = latex_metrics(results7b)\n",
    "thesis7b_precision = thesis7b[[\"strategy\"] + [col for col in thesis7b.columns if \"precision\" in col]]\n",
    "thesis7b_recall = thesis7b[[\"strategy\"] + [col for col in thesis7b.columns if \"recall\" in col]]\n",
    "thesis7b_f1 = thesis7b[[\"strategy\"] + [col for col in thesis7b.columns if \"f1\" in col]]\n",
    "\n",
    "# thesis7b_precision.to_csv(paths.RESULTS_PATH/\"side-effects/thesis7b_precision.csv\", index=False)\n",
    "# thesis7b_recall.to_csv(paths.RESULTS_PATH/\"side-effects/thesis7b_recall.csv\", index=False)\n",
    "# thesis7b_f1.to_csv(paths.RESULTS_PATH/\"side-effects/thesis7b_f1.csv\", index=False)\n",
    "\n",
    "thesis7b_precision.to_csv(paths.THESIS_PATH/\"se_thesis7b_precision.csv\", index=False)\n",
    "thesis7b_recall.to_csv(paths.THESIS_PATH/\"se_thesis7b_recall.csv\", index=False)\n",
    "thesis7b_f1.to_csv(paths.THESIS_PATH/\"se_thesis7b_f1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13B Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths13b_lora_1024 = [\n",
    "                paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_zero_shot_vanilla.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_zero_shot_instruction.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_few_shot_vanilla_10_examples.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_few_shot_instruction_10_examples.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_zero_shot_vanilla_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_zero_shot_instruction_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_few_shot_vanilla_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_few_shot_instruction_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_zero_shot_vanilla_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_zero_shot_instruction_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_few_shot_vanilla_10_examples_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_few_shot_instruction_10_examples_s2a.pt\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results13b_lora_1024 = summarize(file_paths13b_lora_1024, paths.RESULTS_PATH/\"side-effects/labels.xlsx\", scorer=scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results13b_lora_1024 = results13b_lora_1024.round(2)\n",
    "results13b_lora_1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesis13b_1024_precision,thesis13b_1024_recall, thesis13b_1024_f1 =  p_r_f_data(results13b_lora_1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths13b_lora_512 = [\n",
    "                paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_zero_shot_vanilla.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_zero_shot_instruction.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_few_shot_vanilla_10_examples.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_few_shot_instruction_10_examples.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_zero_shot_vanilla_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_zero_shot_instruction_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_few_shot_vanilla_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_few_shot_instruction_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_zero_shot_vanilla_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_zero_shot_instruction_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_few_shot_vanilla_10_examples_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_few_shot_instruction_10_examples_s2a.pt\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results13b_lora_512 = summarize(file_paths13b_lora_512, paths.RESULTS_PATH/\"side-effects/labels.xlsx\", scorer=scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results13b_lora_512 = results13b_lora_512.round(2)\n",
    "results13b_lora_512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesis13b_512_precision,thesis13b_512_recall, thesis13b_512_f1 =  p_r_f_data(results13b_lora_512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis\n",
    "Concat results of 13B models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_13b_precision = pd.concat([thesis13b_precision, thesis13b_512_precision, thesis13b_1024_precision])\n",
    "overall_13b_precision[\"model\"] = [\"base\"] * len(thesis13b_precision) + [\"512\"] * len(thesis13b_precision) + [\"1024\"] * len(thesis13b_precision)\n",
    "overall_13b_recall = pd.concat([thesis13b_recall, thesis13b_512_recall, thesis13b_1024_recall])\n",
    "overall_13b_recall[\"model\"] = [\"base\"] * len(thesis13b_recall) + [\"512\"] * len(thesis13b_recall) + [\"1024\"] * len(thesis13b_recall)\n",
    "overall_13b_f1 = pd.concat([thesis13b_f1, thesis13b_512_f1, thesis13b_1024_f1])\n",
    "overall_13b_f1[\"model\"] = [\"base\"] * len(thesis13b_f1) + [\"512\"] * len(thesis13b_f1) + [\"1024\"] * len(thesis13b_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_13b_f1[[\"instruction\" in strat.lower() for strat in overall_13b_f1[\"strategy\"]]].to_csv(paths.THESIS_PATH/\"se_lora_f1.csv\")\n",
    "overall_13b_precision[[\"instruction\" in strat.lower() for strat in overall_13b_precision[\"strategy\"]]].to_csv(paths.THESIS_PATH/\"se_lora_precision.csv\")\n",
    "overall_13b_recall[[\"instruction\" in strat.lower() for strat in overall_13b_recall[\"strategy\"]]].to_csv(paths.THESIS_PATH/\"se_lora_recall.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_13b_f1[[\"instruction\" in strat.lower() for strat in overall_13b_f1[\"strategy\"]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold\n",
    "Will compare effects of threshold for few shot instruction s2a on results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_files = [\n",
    "    paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_10_examples_s2a.pt\",\n",
    "    paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_few_shot_instruction_10_examples_s2a.pt\",\n",
    "    paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_few_shot_instruction_10_examples_s2a.pt\",\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 11)\n",
    "dfs = []\n",
    "for threshold in thresholds:\n",
    "    _df = summarize(threshold_files, paths.RESULTS_PATH/\"side-effects/labels.xlsx\", model=model, tokenizer=tokenizer, threshold = threshold)\n",
    "    _df[\"model\"] = [\"Llama2-MedTuned-13B\", \"Llama2-MedTuned-13B-lora-512\", \"Llama2-MedTuned-13B-lora-1024\"]\n",
    "    _df[\"threshold\"] = threshold\n",
    "    dfs.append(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thresholds = pd.concat(dfs, axis=0)\n",
    "df_thresholds.drop(columns=[\"successful\", \"medication_precision\", \"medication_recall\", \"medication_f1\",\t\"successful\",\"approach\", \"strategy\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line_plot_group(x:pd.Series, y:pd.Series, groups:pd.Series, title:str = \"Line Plot\",\n",
    "                             xlabel:str = \"Threshold\", ylabel:str = \"Recall\", save_dir:str=None):\n",
    "    \"\"\"\n",
    "    Plot a line plot with different color for group.\n",
    "\n",
    "    Args:\n",
    "        x (pd.Series): x values.\n",
    "        y (pd.Series): y values.\n",
    "        groups (pd.Series): group values.\n",
    "        title (str): Title of the plot.\n",
    "        color_palette (np.ndarray): Color palette to use.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(x) == len(y) == len(groups), \"Length of x, y and group must be the same.\"\n",
    "\n",
    "    unique_groups = groups.unique()\n",
    "\n",
    "    data = pd.DataFrame({\"x\": x, \"y\": y, \"groups\": groups})\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    sns.lineplot(data=data, x=\"x\", y=\"y\", hue=\"groups\", palette=\"viridis\")\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    if save_dir:\n",
    "        plt.savefig(save_dir)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_line_plot_group(df_thresholds[\"threshold\"], df_thresholds[\"side_effects_recall\"], df_thresholds[\"model\"], title=\"Recall vs Threshold\", ylabel=\"Recall\", xlabel=\"Threshold\", save_dir = paths.THESIS_PATH/\"se_threshold_recall.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_line_plot_group(df_thresholds[\"threshold\"], df_thresholds[\"side_effects_precision\"], df_thresholds[\"model\"], title=\"Precision vs Threshold\", ylabel=\"Precision\", xlabel=\"Threshold\", save_dir = paths.THESIS_PATH/\"se_threshold_precision.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_line_plot_group(df_thresholds[\"threshold\"], df_thresholds[\"side_effects_f1\"], df_thresholds[\"model\"], title=\"F1 vs Threshold\", ylabel=\"F1\", xlabel=\"Threshold\", save_dir = paths.THESIS_PATH/\"se_threshold_f1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(df, title=\"Precision-Recall Curve\", xlabel=\"Recall\", ylabel=\"Precision\", save_dir=None):\n",
    "    \"\"\"\n",
    "    Plot the Precision-Recall curve for different models.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing precision, recall, and model name.\n",
    "        title (str): Title of the plot.\n",
    "        xlabel (str): X-axis label.\n",
    "        ylabel (str): Y-axis label.\n",
    "        save_dir (str, optional): Directory path to save the plot. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Use seaborn lineplot to draw precision-recall curves for each model\n",
    "    sns.lineplot(data=df, x=\"side_effects_recall\", y=\"side_effects_precision\", hue=\"model\", palette=\"viridis\", style=\"model\", markers=True, dashes=False)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "\n",
    "    if save_dir:\n",
    "        plt.savefig(save_dir, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(df_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1024 = prepare_results(paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_few_shot_instruction_10_examples_s2a.pt\")\n",
    "res_512 = prepare_results(paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_few_shot_instruction_10_examples_s2a.pt\")\n",
    "res = prepare_results(paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_10_examples_s2a.pt\")\n",
    "labels = prepare_labels(paths.RESULTS_PATH/\"side-effects/labels.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, data in res_1024.groupby(\"index\"):\n",
    "    if len(res_512[res_512[\"index\"] == index]) != len(data):\n",
    "        print(\"labels\")\n",
    "        display(labels[labels[\"index\"] == index])\n",
    "        print(\"1024\")\n",
    "        display(data)\n",
    "        print(\"res\")\n",
    "        display(res[res[\"index\"] == index])\n",
    "        print(\"512\")\n",
    "        display(res_512[res_512[\"index\"] == index])\n",
    "        print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance difference:\n",
    "- index 17: 1024 model understand better, when no medication is given like \"unerwünschte nebenwirkung ona arzneimittel oder droge\", to just put \"unknown\", meanwhile the other 2 repeated the examples from few-shot\n",
    "- index 22 another example: \"indent labor: crp <0.5 mg/ml, bsr 8 mm/h, nw leukozyten + thrombozyten\", also unknown\n",
    "\n",
    "Good examples:\n",
    "- index 3\n",
    "- index 13\n",
    "\n",
    "Bad examples:\n",
    "- index: 55; \"indent st.n. symptomatischer therapie mit fampyra (insgesamt 2 therapieversuche, gestoppt bei wirkungslosogkeit und i.r. der kardialen vorbefunden), therapieversuch mit modasomil/remeron (keine besserung), venlafaxin (bei nebenwirkungen gestoppt), sirdalud\n",
    "indent rebif 2002, abgesetzt 6 monate später wegen unverträglichkeit (grippale nebenwirkungen)\"; only venlafaxin and rebif have se (model extracts venlafaxin with unknown, misses rebif and puts all the other medications like fampyra, modasomil, remeron, sirdalud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_1024[res_1024[\"index\"] == \"3\"][res_1024.columns[:-4]].to_dict(orient = \"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_1024[res_1024[\"index\"] == \"13\"][res_1024.columns[:-4]].to_dict(orient = \"records\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_1024[res_1024[\"index\"] == \"22\"][res_1024.columns[:-4]].to_dict(orient = \"records\"))\n",
    "print(res_512[res_512[\"index\"] == \"22\"][res_512.columns[:-4]].to_dict(orient = \"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_1024[res_1024[\"index\"] == \"55\"][res_1024.columns[:-4]].to_dict(orient = \"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def string_match(ground_truth:list[dict], predicted:list[dict], model:BertModel, tokenizer:BertTokenizer, threshold:float=0.9)->tuple[float, float, float]:\n",
    "#     \"\"\" \n",
    "#     Calculate if the strings match (based on cosine similarity) from two lists of dictionaries. This function is used to evaluate the side effects.\n",
    "\n",
    "#     Args:\n",
    "#         ground_truth (list): List of ground truth medication names and their side effect.\n",
    "#         predicted (list): List of predicted medication names and their side effect.\n",
    "#         model (BertModel): BertModel object to calculate the cosine similarity.\n",
    "#         tokenizer (BertTokenizer): BertTokenizer object to tokenize the text.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Match the prediction to the ground truth with medication name\n",
    "#     tp = 0\n",
    "#     fn = 0\n",
    "#     fp = 0\n",
    "\n",
    "#     for pred_id, pred in enumerate(predicted):\n",
    "#         for gt_id, gt in enumerate(ground_truth):\n",
    "#             if pred[\"medication\"] in gt[\"medication\"] or gt[\"medication\"] in pred[\"medication\"]:\n",
    "#                 if high_cosine_similarity([pred[\"side_effect\"]], [gt[\"side_effect\"]], model, tokenizer, threshold):\n",
    "#                     tp += 1\n",
    "#                     ground_truth.pop(gt_id)\n",
    "#                     predicted.pop(pred_id)\n",
    "#                     break\n",
    "#                 else:\n",
    "#                     fp += 1\n",
    "#                     pass\n",
    "#                     # fn += 1\n",
    "#                 # Remove the ground truth and prediction from the list\n",
    "#                 # ground_truth.pop(gt_id)\n",
    "                    \n",
    "\n",
    "#     # For remaining ground truth\n",
    "#     fn = len(ground_truth)\n",
    "#     fp += len(predicted)\n",
    "\n",
    "#     # Calculate Precision\n",
    "#     if tp + fp == 0:\n",
    "#         precision = 0\n",
    "#     else:\n",
    "#         precision = tp / (tp + fp)\n",
    "\n",
    "#     # Calculate Recall\n",
    "#     if tp + fn == 0:\n",
    "#         recall = 0\n",
    "#     else:\n",
    "#         recall = tp / (tp + fn)\n",
    "\n",
    "#     # Calculate F1 Score\n",
    "#     if precision + recall == 0:\n",
    "#         f1 = 0\n",
    "#     else:\n",
    "#         f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "#     return precision, recall, f1\n",
    "\n",
    "# def string_match(ground_truth: list[dict], predicted: list[dict], model: BertModel, tokenizer: BertTokenizer, threshold: float = 0.9) -> tuple[float, float, float]:\n",
    "#     \"\"\"\n",
    "#     Calculate if the strings match (based on cosine similarity) from two lists of dictionaries.\n",
    "#     \"\"\"\n",
    "#     tp = 0  # True Positives\n",
    "#     # fp = 0  # False Positives\n",
    "#     # fn = 0  # False Negatives\n",
    "\n",
    "#     # Copy the ground truth and predicted lists to avoid modifying the original lists\n",
    "#     remaining_gt = ground_truth.copy()\n",
    "#     matched_gt_indices = set()\n",
    "\n",
    "#     for pred in predicted:\n",
    "#         match_found = False\n",
    "#         for gt_index, gt in enumerate(remaining_gt):\n",
    "#             if gt_index in matched_gt_indices:\n",
    "#                 # Skip already matched ground truth items\n",
    "#                 continue\n",
    "            \n",
    "#             if pred[\"medication\"] in gt[\"medication\"] or gt[\"medication\"] in pred[\"medication\"]:\n",
    "#                 if high_cosine_similarity([pred[\"side_effect\"]], [gt[\"side_effect\"]], model, tokenizer, threshold):\n",
    "#                     tp += 1\n",
    "#                     matched_gt_indices.add(gt_index)\n",
    "#                     match_found = True\n",
    "#                     break\n",
    "        \n",
    "#         if not match_found:\n",
    "#             fp += 1\n",
    "\n",
    "#     # Calculate FN as ground truth items that weren't matched\n",
    "#     fn = len(ground_truth) - tp\n",
    "\n",
    "#     # # Calculate Precision, Recall, and F1 Score\n",
    "#     # precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "#     # recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "#     # f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "#     precision = tp/len(predicted)\n",
    "#     recall = tp/len(ground_truth)\n",
    "#     f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "#     return precision, recall, f1\n",
    "\n",
    "def high_cosine_similarity(ground_truth:str, prediction:str, model:BertModel, tokenizer:BertTokenizer, threshhold:float = 0.7):\n",
    "    \"\"\"\n",
    "    Calculate if the cosine similarity between the ground truth and the prediction is higher than the threshhold.\n",
    "\n",
    "    Args:\n",
    "        ground_truth (str): The ground truth text.\n",
    "        prediction (str): The predicted text.\n",
    "        model (BertModel): BertModel object to calculate the cosine similarity.\n",
    "        tokenizer (BertTokenizer): BertTokenizer object to tokenize the text.\n",
    "        threshhold (float, optional): The threshhold for the cosine similarity. Defaults to 0.7.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the cosine similarity is higher than the threshhold, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ground_truth = model(**tokenizer(ground_truth, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device))[\"last_hidden_state\"].to(\"cpu\")\n",
    "        prediction = model(**tokenizer(prediction, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device))[\"last_hidden_state\"].to(\"cpu\")\n",
    "        ground_truth = ground_truth.mean(dim=1)\n",
    "        prediction = prediction.mean(dim=1)\n",
    "        return cosine_similarity(ground_truth, prediction).item() >= threshhold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
