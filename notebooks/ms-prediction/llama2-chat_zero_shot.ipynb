{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, DataCollatorWithPadding, AutoModelForSequenceClassification\n",
    "\n",
    "from datasets import DatasetDict, Features, Sequence, Value, load_dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import gc\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "from src import paths\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model\n",
    "# checkpoint = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# # Save model and tokenizer\n",
    "# model.save_pretrained(paths.MODEL_PATH/'llama2-chat')\n",
    "# tokenizer.save_pretrained(paths.MODEL_PATH/'llama2-chat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 10.20 GB\n",
      "   Allocated Memory : 0.00 GB\n",
      "   Reserved Memory : 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        for gpu_id in range(num_gpus):\n",
    "            free_mem, total_mem = torch.cuda.mem_get_info(gpu_id)\n",
    "            gpu_properties = torch.cuda.get_device_properties(gpu_id)\n",
    "            print(f\"GPU {gpu_id}: {gpu_properties.name}\")\n",
    "            print(f\"   Total Memory: {total_mem / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Free Memory: {free_mem / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Allocated Memory : {torch.cuda.memory_allocated(gpu_id) / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Reserved Memory : {torch.cuda.memory_reserved(gpu_id) / (1024 ** 3):.2f} GB\")\n",
    "    else:\n",
    "        print(\"No GPU available.\")\n",
    "# Call the function to check GPU memory\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before Model is loaded:\n",
      "\n",
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 10.20 GB\n",
      "   Allocated Memory : 0.00 GB\n",
      "   Reserved Memory : 0.00 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db608e9c06264114b7a7b88e01bde92f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory after Model is loaded:\n",
      "\n",
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 6.42 GB\n",
      "   Allocated Memory : 3.77 GB\n",
      "   Reserved Memory : 3.78 GB\n"
     ]
    }
   ],
   "source": [
    "# Low precision config\n",
    "print(\"Memory before Model is loaded:\\n\")\n",
    "check_gpu_memory()\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(paths.MODEL_PATH/'leo-hessianai-7b', \n",
    "                                             device_map=\"auto\", \n",
    "                                             quantization_config = bnb_config, \n",
    "                                             trust_remote_code = False)\n",
    "print(\"Memory after Model is loaded:\\n\")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model for faster inference. # To-Do https://pytorch.org/blog/pytorch-compile-to-speed-up-inference/\n",
    "# model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size without Pad Token:  32000\n",
      "Tokenizer pad token ID: 32000\n",
      "Model pad token ID: 32000\n",
      "Model config pad token ID: 32000\n",
      "Vocabulary Size with Pad Token:  32001\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(paths.MODEL_PATH/'leo-hessianai-7b', padding_side='left')\n",
    "print(\"Vocabulary Size without Pad Token: \", len(tokenizer))\n",
    "\n",
    "# Check if the pad token is already in the tokenizer vocabulary\n",
    "if '<pad>' not in tokenizer.get_vocab():\n",
    "    # Add the pad token\n",
    "    tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "\n",
    "#Resize the embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#Configure the pad token in the model\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Check if they are equal\n",
    "assert model.config.pad_token_id == tokenizer.pad_token_id, \"The model's pad token ID does not match the tokenizer's pad token ID!\"\n",
    "\n",
    "# Print the pad token ids\n",
    "print('Tokenizer pad token ID:', tokenizer.pad_token_id)\n",
    "print('Model pad token ID:', model.config.pad_token_id)\n",
    "print('Model config pad token ID:', model.config.pad_token_id)\n",
    "print(\"Vocabulary Size with Pad Token: \", len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = \"<s>[INST]\\n<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{user_prompt}[/INST]\\n Translation: \\n\"\n",
    "def get_classification_llama(text):\n",
    "    # Shorten Text so that beam-search can be performed\n",
    "    text = text\n",
    "    input = base_prompt.format(system_prompt = \"Translate the Text to English.\",\n",
    "                               user_prompt = text)\n",
    "    return input\n",
    "\n",
    "def preprocess(example):\n",
    "    example = tokenizer(get_classification_llama(example[\"text\"]), return_tensors =\"pt\")\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_files = {\"train\": \"ms-diag_clean_train.csv\", \"validation\": \"ms-diag_clean_val.csv\", \"test\": \"ms-diag_clean_test.csv\"}\n",
    "df = load_dataset(os.path.join(paths.DATA_PATH_PREPROCESSED,'ms-diag'), data_files = data_files)\n",
    "#df = df.map(preprocess, remove_columns=[\"rid\", \"date\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [tokenizer(get_classification_llama(t)) for t in df[\"train\"][\"text\"]]\n",
    "\n",
    "# Default collate function \n",
    "collate_fn = DataCollatorWithPadding(tokenizer, padding=True) #padding=True, 'max_length'\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset=tokens, collate_fn=collate_fn, batch_size=2, shuffle = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems as though reserved memory is extremely high when using beam search. If I have longer input sequences this will lead to out of memory issues. I will try to set number of tokens to a lower number and check if beam search works then. I truncate the text directly because if I truncate after the prompt insertion I will loose the end of the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Consumption before loop\n",
      "\n",
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 5.92 GB\n",
      "   Allocated Memory : 4.26 GB\n",
      "   Reserved Memory : 4.27 GB\n",
      "Memory Consumption before Batch:  0\n",
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 5.92 GB\n",
      "   Allocated Memory : 3.77 GB\n",
      "   Reserved Memory : 4.27 GB\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "print(\"Memory Consumption before loop\\n\")\n",
    "check_gpu_memory()\n",
    "for idx, batch in enumerate(dataloader):\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Memory Consumption before Batch: \", idx)\n",
    "    check_gpu_memory()\n",
    "    \n",
    "    input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "    attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "    with torch.inference_mode():\n",
    "        generated_ids = model.generate(input_ids = input_ids, attention_mask = attention_mask, max_new_tokens=20, num_beams=1, do_sample=True, temperature = 0.9, num_return_sequences = 1, top_p = 0.6).to(\"cpu\")\n",
    "    outputs.append(tokenizer.batch_decode(generated_ids, skip_special_tokens=True))\n",
    "    break\n",
    "    outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 22.73 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 8.04 GB\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 26.85 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 3.91 GB\n"
     ]
    }
   ],
   "source": [
    "check_gpu_memory()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "outputs = list(chain.from_iterable(outputs))\n",
    "pd.Series(outputs).to_csv(paths.RESULTS_PATH/'ms_diag-llama2-chat_zero_shot-shortened300_beam2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [out.split(\"\\nBased on the information provided in the text, the most likely diagnosis for the patient is:\")[1] for out in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'primary_progressive_multiple_sclerosis',\n",
       " 'relapsing_remitting_multiple_sclerosis',\n",
       " 'secondary_progressive_multiple_sclerosis'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df[\"train\"][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to map keywords to labels\n",
    "keyword_label_mapping = {\n",
    "    \"RRMS\": 'relapsing_remitting_multiple_sclerosis',\n",
    "    \"SPMS\": 'secondary_progressive_multiple_sclerosis',\n",
    "    \"PPMS\": 'primary_progressive_multiple_sclerosis',\n",
    "}\n",
    "\n",
    "# Function to assign labels based on text content\n",
    "def assign_label(text):\n",
    "    for keyword, label in keyword_label_mapping.items():\n",
    "        if keyword in text:\n",
    "            return label\n",
    "    return \"unknown\"  # Default label if no keyword is found\n",
    "\n",
    "# Assign labels to each text in the list\n",
    "labels = [assign_label(text) for text in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6016260162601627"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == df[\"train\"][\"labels\"][i]:\n",
    "        correct += 1\n",
    "correct/len(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
