{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, DataCollatorWithPadding\n",
    "\n",
    "from datasets import DatasetDict, Features, Sequence, Value, load_dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import gc\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "from src import paths\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model\n",
    "# checkpoint = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# # Save model and tokenizer\n",
    "# model.save_pretrained(paths.MODEL_PATH/'llama2-chat')\n",
    "# tokenizer.save_pretrained(paths.MODEL_PATH/'llama2-chat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 30.93 GB\n",
      "   Allocated Memory : 0.00 GB\n",
      "   Reserved Memory : 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        for gpu_id in range(num_gpus):\n",
    "            free_mem, total_mem = torch.cuda.mem_get_info(gpu_id)\n",
    "            gpu_properties = torch.cuda.get_device_properties(gpu_id)\n",
    "            print(f\"GPU {gpu_id}: {gpu_properties.name}\")\n",
    "            print(f\"   Total Memory: {total_mem / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Free Memory: {free_mem / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Allocated Memory : {torch.cuda.memory_allocated(gpu_id) / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Reserved Memory : {torch.cuda.memory_reserved(gpu_id) / (1024 ** 3):.2f} GB\")\n",
    "    else:\n",
    "        print(\"No GPU available.\")\n",
    "# Call the function to check GPU memory\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before Model is loaded:\n",
      "\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 30.93 GB\n",
      "   Allocated Memory : 0.00 GB\n",
      "   Reserved Memory : 0.00 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f43b94a5e124a94a40ff07e8fc1857b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory after Model is loaded:\n",
      "\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 27.05 GB\n",
      "   Allocated Memory : 3.69 GB\n",
      "   Reserved Memory : 3.88 GB\n"
     ]
    }
   ],
   "source": [
    "# Low precision config\n",
    "print(\"Memory before Model is loaded:\\n\")\n",
    "check_gpu_memory()\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(paths.MODEL_PATH/'llama2-chat', device_map=\"auto\", quantization_config=bnb_config)\n",
    "print(\"Memory after Model is loaded:\\n\")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model for faster inference. # To-Do https://pytorch.org/blog/pytorch-compile-to-speed-up-inference/\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size without Pad Token:  32000\n",
      "Tokenizer pad token ID: 32000\n",
      "Model pad token ID: 32000\n",
      "Model config pad token ID: 32000\n",
      "Vocabulary Size with Pad Token:  32001\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(paths.MODEL_PATH/'llama2', padding_side='left')\n",
    "print(\"Vocabulary Size without Pad Token: \", len(tokenizer))\n",
    "\n",
    "# Check if the pad token is already in the tokenizer vocabulary\n",
    "if '<pad>' not in tokenizer.get_vocab():\n",
    "    # Add the pad token\n",
    "    tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "\n",
    "#Resize the embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#Configure the pad token in the model\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Check if they are equal\n",
    "assert model.config.pad_token_id == tokenizer.pad_token_id, \"The model's pad token ID does not match the tokenizer's pad token ID!\"\n",
    "\n",
    "# Print the pad token ids\n",
    "print('Tokenizer pad token ID:', tokenizer.pad_token_id)\n",
    "print('Model pad token ID:', model.config.pad_token_id)\n",
    "print('Model config pad token ID:', model.config.pad_token_id)\n",
    "print(\"Vocabulary Size with Pad Token: \", len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device of _orig_mod.model.embed_tokens.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.0.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.0.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.0.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.0.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.0.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.0.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.0.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.0.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.0.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.1.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.1.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.1.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.1.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.1.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.1.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.1.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.1.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.1.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.2.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.2.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.2.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.2.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.2.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.2.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.2.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.2.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.2.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.3.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.3.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.3.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.3.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.3.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.3.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.3.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.3.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.3.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.4.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.4.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.4.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.4.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.4.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.4.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.4.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.4.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.4.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.5.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.5.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.5.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.5.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.5.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.5.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.5.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.5.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.5.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.6.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.6.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.6.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.6.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.6.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.6.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.6.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.6.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.6.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.7.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.7.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.7.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.7.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.7.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.7.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.7.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.7.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.7.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.8.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.8.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.8.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.8.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.8.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.8.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.8.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.8.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.8.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.9.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.9.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.9.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.9.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.9.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.9.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.9.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.9.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.9.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.10.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.10.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.10.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.10.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.10.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.10.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.10.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.10.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.10.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.11.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.11.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.11.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.11.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.11.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.11.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.11.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.11.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.11.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.12.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.12.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.12.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.12.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.12.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.12.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.12.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.12.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.12.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.13.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.13.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.13.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.13.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.13.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.13.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.13.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.13.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.13.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.14.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.14.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.14.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.14.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.14.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.14.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.14.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.14.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.14.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.15.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.15.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.15.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.15.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.15.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.15.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.15.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.15.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.15.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.16.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.16.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.16.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.16.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.16.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.16.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.16.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.16.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.16.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.17.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.17.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.17.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.17.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.17.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.17.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.17.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.17.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.17.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.18.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.18.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.18.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.18.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.18.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.18.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.18.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.18.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.18.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.19.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.19.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.19.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.19.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.19.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.19.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.19.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.19.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.19.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.20.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.20.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.20.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.20.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.20.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.20.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.20.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.20.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.20.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.21.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.21.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.21.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.21.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.21.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.21.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.21.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.21.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.21.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.22.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.22.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.22.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.22.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.22.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.22.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.22.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.22.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.22.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.23.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.23.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.23.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.23.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.23.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.23.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.23.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.23.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.23.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.24.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.24.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.24.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.24.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.24.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.24.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.24.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.24.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.24.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.25.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.25.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.25.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.25.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.25.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.25.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.25.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.25.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.25.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.26.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.26.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.26.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.26.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.26.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.26.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.26.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.26.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.26.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.27.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.27.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.27.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.27.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.27.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.27.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.27.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.27.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.27.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.28.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.28.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.28.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.28.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.28.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.28.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.28.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.28.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.28.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.29.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.29.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.29.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.29.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.29.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.29.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.29.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.29.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.29.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.30.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.30.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.30.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.30.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.30.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.30.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.30.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.30.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.30.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.31.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.31.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.31.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.31.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.31.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.31.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.31.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.31.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.31.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.norm.weight:  cuda:0\n",
      "Device of _orig_mod.lm_head.weight:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check device allocation\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Device of {name}: \", param.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = \"<s>[INST]\\n<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{user_prompt}[/INST]\\nBased on the information provided in the text, the most likely diagnosis for the patient is: \"\n",
    "def get_classification_llama(text):\n",
    "    # Shorten Text so that beam-search can be performed\n",
    "    text = text[:500]\n",
    "    input = base_prompt.format(system_prompt = \"Is the MS diagnosis in the text of type \\\"Sekundär progrediente Multiple Sklerose (SPMS)\\\", \\\"primäre progrediente Multiple Sklerose (PPMS)\\\" or \\\"schubförmig remittierende Multiple Sklerose (RRMS)\\\"?\",\n",
    "                               user_prompt = text)\n",
    "    return input\n",
    "\n",
    "def preprocess(example):\n",
    "    example = tokenizer(get_classification_llama(example[\"text\"]), return_tensors =\"pt\")\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_files = {\"train\": \"ms-diag_clean_train.csv\", \"validation\": \"ms-diag_clean_val.csv\", \"test\": \"ms-diag_clean_test.csv\"}\n",
    "df = load_dataset(os.path.join(paths.DATA_PATH_PREPROCESSED,'ms-diag'), data_files = data_files)\n",
    "#df = df.map(preprocess, remove_columns=[\"rid\", \"date\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5504\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "index = 0\n",
    "for i, text in enumerate(df[\"train\"][\"text\"]):\n",
    "    if max_len < len(text):\n",
    "        max_len = len(text)\n",
    "        index = i\n",
    "print(max_len)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST]\\n<<SYS>>\\nIs the MS diagnosis in the text of type \"Sekundär progrediente Multiple Sklerose (SPMS)\", \"primäre progrediente Multiple Sklerose (PPMS)\" or \"schubförmig remittierende Multiple Sklerose (RRMS)\"?\\n<</SYS>>\\n\\n1. Primär progrediente Multiple Sklerose, EM 1992, ED 1996, aktuell EDSS 7.0 INDENT Verlauf: Erstmanifestation mit Sensibilitätsstörungen am rechten Bein. Seitdem progrediente Beinschwäche sowie Extremitätenataxie, im gesamten Krankheitsverlauf kein Hinweis auf schubverdächtige Episoden INDENT Klinisch aktuell: Sakkadierte Blickfolge, bitemporal eingeschränkte Gesichtsfelder, Dysarthrie, links-/bein- und proximal betonte spastische Tetraparese, Babinski bds. positiv. Rechts- und beinbetonter Hol[/INST]\\nBased on the information provided in the text, the most likely diagnosis for the patient is: '"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_classification_llama(df[\"train\"][\"text\"][48])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 352])\n"
     ]
    }
   ],
   "source": [
    "tokens = [tokenizer(get_classification_llama(t)) for t in df[\"train\"][\"text\"]]\n",
    "\n",
    "# Default collate function \n",
    "collate_fn = DataCollatorWithPadding(tokenizer, padding=True) #padding=True, 'max_length'\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset=tokens, collate_fn=collate_fn, batch_size=8, shuffle = False) \n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 22.10 GB\n",
      "   Allocated Memory : 6.31 GB\n",
      "   Reserved Memory : 8.66 GB\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 13.00 GB\n",
      "   Allocated Memory : 6.31 GB\n",
      "   Reserved Memory : 17.76 GB\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 13.00 GB\n",
      "   Allocated Memory : 6.31 GB\n",
      "   Reserved Memory : 17.76 GB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m):\n\u001b[1;32m      4\u001b[0m     check_gpu_memory()\n\u001b[0;32m----> 5\u001b[0m     generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/transformers/generation/utils.py:1834\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1826\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1827\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1828\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1829\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1830\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1831\u001b[0m     )\n\u001b[1;32m   1833\u001b[0m     \u001b[38;5;66;03m# 14. run beam sample\u001b[39;00m\n\u001b[0;32m-> 1834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1835\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1841\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1842\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1844\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1851\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1852\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1859\u001b[0m     )\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/transformers/generation/utils.py:3515\u001b[0m, in \u001b[0;36mGenerationMixin.beam_sample\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3511\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   3513\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 3515\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3516\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3518\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3519\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3520\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3523\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1174\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1171\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1186\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1061\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1051\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1052\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1053\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         use_cache,\n\u001b[1;32m   1059\u001b[0m     )\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1061\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1070\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:789\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    786\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:692\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m    682\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    683\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    687\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    688\u001b[0m     )\n\u001b[1;32m    690\u001b[0m bsz, q_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m--> 692\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[1;32m    694\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/bitsandbytes/nn/modules.py:256\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    253\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    255\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 256\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:577\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul4Bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:516\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty(A\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m B_shape[:\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(A, \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdequantize_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(A\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mt(), bias)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n\u001b[1;32m    519\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m quant_state\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/bitsandbytes/functional.py:1041\u001b[0m, in \u001b[0;36mdequantize_4bit\u001b[0;34m(A, quant_state, absmax, out, blocksize, quant_type)\u001b[0m\n\u001b[1;32m   1039\u001b[0m         lib\u001b[38;5;241m.\u001b[39mcdequantize_blockwise_fp16_fp4(get_ptr(\u001b[38;5;28;01mNone\u001b[39;00m), get_ptr(A), get_ptr(absmax), get_ptr(out), ct\u001b[38;5;241m.\u001b[39mc_int(quant_state\u001b[38;5;241m.\u001b[39mblocksize), ct\u001b[38;5;241m.\u001b[39mc_int(n))\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1041\u001b[0m         \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdequantize_blockwise_fp16_nf4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabsmax\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16:\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m quant_state\u001b[38;5;241m.\u001b[39mquant_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfp4\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test = next(iter(dataloader))\n",
    "test = {k:v.to(\"cuda\") for k,v in test.items()}\n",
    "for i in range(6):\n",
    "    check_gpu_memory()\n",
    "    generated_ids = model.generate(**test, max_new_tokens=20, num_beams=2, do_sample=True, num_return_sequences=1, temperature = 0.9, top_p = 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems as though reserved memory is extremely high when using beam search. If I have longer input sequences this will lead to out of memory issues. I will try to set number of tokens to a lower number and check if beam search works then. I truncate the text directly because if I truncate after the prompt insertion I will loose the end of the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[INST]\\n<<SYS>>\\nIs the MS diagnosis in the text of type \"Sekundär progrediente Multiple Sklerose (SPMS)\", \"primäre progrediente Multiple Sklerose (PPMS)\" or \"schubförmig remittierende Multiple Sklerose (RRMS)\"?\\n<</SYS>>\\n\\nMultiple Sklerose mit sekundär progredientem Verlauf seit ca. 2004 (EM 1983, ED 1996) klinisch: aktuell spastische Hemiparese links, stark eingeschränkte Gehstrecke (Rollator, Rollauto für längere Strecken ), Miktionsstörung, Fatigue-Syndrom, Abduzensparese rechts letzter eindeutiger Schub 2001, 1983 Neuritis optici bds., 1996 sensibler Querschnitt Th5, seit 2004 zunehmende Gehbehinderung (04/2007 EDSS 5.5), deutliche Verschlechterung nach Patellafraktur 2014, seitdem Ausgangniveua nicht erreicht bildgebend: 2005 MRI cerebral sowie HWS und BWS: Cerebral alte Läsion ohne KM-Anreicherung, spinal werden aktive Herde auf Höhe C4 und Th1/2 beschrieben. 01/2006 Kontroll-MRI spinal ohne aktive entzündliche Läsionen. Alte Läsionen sowie eine Verschmächtigung des Rückenmarks auf Höhe C2/3. therapeutisch: 2004 Methylprednisolon oral, Beginn mit 500 mg, schrittweise Reduktion über 3 Wochen. Darunter keine Besserung des Zustandsbildes keine immunmodulatorische Basistherapie  INDENT Eine Reha ist nötig um weiterer Immobilisierung entgegenzuwirken und eine Verbesserung der Gehstrecke zu erreichen, die Spastik adequat zu behandeln sowie Copingstrategien zu entwickeln. (s. Einzelpunkte). Bei Sekundär progredienter MS sind die medikamentösen Behandlungsmöglichkeiten sehr eingeschränkt. Die Rehabilitation ist eine der wichtigsten Massnahmen um einer Immobilisierung und vollständigen Rollstuhlpflicht entgegenzuwirken.[/INST]\\nBased on the information provided in the text, the most likely diagnosis for the patient is \"sekundär progredienter Multiple Sklerose'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               [memory]         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b      19.00 Kb      19.00 Kb        106621  \n",
      "                                       cudaLaunchKernel        44.34%        2.424s        44.34%        2.424s      43.573us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b         55639  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         0.00%       2.000us       2.000us           0 b           0 b           0 b           0 b             1  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      41.000us         0.00%      41.000us       1.952us           0 b           0 b           0 b           0 b            21  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       5.000us         0.00%       5.000us       5.000us           0 b           0 b           0 b           0 b             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.000us         0.00%       3.000us       3.000us           0 b           0 b           0 b           0 b             1  \n",
      "                                        cudaMemcpyAsync        55.63%        3.041s        55.63%        3.041s      35.365ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            86  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us      43.000us         0.00%      43.000us       1.024us           0 b           0 b           0 b           0 b            42  \n",
      "                                  cudaStreamSynchronize         0.01%     453.000us         0.01%     453.000us       9.848us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            46  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       4.000us         0.00%       4.000us       1.000us           0 b           0 b           0 b           0 b             4  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 5.468s\n",
      "Self CUDA time total: 8.667s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 24.12 GB\n",
      "   Allocated Memory : 6.05 GB\n",
      "   Reserved Memory : 6.65 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Consumption before loop\n",
      "\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 24.12 GB\n",
      "   Allocated Memory : 6.05 GB\n",
      "   Reserved Memory : 6.65 GB\n",
      "Memory Consumption before Batch:  0\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 24.12 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 6.65 GB\n",
      "Memory Consumption before Batch:  1\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 26.85 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 3.91 GB\n",
      "Memory Consumption before Batch:  2\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 26.85 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 3.91 GB\n",
      "Memory Consumption before Batch:  3\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 26.85 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 3.91 GB\n",
      "Memory Consumption before Batch:  4\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 26.85 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 3.91 GB\n",
      "Memory Consumption before Batch:  5\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 26.85 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 3.91 GB\n",
      "Memory Consumption before Batch:  6\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 26.85 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 3.91 GB\n",
      "Memory Consumption before Batch:  7\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 26.85 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 3.91 GB\n",
      "Memory Consumption before Batch:  8\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 26.85 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 3.91 GB\n",
      "Memory Consumption before Batch:  9\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 26.85 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 3.91 GB\n",
      "Memory Consumption before Batch:  10\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 26.85 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 3.91 GB\n",
      "Memory Consumption before Batch:  11\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 26.85 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 3.91 GB\n",
      "Memory Consumption before Batch:  12\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 26.85 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 3.91 GB\n",
      "Memory Consumption before Batch:  13\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 26.85 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 3.91 GB\n",
      "Memory Consumption before Batch:  14\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 26.85 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 3.91 GB\n",
      "Memory Consumption before Batch:  15\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 26.85 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 3.91 GB\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "print(\"Memory Consumption before loop\\n\")\n",
    "check_gpu_memory()\n",
    "for idx, batch in enumerate(dataloader):\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Memory Consumption before Batch: \", idx)\n",
    "    check_gpu_memory()\n",
    "    \n",
    "    input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "    attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "    with torch.inference_mode():\n",
    "        generated_ids = model.generate(input_ids = input_ids, attention_mask = attention_mask, max_new_tokens=20, num_beams=2, do_sample=True, temperature = 0.9, num_return_sequences = 1, top_p = 0.6).to(\"cpu\")\n",
    "    outputs.append(tokenizer.batch_decode(generated_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 22.73 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 8.04 GB\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 26.85 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 3.91 GB\n"
     ]
    }
   ],
   "source": [
    "check_gpu_memory()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "outputs = list(chain.from_iterable(outputs))\n",
    "pd.Series(outputs).to_csv(paths.RESULTS_PATH/'ms_diag-llama2-chat_zero_shot-shortened300_beam2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [out.split(\"\\nBased on the information provided in the text, the most likely diagnosis for the patient is:\")[1] for out in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'primary_progressive_multiple_sclerosis',\n",
       " 'relapsing_remitting_multiple_sclerosis',\n",
       " 'secondary_progressive_multiple_sclerosis'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df[\"train\"][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to map keywords to labels\n",
    "keyword_label_mapping = {\n",
    "    \"RRMS\": 'relapsing_remitting_multiple_sclerosis',\n",
    "    \"SPMS\": 'secondary_progressive_multiple_sclerosis',\n",
    "    \"PPMS\": 'primary_progressive_multiple_sclerosis',\n",
    "}\n",
    "\n",
    "# Function to assign labels based on text content\n",
    "def assign_label(text):\n",
    "    for keyword, label in keyword_label_mapping.items():\n",
    "        if keyword in text:\n",
    "            return label\n",
    "    return \"unknown\"  # Default label if no keyword is found\n",
    "\n",
    "# Assign labels to each text in the list\n",
    "labels = [assign_label(text) for text in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6016260162601627"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == df[\"train\"][\"labels\"][i]:\n",
    "        correct += 1\n",
    "correct/len(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
