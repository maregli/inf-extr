{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset\n",
    "class MedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe: DatasetDict, tokenizer, max_length: int = 512, split: str = 'train'):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False).fit(np.stack(self.dataframe['train']['label']).reshape(-1, 1))\n",
    "        self.labels = self.enc.transform(np.stack(self.dataframe[split]['label']).reshape(-1, 1))\n",
    "        self.encodings = self.tokenizer(self.dataframe[split]['text'], truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: (val[idx].clone().detach()) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset\n",
    "class MedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe: DatasetDict, tokenizer, max_length: int = 512, split: str = 'train'):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False).fit(np.stack(self.dataframe['train']['label']).reshape(-1, 1))\n",
    "        self.labels = self.enc.transform(np.stack(self.dataframe[split]['label']).reshape(-1, 1))\n",
    "        self.encodings = self.tokenizer(self.dataframe[split]['text'], truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: (val[idx].clone().detach()) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datasets\n",
    "train_dataset = MedDataset(dataset, tokenizer, split='train')\n",
    "val_dataset = MedDataset(dataset, tokenizer, split='validation')\n",
    "test_dataset = MedDataset(dataset, tokenizer, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Implementation\n",
    "\n",
    "# # Set only specific layers to be trainable\n",
    "# for param in model.base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Set only specific layers to be trainable\n",
    "# for param in model.classifier.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# Dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Loss\n",
    "def loss_fn(logits, targets):\n",
    "    loss = (torch.nn.CrossEntropyLoss()(logits, targets) + \n",
    "            torchmetrics.classification.MulticlassF1Score(num_classes=num_labels, average='weighted').to(device)(logits, targets))\n",
    "    return loss\n",
    "\n",
    "# GPU Memory optimization\n",
    "model.gradient_checkpointing_enable()\n",
    "accelerator = Accelerator(fp16=True)\n",
    "model, optimizer, train_loader, val_loader, test_loader = accelerator.prepare(model, optimizer, train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "epochs = 36\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    pbar = tqdm.tqdm(train_loader)\n",
    "    for i, batch in enumerate(pbar):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging Progress\n",
    "        if i % 10 == 0:\n",
    "            pbar.set_description(f\"Epoch {epoch} training loss: {loss.item()}\")\n",
    "    \n",
    "    # Evaluate on Validation\n",
    "    val_CE_loss = []\n",
    "    val_f1 = []\n",
    "\n",
    "    pbar = tqdm.tqdm(val_loader)\n",
    "    pbar.set_description(f\"Epoch {epoch} Validation\")\n",
    "    for batch in pbar:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        val_CE_loss.append(torch.nn.CrossEntropyLoss()(outputs.logits, labels).item())\n",
    "        val_f1.append(torchmetrics.classification.MulticlassF1Score(num_classes=num_labels, average='weighted').to(device)(outputs.logits, labels).item())\n",
    "    \n",
    "    print(f\"Epoch {epoch} CrossEntropy Val loss: {np.mean(val_CE_loss)}\")\n",
    "    print(f\"Epoch {epoch} F1 Val score: {np.mean(val_f1)}\")\n",
    "    \n",
    "    # # Saving Model    \n",
    "    # if epoch % 10 == 0:\n",
    "    #     torch.save(model.state_dict(), paths.MODEL_PATH/f\"line-label_medBERT-finetuned_{epoch}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU Memory\n",
    "torch.cuda.empty_cache()\n",
    "del input_ids\n",
    "del attention_mask\n",
    "del labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths.MODEL_PATH/f\"line-label_medBERT-finetuned_{epoch}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "outputs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm.tqdm(test_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs.append(model(input_ids, attention_mask=attention_mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "preds = [np.argmax(output.logits.cpu().numpy(), axis=1) for output in outputs]\n",
    "preds = np.concatenate(preds)\n",
    "\n",
    "# Get true labels\n",
    "true = np.argmax(test_labels_enc, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "acc = np.sum(preds == true) / len(true)\n",
    "\n",
    "# F1 Score\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(true, preds, average='weighted')\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting pooling and head to trainable\n",
    "for name, param in trainer.model.named_parameters():\n",
    "    if \"pooler\" in name or \"classifier\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "trainer.save_model(os.path.join(paths.MODEL_PATH, \"medbert-diag-label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
