{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3218e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "from src import paths\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from src.data import data_loader\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3737b0",
   "metadata": {},
   "source": [
    "## diagnoses.csv\n",
    "\n",
    "- contains the disease labels for a given rid.\n",
    "- Question: there are sometimes multiple diagnoses per rid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a82305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nested_csv(dir_name: str, file_name: str):\n",
    "    \"\"\"\n",
    "    Returns a joint pandas dataframe from the files matching file_name\n",
    "    in all the different import dates subdirectories of the directory\n",
    "    specified by dir_name\n",
    "    \n",
    "    :param dir_name: The name of the directory (e.g. \"seantis\")\n",
    "    :param file_name: The name of the csv file to be read. (e.g. \"diagnoses.csv\")\n",
    "    \"\"\"\n",
    "    list_dfs = []\n",
    "\n",
    "    for root, dirs, files in os.walk(os.path.join(paths.DATA_PATH_RAW, dir_name)):\n",
    "        \n",
    "        if root.split(os.sep)[-1].startswith(\"imported_20210507\"):\n",
    "            try:\n",
    "                _df = pd.read_csv(os.path.join(root, file_name))\n",
    "                list_dfs.append(_df)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found in: {root}\")\n",
    "                continue\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"UnicodeDecodeError in: {root}\")\n",
    "                continue\n",
    "\n",
    "    df = pd.concat(list_dfs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2701d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnoses.csv files\n",
    "diagnoses = get_nested_csv(\"seantis\", \"diagnoses.csv\")\n",
    "display(diagnoses.head())\n",
    "\n",
    "# Count rows\n",
    "print(\"Number of rows: \", len(diagnoses))\n",
    "\n",
    "# Count unique research_ids\n",
    "print(\"Number of unique research_ids: \", diagnoses.research_id.nunique())\n",
    "\n",
    "# rids that have 2 or more occurences\n",
    "print(\"Number of rids that have 2 or more occurences: \", diagnoses[diagnoses.research_id.duplicated()].research_id.nunique())\n",
    "\n",
    "# Occurences per rid\n",
    "print(\"Occurences per rid: \", diagnoses.research_id.value_counts())\n",
    "\n",
    "# Occurences per disease\n",
    "print(\"Occurences per disease: \", diagnoses.disease.value_counts())\n",
    "\n",
    "# Examples of duplicate research_ids\n",
    "display(diagnoses[diagnoses.research_id.duplicated()].iloc[:2])\n",
    "\n",
    "# List of rids\n",
    "diagnoses_rids = diagnoses.research_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb2b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses[diagnoses.research_id == \"2048FE8D-4DFF-4939-9739-1B5A470914DA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ae796",
   "metadata": {},
   "source": [
    "### kisim_diagnoses.csv\n",
    "- Nearly all rid have multiple diagnoses\n",
    "- Some have up to 676 diagnoses. How do I match this to the diagnoses.csv file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73540e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kisim Diagnoses \n",
    "kisim_diagnoses = get_nested_csv(\"seantis\", \"kisim_diagnoses.csv\")\n",
    "display(kisim_diagnoses.head())\n",
    "print(\"Lenght of kisim_diagnoses: \", len(kisim_diagnoses))\n",
    "\n",
    "#Count duplicate diagnosis ids\n",
    "print(\"Duplicate diagnosis_id: \", kisim_diagnoses.diagnosis_id.duplicated().sum())\n",
    "\n",
    "# Count unique research ids\n",
    "print(\"Unique research_ids: \", kisim_diagnoses.research_id.nunique())\n",
    "\n",
    "# Number of rid that have 2 or more occurences\n",
    "print(\"Number of rids that have 2 or more occurences: : \", kisim_diagnoses[kisim_diagnoses.research_id.duplicated()].research_id.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9fc5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of diagnosis per research_id\n",
    "print(\"Number of diagnosis per research_id: \")\n",
    "display(kisim_diagnoses[\"research_id\"].value_counts())\n",
    "\n",
    "# Id \"2048FE8D-4DFF-4939-9739-1B5A470914DA\" has 676 diagnoses\n",
    "display(kisim_diagnoses[(kisim_diagnoses.research_id == \"2048FE8D-4DFF-4939-9739-1B5A470914DA\")])\n",
    "\n",
    "#Extract longest diagnosis_label from kisim_diagnoses[(kisim_diagnoses.research_id == \"2048FE8D-4DFF-4939-9739-1B5A470914DA\")]\n",
    "print(\"Longest Diagnosis Length\" ,kisim_diagnoses[(kisim_diagnoses.research_id == \"2048FE8D-4DFF-4939-9739-1B5A470914DA\")].diagnosis_label.str.len().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1629f0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for a kisim_diagnosis\n",
    "kisim_diagnoses.diagnosis_label.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2016ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlap with diagnoses.csv rid\n",
    "print(\"Overlap with diagnoses.csv rid: \", kisim_diagnoses.research_id.isin(diagnoses_rids).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdfcfa1",
   "metadata": {},
   "source": [
    "### reports_kisim_diagnoses\n",
    "\n",
    "- How do these differ from seantis/kisim_diagnoses? Sometimes there are doubles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08bc592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reports diagnosis\n",
    "reports_kisim_diagnoses = get_nested_csv(\"reports_with_struct_data\",\n",
    "                                         \"reports_kisim_diagnoses.csv\")\n",
    "display(reports_kisim_diagnoses.head())\n",
    "\n",
    "# Length of reports_kisim_diagnoses\n",
    "print(\"Length of reports_kisim_diagnoses: \", len(reports_kisim_diagnoses))\n",
    "\n",
    "# Count unique research_ids\n",
    "print(\"Unique research_ids: \", len(reports_kisim_diagnoses.research_id.unique()))\n",
    "\n",
    "# Number of rid that have 2 or more occurences\n",
    "print(\"Number of rids that have 2 or more occurences: \", reports_kisim_diagnoses[reports_kisim_diagnoses.research_id.duplicated()].research_id.nunique())\n",
    "\n",
    "# Count number of diagnosis per research_id\n",
    "print(\"Number of diagnosis per research_id: \")\n",
    "display(reports_kisim_diagnoses[\"research_id\"].value_counts())\n",
    "\n",
    "# Overlap with diagnoses.csv rid\n",
    "print(\"Overlap with diagnoses.csv rid: \", reports_kisim_diagnoses.research_id.isin(diagnoses_rids).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa768742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if these are identical to kisim_diagnoses\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "reports_kisim_diagnoses[reports_kisim_diagnoses.diagnosis_id == kisim_diagnoses.diagnosis_id.iloc[0]].diagnosis_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ce5804",
   "metadata": {},
   "source": [
    "### Reports\n",
    "\n",
    "- What is important here? Beurteilung? Zusatz? Form?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cd6af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = pd.read_csv(os.path.join(paths.PROJECT_ROOT, r\"preprocessed_nlp/midata-text-extraction/data/reports_with_struct_data/imported_20210507/all_info_reports.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af2502",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8209f60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d7347",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(paths.PROJECT_ROOT, r\"preprocessed_nlp/midata-text-extraction/data/reports_with_struct_data/imported_20210507/reports/Report_MitrendS_000B5446-F07C-4D9A-A336-39691B65AA7A_2021.05.05-12-39-58.json\")) as f:\n",
    "    file = json.load(f)\n",
    "\n",
    "json.dumps(file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e65ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_json(d, parent_key='', sep='_'):\n",
    "    \"\"\"\n",
    "    Flattens a nested JSON dict and handles the special case where there is an \"Item\" key with \"@num\" and \"CONTENT\" values.\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            # Handle special case where there is an \"Item\" key with \"@num\" and \"CONTENT\" values\n",
    "            if \"Item\" in v.keys():\n",
    "                try:\n",
    "                    item_num = v[\"Item\"][\"@num\"]\n",
    "                    item_content = v[\"Item\"][\"CONTENT\"]\n",
    "                    items.append((new_key + sep + \"Item_\" + item_num, item_content))\n",
    "                    del v[\"Item\"]\n",
    "                except TypeError:\n",
    "                    print(f\"TypeError in {new_key}\")\n",
    "                    continue\n",
    "                items.extend(flatten_json(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "        \n",
    "\n",
    "def get_nested_json(path: str):\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    return flatten_json(data)\n",
    "\n",
    "\n",
    "def get_reports():\n",
    "    \"\"\"\n",
    "    Returns the reports dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    # research_id and filename are stored in reports_with_struct_data/reportfilename_researchid.csv files\n",
    "    rid_filename = get_nested_csv(\"reports_with_struct_data\", \"reportfilename_researchid.csv\")\n",
    "\n",
    "    # Reports are in JSON format in reports_with_struct_data/reports\n",
    "    # We need to 1) read the JSON files 2) flatten them to pd.DataFrame() format and 3) add the research_id from the csv files\n",
    "    ls_reports = []\n",
    "\n",
    "    for root, dirs, files in os.walk(os.path.join(paths.DATA_PATH_RAW, \"reports_with_struct_data\")):\n",
    "\n",
    "        # All reports are in the reports folder\n",
    "        if root.endswith(\"reports\"):\n",
    "            for file in files:\n",
    "                try: \n",
    "                    # Get the research_id from the csv file\n",
    "                    _rid = rid_filename[rid_filename.report_filename + \".json\" == file].research_id.iloc[0]\n",
    "\n",
    "                    # Read the JSON file\n",
    "                    with open(os.path.join(root, file)) as f:\n",
    "                        _json = json.load(f)\n",
    "                    _df = pd.json_normalize(_json)\n",
    "\n",
    "                    # Add the research_id\n",
    "                    _df[\"research_id\"] = _rid\n",
    "\n",
    "                    # Append _df to list\n",
    "                    ls_reports.append(_df)\n",
    "                \n",
    "                except IndexError:\n",
    "                    print(f\"No research_id found for {file}\")\n",
    "                    continue\n",
    "    \n",
    "    return pd.concat(ls_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reports = get_reports()\n",
    "df_reports.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb38a98",
   "metadata": {},
   "source": [
    "### medication.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842692c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kisim_medication.csv\n",
    "kisim_medication = get_nested_csv(\"seantis\", \"kisim_medication.csv\")\n",
    "display(kisim_medication.head())\n",
    "\n",
    "# Length of kisim_medication\n",
    "print(\"Length of kisim_medication: \", len(kisim_medication))\n",
    "\n",
    "# Count unique research_ids\n",
    "print(\"Unique research_ids: \", len(kisim_medication.research_id.unique()))\n",
    "\n",
    "# Number of rid that have 2 or more occurences\n",
    "print(\"Number of rids that have 2 or more occurences: \", kisim_medication[kisim_medication.research_id.duplicated()].research_id.nunique())\n",
    "\n",
    "# Count number of medication per research_id\n",
    "print(\"Number of medication entries per research_id: \")\n",
    "display(kisim_medication[\"research_id\"].value_counts())\n",
    "\n",
    "# Overlap with diagnoses.csv rid\n",
    "print(\"Overlap with diagnoses.csv rid: \", kisim_medication.research_id.isin(diagnoses_rids).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b4b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDSS Score\n",
    "visits = get_nested_csv(\"seantis\", \"visits.csv\")\n",
    "display(visits.head())\n",
    "\n",
    "# Count visits\n",
    "print(\"Number of visits: \", len(visits))\n",
    "\n",
    "# Count columns\n",
    "print(\"Number of columns: \", len(visits.columns))\n",
    "\n",
    "# Count unique research_ids\n",
    "print(\"Unique research_ids: \", visits.research_id.nunique())\n",
    "\n",
    "# Number of rid that have 2 or more occurences\n",
    "print(\"Number of rids that have 2 or more occurences: \", visits[visits.research_id.duplicated()].research_id.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e61f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visits.csv contains the EDSS score\n",
    "display(visits.edss_score.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5976b8",
   "metadata": {},
   "source": [
    "### Exacerbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe6dee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "exacerbations = get_nested_csv(\"seantis\", \"exacerbations.csv\")\n",
    "display(exacerbations.head())\n",
    "\n",
    "# Count exacerbations\n",
    "print(\"Number of exacerbations: \", len(exacerbations))\n",
    "\n",
    "# Count columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d193d",
   "metadata": {},
   "source": [
    "### MRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ab61df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRI\n",
    "mri = get_nested_csv(\"seantis\", \"magnetic_resonance_images.csv\")\n",
    "display(mri.head())\n",
    "\n",
    "# Count MRI\n",
    "print(\"Number of MRI: \", len(mri))\n",
    "\n",
    "# Count columns\n",
    "print(\"Number of columns: \", len(mri.columns))\n",
    "\n",
    "# Count unique research_ids\n",
    "print(\"Unique research_ids: \", mri.research_id.nunique())\n",
    "\n",
    "# Number of rid that have 2 or more occurences\n",
    "print(\"Number of rids that have 2 or more occurences: \", mri[mri.research_id.duplicated()].research_id.nunique())\n",
    "\n",
    "# Count number of MRI per research_id\n",
    "print(\"Number of MRI per research_id: \")\n",
    "display(mri[\"research_id\"].value_counts())\n",
    "\n",
    "# Overlap with diagnoses.csv rid\n",
    "print(\"Overlap with diagnoses.csv rid: \", mri.research_id.isin(diagnoses_rids).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2986e79b",
   "metadata": {},
   "source": [
    "## My current understanding\n",
    "- Taking the newest import (20210507) is sufficient\n",
    "- I have three sources of input text (seantis/kisim_diagnoses.csv and reports_with_struct_data/reports_kisim_diagnoses.csv) and the reports?\n",
    "- I have labels per text line in the preprocessed_nlp/labelling directory to classify text lines. But these only have the rid and not diagnosis id.\n",
    "\n",
    "## Questions\n",
    "\n",
    "### General\n",
    "- Different imports, are all relevant?\n",
    "- Are the dates listed in the files accuracte (could be used for matching?)\n",
    "- There are a lot of data files, in different directories. Is the original data directory in: dataset/midatams/data? Because there is also some inside the preprocessed_nlp directory and then again in the midata extraction.\n",
    "- What are the keywords you mention for the classifier 2?\n",
    "- Is the seantis data done updating?\n",
    "- The crf classifier uses probability matrices generated from random forrest. I have never seen this approach is there a paper for that? Or a documentation? Or can you explain shortly how you use the RF+CRF pipeline for both classifier 1&2? How does it help?\n",
    "\n",
    "### diagnoses.csv\n",
    "- What is the purpose of diagnoses.csv? Is this the labelling file for MS diagnosis?\n",
    "- Is the diagnosis the \"disease\" column?\n",
    "- Are you just interested in the 3 types of MS in the PDF (10 PPMS, 128 RRMS, 11 SPMS)\n",
    "- Why are there duplicate research_ids? Are there multiple diagnoses per patient?\n",
    "- What should we do if a patient has multiple diagnoses? Take last (from date)? \n",
    "- You used only 149 samples for training for the MS classifier?\n",
    "\n",
    "### kisim_diagnoses.csv\n",
    "- What is the purpose of kisim_diagnosis? Is this the text I should base the prediction on (in diagnosis_label)?\n",
    "- What are the columns of interest here? diagnosis_label?\n",
    "- Diagnosis ranking primary/secondary etc.? \n",
    "- What was labelled in diagnoses.csv? How do I match kisim_diagnosis.csv text to diagnosis.csv?\n",
    "- What if there are multiple texts per rid, did you embed them, then aggregate and classify?\n",
    "\n",
    "### reports_with_struct_data/reports_kisim_diagnoses\n",
    "- Are these identical to the kisim_diagnoses? From the example it seems so. Or do they have overlap?\n",
    "- What are the columns of interest here? diagnosis_label?\n",
    "\n",
    "### Reports\n",
    "- What exactly are the entries in the JSON that are of importance? ZUSATZ? FORM? Visum?\n",
    "- How do I match the reports to diagnosis if there are multiple ids?\n",
    "\n",
    "### seantis/medications.csv, seantis/kisim_medications.csv, rsd/kisim_medications.csv\n",
    "- Are these the labels file for the medication task? Should I predict the medications based on seantis/kisim_diagnoses? or rsd/kisim_rsd_reports?\n",
    "- How do I match the report to the medication label if I only have rids?\n",
    "- Did you do prediction for this or just extracted the labels rule based? Do you want me to construct an approach that is not rule based and can be applied to seantis/medications.csv, seantis/kisim_medications.csv, rsd/kisim_medications.csv to extract medications? Or should I take these files as the labels and try to extract this info from seantis/kisim_diagnoses? or rsd/kisim_rsd_reports\n",
    "\n",
    "### seantis/visits.csv\n",
    "- Does this contain all the info about the edss score? Is this the labels file of the edss score?\n",
    "- What if there are multiple entries per rid? How do I match it to the correct text?\n",
    "\n",
    "### mri\n",
    "- I can see the mri kind (spinal/cranial) but how do I check new/old and KM yes/no?\n",
    "\n",
    "### Labelling\n",
    "- There is a subdirectory preprocessed_nlp/labelling containing labelling for some text. What text exactly was labelled (from seantis/kisim_diagnoses)?\n",
    "- There is only the rid to go off, but how would I match the labelling to patients with multiple diagnoses? How would I match the label to the correct text?\n",
    "\n",
    "### Goal\n",
    "- Do you want me to do predictions based on one text file, or aggregated text files (if multiple per rid) and then give one prediction for MS diagnosis, MRI, medication and edss score?\n",
    "\n",
    "## Tasks:\n",
    "- MS diagnosis: use diagnoses.csv for labels per rid, then predict from seantis/kisim_diagnoses.csv (diagnosis_label) or reports_with_struct_data/reports_kisim_diagnoses.csv (diagnosis_label) or the reports\n",
    "- MRI: use magnetic_resonance_images.csv for labels, then predict from seantis/kisim_diagnoses.csv (diagnosis_label) or reports_with_struct_data/reports_kisim_diagnoses.csv (diagnosis_label) or the reports\n",
    "- Medication: labels in seantis/medications.csv, seantis/kisim_medications.csv, rsd/kisim_medications.csv (need to be extracted rule based) then predict from seantis/kisim_diagnoses.csv (diagnosis_label) or reports_with_struct_data/reports_kisim_diagnoses.csv (diagnosis_label) or the report\n",
    "- EDSS Score: labels in seantis/visits (edss_score) seantis/kisim_diagnoses.csv and reports (extract rule based) then predict from seantis/kisim_diagnoses.csv (diagnosis_label) or reports_with_struct_data/reports_kisim_diagnoses.csv (diagnosis_label) or the reports"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
