{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c836af-5638-484f-a431-c2ee674b762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "\n",
    "from src import paths\n",
    "from src import paths\n",
    "# from src.utils import (load_model_and_tokenizer, \n",
    "#                        check_gpu_memory, \n",
    "# )\n",
    "\n",
    "# import argparse\n",
    "\n",
    "# import torch\n",
    "\n",
    "# from transformers import TrainingArguments\n",
    "\n",
    "# from trl import SFTTrainer\n",
    "\n",
    "# from peft import get_peft_config, get_peft_model\n",
    "\n",
    "# import json\n",
    "\n",
    "\n",
    "# from datasets import Dataset\n",
    "\n",
    "# import bitsandbytes as bnb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52200719-12f0-4666-b670-5a7b44713110",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Llama2-MedTuned-13b\"\n",
    "# NEW_MODEL_NAME = args.new_model_name\n",
    "QUANTIZATION = \"4bit\"\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-05\n",
    "NUM_EPOCHS = 1\n",
    "PEFT_CONFIG = None\n",
    "ATTN_IMPLEMENTATION = None\n",
    "BF16 = False\n",
    "\n",
    "# Check GPU Memory\n",
    "check_gpu_memory()\n",
    "\n",
    "##########################\n",
    "# Model and Tokenizer\n",
    "##########################\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(model_name=MODEL_NAME, \n",
    "                                            quantization=QUANTIZATION, \n",
    "                                            attn_implementation=ATTN_IMPLEMENTATION,\n",
    "                                            task_type=\"clm\")\n",
    "\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1 # recommended for quantized models I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550068a4-242e-4d75-b219-e160a3652347",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614068bc-c9d3-45bf-a983-b9b375a3e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/artidoro/qlora/blob/7f4e95a68dc076bea9b3a413d2b512eca6d004e5/qlora.py#L249\n",
    "def find_all_linear_names(args, model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b5cf4-4e8b-477f-980e-5fab288fc982",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_all_linear_names(\"bla\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd1bc65-d4d2-4170-813c-296ed0cc683d",
   "metadata": {},
   "source": [
    "- https://arxiv.org/abs/2012.14913 talks about number of layers. earlier layers are more for shallow patterns while later layers specialize more in semantic understanding. In paper a model with 16 layers they said layers 1-9 were for shallow detection (so half).\n",
    "    - 40 Decoder Layers; target the first 10 should be enough.\n",
    "- Original Lora Paper (https://arxiv.org/abs/2106.09685). \"Note that putting all the parameters in Wq or Wk results in significantly lower performance, while adapting both Wq and Wv yields the best result (so adapting Key and Query).\n",
    "- QLoRa Paper: \"we find that the most critical LoRA hyperparameter is how many LoRA adapters are used in total and that LoRA on all linear transformer block layers are required to match full finetuning performance. Other LoRA hyperparameters, such as the projection dimension r, do not affect performance\" (https://arxiv.org/pdf/2305.14314.pdf)\n",
    "- As the QLoRa Paper is newer I would go with this one (so target all linear layers) and set the rank lower than I originally did.\n",
    "    - Find all linear layers, and target these\n",
    "    - Set Rank to 4\n",
    "- https://medium.com/@fartypantsham/what-rank-r-and-alpha-to-use-in-lora-in-llm-1b4f025fd133 suggests setting rank = alpha if you don't want to completely change your model. In my case I want to make as little changes as possible. So I try this.\n",
    "\n",
    "Thus config:\n",
    "\n",
    "config = {\n",
    "        \"peft_type\": \"LORA\",\n",
    "        \"r\": 4,\n",
    "        \"lora_alpha\": 4,\n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"bias\":\"none\",\n",
    "        \"task_type\":\"CAUSAL_LM\",\n",
    "        \"target_modules\": ['down_proj', 'o_proj', 'k_proj', 'up_proj', 'v_proj', 'q_proj', 'gate_proj'],\n",
    "        \"layers_to_transform\" : list(range(0, 5)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e3d3fb-a48b-48eb-badf-cb76cb767fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##########################\n",
    "# Data\n",
    "##########################\n",
    "\n",
    "dataset = Dataset.load_from_disk(os.path.join(paths.DATA_PATH_PREPROCESSED, \"text-finetune/kisim_diagnoses\"))\n",
    "\n",
    "##########################\n",
    "# Specifiations\n",
    "##########################\n",
    "\n",
    "# adapted from: https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = os.path.join(paths.MODEL_PATH, \"results\")\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = NUM_EPOCHS\n",
    "\n",
    "# Enable fp16/bf16 training\n",
    "fp16 = False\n",
    "bf16 = BF16\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = BATCH_SIZE\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = LEARNING_RATE\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 50\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 50\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = 128\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = True\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c02de5-705f-4275-a967-e116a51db9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e9a158-ad14-4344-b2b1-b942a7e675ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"peft_type\": \"LORA\",\n",
    "        \"r\": 4,\n",
    "        \"lora_alpha\": 4,\n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"bias\":\"none\",\n",
    "        \"task_type\":\"CAUSAL_LM\",\n",
    "        \"target_modules\": ['down_proj', 'o_proj', 'k_proj', 'up_proj', 'v_proj', 'q_proj', 'gate_proj'],\n",
    "        \"layers_to_transform\" : list(range(5)),\n",
    "    }\n",
    "\n",
    "PEFT_CONFIG = get_peft_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c41e7f-cdc4-4641-b0a9-f653a29999bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PEFT_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c43065b-6e51-4904-951e-6fb8f0cde6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, PEFT_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f98cc2-103c-4353-babf-354005e04825",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a304b-834d-49f8-8fce-79f34b10e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e82d46-f516-4fc4-ad6f-beb693b360f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################\n",
    "# Training\n",
    "##########################\n",
    "\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant':False},\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=PEFT_CONFIG,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "print(\"Starting Training\")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "print(\"Finished Training\")\n",
    "\n",
    "##########################\n",
    "# Saving\n",
    "##########################\n",
    "\n",
    "if NEW_MODEL_NAME is None:\n",
    "    NEW_MODEL_NAME = MODEL_NAME + \"_finetuned\"\n",
    "\n",
    "print(\"Saving Model at:\", paths.MODEL_PATH/NEW_MODEL_NAME)\n",
    "trainer.save_model(paths.MODEL_PATH/NEW_MODEL_NAME)\n",
    "\n",
    "print(\"Saving Tokenizer at:\", paths.MODEL_PATH/NEW_MODEL_NAME)\n",
    "tokenizer.save_pretrained(paths.MODEL_PATH/NEW_MODEL_NAME)\n",
    "\n",
    "print(\"Saving training logs at:\", paths.MODEL_PATH/NEW_MODEL_NAME/\"log_history.pt\")\n",
    "torch.save(trainer.state.log_history, paths.MODEL_PATH/NEW_MODEL_NAME/\"log_history.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ba6756",
   "metadata": {},
   "source": [
    "# Training Progress\n",
    "\n",
    "Loss is CrossEntropy Loss with labels being the tokens shifted by one, so token is predicted by n preceeding tokens. See Source code of LLama:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L1217"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d9cd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib import colors as mcolors\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26cc033",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_1024 = torch.load(paths.MODEL_PATH/\"Llama2-MedTuned-13b-1024-lora/log_history.pt\")\n",
    "logs_512 = torch.load(paths.MODEL_PATH/\"Llama2-MedTuned-13b-512-lora/log_history.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239a89b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_over_epochs(loss_data, title=\"Loss Over Epochs\", save_dir:str = None):\n",
    "    # Convert the data to a DataFrame\n",
    "    df = pd.DataFrame(loss_data)\n",
    "\n",
    "    viridis_cmap = plt.get_cmap('viridis_r')\n",
    "    cmap = sns.color_palette(\"ch:s=.25,rot=-.25\", as_cmap=True)\n",
    "\n",
    "    # Extract a subset of colors from the \"viridis\" colormap\n",
    "    start_index = 120  # Start index of colors to include\n",
    "    end_index = 200 # End index of colors to include\n",
    "    subset_colors = viridis_cmap(np.linspace(start_index / 255, end_index / 255, end_index - start_index + 1))\n",
    "\n",
    "    # Create a custom colormap using the subset of colors\n",
    "    custom_cmap = mcolors.ListedColormap(subset_colors)\n",
    "    custom_cmap = sns.color_palette(\"light:#5A9\", as_cmap=True)\n",
    "\n",
    "\n",
    "    # Create the plot\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.lineplot(x='epoch', y='loss', data=df, marker='o', color = custom_cmap(400), linewidth=2.5, markersize=8)\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set(xlabel='Epoch', ylabel='Loss', title=title)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.ylabel(\"CE-Loss\", fontsize=20, fontweight='bold', labelpad=15)\n",
    "    plt.xlabel(\"Epoch\", fontsize=20, fontweight='bold', labelpad=15)\n",
    "    plt.ylim(2.2, 3.1)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_dir:\n",
    "        plt.savefig(save_dir, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c81697",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_over_epochs(logs_1024[:-1], title=\"\", save_dir = paths.THESIS_PATH/\"llama2-finetuning-1024.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2206ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_over_epochs(logs_512[:-1], title=\"\", save_dir = paths.THESIS_PATH/\"llama2-finetuning-512.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
