{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, BitsAndBytesConfig, TrainingArguments, Trainer, get_linear_schedule_with_warmup, pipeline\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "\n",
    "from peft import get_peft_config, get_peft_model, prepare_model_for_kbit_training, PeftConfig, PromptEncoderConfig, LoraConfig, PeftModel, PromptTuningConfig, PromptTuningInit\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "\n",
    "from src import paths\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        for gpu_id in range(num_gpus):\n",
    "            free_mem, total_mem = torch.cuda.mem_get_info(gpu_id)\n",
    "            gpu_properties = torch.cuda.get_device_properties(gpu_id)\n",
    "            print(f\"GPU {gpu_id}: {gpu_properties.name}\")\n",
    "            print(f\"   Total Memory: {total_mem / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Free Memory: {free_mem / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Allocated Memory : {torch.cuda.memory_allocated(gpu_id) / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Reserved Memory : {torch.cuda.memory_reserved(gpu_id) / (1024 ** 3):.2f} GB\")\n",
    "    else:\n",
    "        print(\"No GPU available.\")\n",
    "\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "def get_artifical_data_for_label(label:str):\n",
    "    label_dict = {\n",
    "        \"rrms\": \"relapsing_remitting_multiple_sclerosis\",\n",
    "        \"ppms\": \"primary_progressive_multiple_sclerosis\",\n",
    "        \"spms\": \"secondary_progressive_multiple_sclerosis\"\n",
    "    }\n",
    "    generated_data = pd.read_csv(paths.DATA_PATH_PREPROCESSED/f'ms-diag/artificial_{label}.csv')\n",
    "    generated_data[\"labels\"] = label_dict[label]\n",
    "    generated_data = generated_data[[\"0\", \"labels\"]].rename(columns = {\"0\":\"text\"})\n",
    "\n",
    "    return generated_data\n",
    "\n",
    "def get_artifical_data_all():\n",
    "    artifical_data = []\n",
    "    for label in [\"rrms\", \"ppms\", \"spms\"]:\n",
    "        try: \n",
    "            artifical_data.append(get_artifical_data_for_label(label))\n",
    "        except:\n",
    "            print(f\"Could not find data for {label}\")\n",
    "    artifical_data = pd.concat(artifical_data)\n",
    "    artifical_data = Dataset.from_pandas(artifical_data).remove_columns('__index_level_0__')\n",
    "    return artifical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_files = {\"train\": \"ms-diag_clean_train.csv\", \"validation\": \"ms-diag_clean_val.csv\", \"test\": \"ms-diag_clean_test.csv\", \"augmented\": \"ms-diag_augmented.csv\"}\n",
    "df = load_dataset(os.path.join(paths.DATA_PATH_PREPROCESSED,'ms-diag'), data_files = data_files)\n",
    "\n",
    "# Samples Generated from medBERT masked\n",
    "df[\"train\"] = concatenate_datasets([df[\"augmented\"], df[\"train\"]])\n",
    "\n",
    "# Samples Generated from Zero Shot\n",
    "# df[\"train\"] = concatenate_datasets([get_artifical_data_all(), df[\"train\"]])\n",
    "\n",
    "# # Oversampling\n",
    "# minority_classes = df['train'].filter(lambda example: example['labels'] != 'relapsing_remitting_multiple_sclerosis')\n",
    "# df[\"train\"] = concatenate_datasets(9*[minority_classes] + [df['train']])\n",
    "\n",
    "\n",
    "# Number of labels\n",
    "num_labels = len(set(df['train']['labels']))\n",
    "\n",
    "# Label to id\n",
    "label2id = {'primary_progressive_multiple_sclerosis': 0,\n",
    "            'relapsing_remitting_multiple_sclerosis': 1,\n",
    "            'secondary_progressive_multiple_sclerosis': 2}\n",
    "id2label = {v:k for k,v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low precision config\n",
    "print(\"Memory before Model is loaded:\\n\")\n",
    "check_gpu_memory()\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(paths.MODEL_PATH/'llama2', \n",
    "                                                           device_map=\"auto\", \n",
    "                                                           quantization_config=bnb_config,\n",
    "                                                          num_labels = num_labels)\n",
    "print(\"Memory after Model is loaded:\\n\")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(paths.MODEL_PATH/'llama2', padding_side='left')\n",
    "print(\"Vocabulary Size without Pad Token: \", len(tokenizer))\n",
    "\n",
    "# Check if the pad token is already in the tokenizer vocabulary\n",
    "if '<pad>' not in tokenizer.get_vocab():\n",
    "    # Add the pad token\n",
    "    tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "\n",
    "#Resize the embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#Configure the pad token in the model\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Check if they are equal\n",
    "assert model.config.pad_token_id == tokenizer.pad_token_id, \"The model's pad token ID does not match the tokenizer's pad token ID!\"\n",
    "\n",
    "# Print the pad token ids\n",
    "print('Tokenizer pad token ID:', tokenizer.pad_token_id)\n",
    "print('Model pad token ID:', model.config.pad_token_id)\n",
    "print('Model config pad token ID:', model.config.pad_token_id)\n",
    "print(\"Vocabulary Size with Pad Token: \", len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft_config = PromptEncoderConfig(task_type=\"SEQ_CLS\", num_virtual_tokens=20, encoder_hidden_size=128, encoder_dropout=0.1)\n",
    "# model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# print(peft_config)\n",
    "# peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=20,\n",
    "    prompt_tuning_init_text=\"Klassifiziere als primär, sekundär oder schubförmige MS\",\n",
    "    tokenizer_name_or_path=paths.MODEL_PATH/'llama2-chat',\n",
    ")\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# peft_config = LoraConfig(\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0.1,\n",
    "#     r=8,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"SEQ_CLS\"\n",
    "# )\n",
    "# peft_model = get_peft_model(model, peft_config)\n",
    "# peft_model.to(device)\n",
    "# check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For quantized training need to prepare model\n",
    "# model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# config = {\n",
    "#     \"peft_type\": \"PREFIX_TUNING\",\n",
    "#     \"task_type\": \"SEQ_CLS\",\n",
    "#     \"inference_mode\": False,\n",
    "#     \"num_virtual_tokens\": 0,\n",
    "#     \"token_dim\": model.config.hidden_size,\n",
    "#     \"num_transformer_submodules\": 1,\n",
    "#     \"num_attention_heads\": model.config.num_attention_heads,\n",
    "#     \"num_layers\": model.config.num_hidden_layers,\n",
    "#     \"encoder_hidden_size\": 128,\n",
    "#     \"prefix_projection\": True,\n",
    "# }\n",
    "# peft_config = get_peft_config(config)\n",
    "# print(peft_config)\n",
    "# peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainable parameters\n",
    "peft_model.print_trainable_parameters()\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, \"Shape:\",param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare dataset\n",
    "# if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "#     tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # We need space for the prefixes, so if the sequence is longer/equal than the max model length we need to truncate to tokenizer.model_max_length - peft_config.num_virtual_tokens\n",
    "    outputs = tokenizer(examples[\"text\"], truncation=True, max_length=256)\n",
    "    outputs[\"labels\"] = [label2id[label] for label in examples[\"labels\"]]\n",
    "    return outputs\n",
    "\n",
    "encoded_dataset = df.map(tokenize_function, batched=True, remove_columns=[\"text\", \"rid\", \"date\"])\n",
    "# # Small train dataset with balanced classes\n",
    "# rmms_id = label2id['relapsing_remitting_multiple_sclerosis']\n",
    "# small_train_dataset = concatenate_datasets([encoded_dataset['train'].filter(lambda example: example['labels'] != rmms_id), encoded_dataset['train'].filter(lambda example: example['labels'] == rmms_id).shuffle(seed=42).select(range(10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "train_batch_size = 8\n",
    "eval_batch_size = 8\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 8\n",
    "gradient_accumulation_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Small train dataset with balanced classes\n",
    "# rmms_id = label2id['relapsing_remitting_multiple_sclerosis']\n",
    "# small_train_dataset = concatenate_datasets([encoded_dataset['train'].filter(lambda example: example['labels'] != rmms_id), encoded_dataset['train'].filter(lambda example: example['labels'] == rmms_id).shuffle(seed=42).select(range(10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding=\"longest\")\n",
    "\n",
    "# DataLoaders creation\n",
    "train_dataloader = DataLoader(\n",
    "    encoded_dataset[\"train\"], shuffle=True, collate_fn=data_collator, batch_size=train_batch_size,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    encoded_dataset[\"validation\"], shuffle=False, collate_fn=data_collator, batch_size=eval_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(peft_model.parameters(), lr=learning_rate)\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")\n",
    "\n",
    "# # Loss\n",
    "# # Class weights\n",
    "# power = 1\n",
    "# class_weights = [1/np.log(len(encoded_dataset['train'].filter(lambda example: example['labels'] == label))) for label in set(encoded_dataset['train']['labels'])]\n",
    "# class_weights = [weight**power for weight in class_weights]\n",
    "# class_weights = torch.tensor(class_weights, dtype=model.dtype).detach().to(device)\n",
    "\n",
    "\n",
    "# loss_fun = torch.nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Training\n",
    "peft_model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    peft_model.train()\n",
    "    total_loss = 0\n",
    "    bar = tqdm(train_dataloader)\n",
    "    for step, batch in enumerate(bar):\n",
    "        optimizer.zero_grad()\n",
    "        batch.to(device)\n",
    "        outputs = peft_model(**batch)\n",
    "        logits = outputs.logits\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "        bar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    peft_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        eval_loss = 0\n",
    "        eval_preds = []\n",
    "        labels = []\n",
    "        for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "            batch.to(device)\n",
    "            outputs = peft_model(**batch)\n",
    "                \n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            eval_preds.extend(predictions.tolist())\n",
    "            labels.extend(batch['labels'].tolist())\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.detach().float()\n",
    "            \n",
    "    f1 = f1_score(labels, eval_preds, average='macro')\n",
    "\n",
    "    if epoch == 0:\n",
    "        max_f1 = 0\n",
    "        min_eval_loss = eval_loss\n",
    "        peft_model.save_pretrained(paths.MODEL_PATH/'peft-prompt_llama2_msdiag')\n",
    "    elif f1 > max_f1:\n",
    "        max_f1 = f1\n",
    "        if eval_loss < min_eval_loss:\n",
    "            min_eval_loss = eval_loss\n",
    "        print(\"Saved\")\n",
    "        peft_model.save_pretrained(paths.MODEL_PATH/'peft-prompt_llama2_msdiag')\n",
    "    elif (f1 == max_f1) and (eval_loss < min_eval_loss):\n",
    "        print(\"Saved\")\n",
    "        peft_model.save_pretrained(paths.MODEL_PATH/'peft-prompt_llama2_msdiag')\n",
    "        \n",
    "    eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"{epoch=}: {train_epoch_loss=} {eval_epoch_loss=} {f1=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft_model.save_pretrained(paths.MODEL_PATH/'msdiag_llama2_peft_lora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on test set\n",
    "peft_model.eval()\n",
    "test_dataloader = DataLoader(\n",
    "    encoded_dataset[\"test\"], shuffle=False, collate_fn=data_collator, batch_size=1\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "        test_preds = []\n",
    "        labels = []\n",
    "        for step, batch in enumerate(tqdm(test_dataloader)):\n",
    "            batch.to(device)\n",
    "            outputs = peft_model(**batch)\n",
    "                \n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            test_preds.extend(predictions.tolist())\n",
    "            \n",
    "            labels.extend(batch['labels'].tolist())            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn F1, precision, recall, accuracy\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, ConfusionMatrixDisplay\n",
    "print(f\"Accuracy: {accuracy_score(encoded_dataset['test']['labels'], test_preds)}\")\n",
    "print(f\"Precision: {precision_score(encoded_dataset['test']['labels'], test_preds, average='weighted')}\")\n",
    "print(f\"Recall: {recall_score(encoded_dataset['test']['labels'], test_preds, average='weighted')}\")\n",
    "print(f\"F1: {f1_score(encoded_dataset['test']['labels'], test_preds, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_predictions(encoded_dataset[\"test\"][\"labels\"], test_preds, display_labels=label2id, xticks_rotation=\"vertical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv(os.path.join(paths.DATA_PATH_PREPROCESSED,'ms-diag/ms-diag_clean_test.csv'))\n",
    "for i, label in enumerate(labels):\n",
    "    if label != test_preds[i]:\n",
    "        print(\"Entry: \", i)\n",
    "        print(\"Label\", id2label[label])\n",
    "        print(\"Predicted\", id2label[test_preds[i]])\n",
    "        print(\"Text: \", df[\"test\"][i][\"text\"])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "# config = PeftConfig.from_pretrained(paths.MODEL_PATH/'peft_llama2_msdiag')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class PromptLearningConfig(PeftConfig):\n",
    "    \"\"\"\n",
    "    This is the base configuration class to store the configuration of [`PrefixTuning`], [`PromptEncoder`], or\n",
    "    [`PromptTuning`].\n",
    "\n",
    "    Args:\n",
    "        num_virtual_tokens (`int`): The number of virtual tokens to use.\n",
    "        token_dim (`int`): The hidden embedding dimension of the base transformer model.\n",
    "        num_transformer_submodules (`int`): The number of transformer submodules in the base transformer model.\n",
    "        num_attention_heads (`int`): The number of attention heads in the base transformer model.\n",
    "        num_layers (`int`): The number of layers in the base transformer model.\n",
    "    \"\"\"\n",
    "\n",
    "    num_virtual_tokens: int = field(default=None, metadata={\"help\": \"Number of virtual tokens\"})\n",
    "    token_dim: int = field(\n",
    "        default=None, metadata={\"help\": \"The hidden embedding dimension of the base transformer model\"}\n",
    "    )\n",
    "    num_transformer_submodules: Optional[int] = field(\n",
    "        default=None, metadata={\"help\": \"Number of transformer submodules\"}\n",
    "    )\n",
    "    num_attention_heads: Optional[int] = field(default=None, metadata={\"help\": \"Number of attention heads\"})\n",
    "    num_layers: Optional[int] = field(default=None, metadata={\"help\": \"Number of transformer layers\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "configuratio interpretation:\n",
    "- num_virtual_tokens: number of prefix tokens to append to (each layer?)\n",
    "- num_transformer_submodules: 1 or 2? for decoder/encoder? or is it number of layers? I think decoder encoder architecture because if it is not set, they set it to 1 except in a Seq2Seq task. And they create embeddings for the virtual tokens only once for each submodule?\n",
    "- num_layers: to how many layers we should add the prefix tuning?\n",
    "- The peft model will automatically try to infer all the model specification from model.config in the `get_peft_model()` call. It will also automatically construct the PEFT Model class from the task type: return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model, peft_config, adapter_name=adapter_name)\n",
    "- you can add multiple adapters (must be all of same peft type though) to a mode each with their own configurations. You must give them unique names, otherwise gives name \"default\". Each prompt encoding and the whole process will be done for every single adapter I think. It first prepares the peft config by matching non specified elements with the model config. It then sets up the prompt encoder (_set_up_prompt_encoder(adapter_name)) using the created config for the adapter\n",
    "- If specification is PROMPT_TUNING we just create a trainable word embeddings matrix. By inserting an init text you can initialize the embedding from the base tokenizer and base model. \n",
    "- I think the prompt_token ids are initialized separately from the existing ids for the tokenizer. Because we have a different embedding matrix we can initialize from 0 to 19 say and then put it through, then concatenate the weights again to the embeddings of the base model without the virtual tokens. As they don't have any meaning and we will always have the same order of them (first will be virtual token 0, then 1 etc.) we also don't really neew to match them in the forward pass order will always be same.\n",
    "\n",
    "``` python\n",
    "if config.num_transformer_submodules is None:\n",
    "            config.num_transformer_submodules = 2 if config.task_type == TaskType.SEQ_2_SEQ_LM else 1\n",
    "total_virtual_tokens = config.num_virtual_tokens * config.num_transformer_submodules\n",
    "```\n",
    "\n",
    "```\n",
    "def _prepare_prompt_learning_config(peft_config, model_config):\n",
    "    if peft_config.num_layers is None:\n",
    "        if \"num_hidden_layers\" in model_config:\n",
    "            num_layers = model_config[\"num_hidden_layers\"]\n",
    "        elif \"num_layers\" in model_config:\n",
    "            num_layers = model_config[\"num_layers\"]\n",
    "        elif \"n_layer\" in model_config:\n",
    "            num_layers = model_config[\"n_layer\"]\n",
    "        else:\n",
    "            raise ValueError(\"Please specify `num_layers` in `peft_config`\")\n",
    "        peft_config.num_layers = num_layers\n",
    "\n",
    "    if peft_config.token_dim is None:\n",
    "        if \"hidden_size\" in model_config:\n",
    "            token_dim = model_config[\"hidden_size\"]\n",
    "        elif \"n_embd\" in model_config:\n",
    "            token_dim = model_config[\"n_embd\"]\n",
    "        elif \"d_model\" in model_config:\n",
    "            token_dim = model_config[\"d_model\"]\n",
    "        else:\n",
    "            raise ValueError(\"Please specify `token_dim` in `peft_config`\")\n",
    "        peft_config.token_dim = token_dim\n",
    "\n",
    "    if peft_config.num_attention_heads is None:\n",
    "        if \"num_attention_heads\" in model_config:\n",
    "            num_attention_heads = model_config[\"num_attention_heads\"]\n",
    "        elif \"n_head\" in model_config:\n",
    "            num_attention_heads = model_config[\"n_head\"]\n",
    "        elif \"num_heads\" in model_config:\n",
    "            num_attention_heads = model_config[\"num_heads\"]\n",
    "        elif \"encoder_attention_heads\" in model_config:\n",
    "            num_attention_heads = model_config[\"encoder_attention_heads\"]\n",
    "        else:\n",
    "            raise ValueError(\"Please specify `num_attention_heads` in `peft_config`\")\n",
    "        peft_config.num_attention_heads = num_attention_heads\n",
    "\n",
    "    if getattr(peft_config, \"encoder_hidden_size\", None) is None:\n",
    "        setattr(peft_config, \"encoder_hidden_size\", peft_config.token_dim)\n",
    "\n",
    "    return peft_config\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prefix tuning forward:\n",
    "- The prefixes are inserted into the model using the past_key_values which is the cached keys and values of the whole sequence before the current token in the autoregressive generation procedure. Instead of computing these keys and values with a model forward pass, we simply compute them directly with our MLP model. This is also why we need the number of attention heads (as we need a pair for every head).\n",
    "- The prompt encoder returns num_layers * 2 * token_dim past key values, so for every layer a key and value. Reading this one could think only \"one token\" (one key value pair) is generated. But later this is reshaped into:\n",
    "\n",
    "past_key_values = past_key_values.view(\n",
    "                batch_size,\n",
    "                peft_config.num_virtual_tokens,\n",
    "                peft_config.num_layers * 2,\n",
    "                peft_config.num_attention_heads,\n",
    "                peft_config.token_dim // peft_config.num_attention_heads,\n",
    "            )\n",
    "\n",
    "so we again have num_virtual_tokens as a dimension. This matrix is then permuted and split which I don't know the specifics of but it ensures that it is in the correct format.\n",
    "\n",
    "def _prefix_tuning_forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        batch_size = _get_batch_size(input_ids, inputs_embeds)\n",
    "        past_key_values = self.get_prompt(batch_size)\n",
    "        fwd_params = list(inspect.signature(self.base_model.forward).parameters.keys())\n",
    "        kwargs.update(\n",
    "            {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"inputs_embeds\": inputs_embeds,\n",
    "                \"output_attentions\": output_attentions,\n",
    "                \"output_hidden_states\": output_hidden_states,\n",
    "                \"return_dict\": return_dict,\n",
    "                \"past_key_values\": past_key_values,\n",
    "            }\n",
    "        )\n",
    "        if \"past_key_values\" in fwd_params:\n",
    "            return self.base_model(labels=labels, **kwargs)\n",
    "        else:\n",
    "            transformer_backbone_name = self.base_model.get_submodule(self.transformer_backbone_name)\n",
    "            fwd_params = list(inspect.signature(transformer_backbone_name.forward).parameters.keys())\n",
    "            if \"past_key_values\" not in fwd_params:\n",
    "                raise ValueError(\"Model does not support past key values which are required for prefix tuning.\")\n",
    "            outputs = transformer_backbone_name(**kwargs)\n",
    "            pooled_output = outputs[1] if len(outputs) > 1 else outputs[0]\n",
    "            if \"dropout\" in [name for name, _ in list(self.base_model.named_children())]:\n",
    "                pooled_output = self.base_model.dropout(pooled_output)\n",
    "            logits = self.base_model.get_submodule(self.cls_layer_name)(pooled_output)\n",
    "\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                if self.config.problem_type is None:\n",
    "                    if self.base_model.num_labels == 1:\n",
    "                        self.config.problem_type = \"regression\"\n",
    "                    elif self.base_model.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                        self.config.problem_type = \"single_label_classification\"\n",
    "                    else:\n",
    "                        self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "                if self.config.problem_type == \"regression\":\n",
    "                    loss_fct = MSELoss()\n",
    "                    if self.base_model.num_labels == 1:\n",
    "                        loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                    else:\n",
    "                        loss = loss_fct(logits, labels)\n",
    "                elif self.config.problem_type == \"single_label_classification\":\n",
    "                    loss_fct = CrossEntropyLoss()\n",
    "                    loss = loss_fct(logits.view(-1, self.base_model.num_labels), labels.view(-1))\n",
    "                elif self.config.problem_type == \"multi_label_classification\":\n",
    "                    loss_fct = BCEWithLogitsLoss()\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            if not return_dict:\n",
    "                output = (logits,) + outputs[2:]\n",
    "                return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "            return SequenceClassifierOutput(\n",
    "                loss=loss,\n",
    "                logits=logits,\n",
    "                hidden_states=outputs.hidden_states,\n",
    "                attentions=outputs.attentions,\n",
    "            )\n",
    "\n",
    "def get_prompt(self, batch_size: int, task_ids: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the virtual prompts to use for Peft. Only applicable when using a prompt learning method.\n",
    "        \"\"\"\n",
    "        peft_config = self.active_peft_config\n",
    "        prompt_encoder = self.prompt_encoder[self.active_adapter]\n",
    "        prompt_tokens = (\n",
    "            self.prompt_tokens[self.active_adapter]\n",
    "            .unsqueeze(0)\n",
    "            .expand(batch_size, -1)\n",
    "            .to(prompt_encoder.embedding.weight.device)\n",
    "        )\n",
    "        if peft_config.peft_type == PeftType.PREFIX_TUNING:\n",
    "            prompt_tokens = prompt_tokens[:, : peft_config.num_virtual_tokens]\n",
    "            if peft_config.inference_mode:\n",
    "                past_key_values = prompt_encoder.embedding.weight.repeat(batch_size, 1, 1)\n",
    "            else:\n",
    "                past_key_values = prompt_encoder(prompt_tokens)\n",
    "            if self.base_model_torch_dtype is not None:\n",
    "                past_key_values = past_key_values.to(self.base_model_torch_dtype)\n",
    "            past_key_values = past_key_values.view(\n",
    "                batch_size,\n",
    "                peft_config.num_virtual_tokens,\n",
    "                peft_config.num_layers * 2,\n",
    "                peft_config.num_attention_heads,\n",
    "                peft_config.token_dim // peft_config.num_attention_heads,\n",
    "            )\n",
    "            if peft_config.num_transformer_submodules == 2:\n",
    "                past_key_values = torch.cat([past_key_values, past_key_values], dim=2)\n",
    "            past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(\n",
    "                peft_config.num_transformer_submodules * 2\n",
    "            )\n",
    "            if TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING.get(self.config.model_type, None) is not None:\n",
    "                post_process_fn = TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING[self.config.model_type]\n",
    "                past_key_values = post_process_fn(past_key_values)\n",
    "            return past_key_values\n",
    "        else:\n",
    "            if peft_config.peft_type == PeftType.MULTITASK_PROMPT_TUNING:\n",
    "                prompts = prompt_encoder(prompt_tokens, task_ids)\n",
    "            else:\n",
    "                if peft_config.inference_mode:\n",
    "                    prompts = prompt_encoder.embedding.weight.repeat(batch_size, 1, 1)\n",
    "                else:\n",
    "                    prompts = prompt_encoder(prompt_tokens)\n",
    "            return prompts\n",
    "\n",
    "\n",
    "class PrefixEncoder(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    The `torch.nn` model to encode the prefix.\n",
    "\n",
    "    Args:\n",
    "        config ([`PrefixTuningConfig`]): The configuration of the prefix encoder.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    ```py\n",
    "    >>> from peft import PrefixEncoder, PrefixTuningConfig\n",
    "\n",
    "    >>> config = PrefixTuningConfig(\n",
    "    ...     peft_type=\"PREFIX_TUNING\",\n",
    "    ...     task_type=\"SEQ_2_SEQ_LM\",\n",
    "    ...     num_virtual_tokens=20,\n",
    "    ...     token_dim=768,\n",
    "    ...     num_transformer_submodules=1,\n",
    "    ...     num_attention_heads=12,\n",
    "    ...     num_layers=12,\n",
    "    ...     encoder_hidden_size=768,\n",
    "    ... )\n",
    "    >>> prefix_encoder = PrefixEncoder(config)\n",
    "    ```\n",
    "\n",
    "    **Attributes**:\n",
    "        - **embedding** (`torch.nn.Embedding`) -- The embedding layer of the prefix encoder.\n",
    "        - **transform** (`torch.nn.Sequential`) -- The two-layer MLP to transform the prefix embeddings if\n",
    "          `prefix_projection` is `True`.\n",
    "        - **prefix_projection** (`bool`) -- Whether to project the prefix embeddings.\n",
    "\n",
    "    Input shape: (`batch_size`, `num_virtual_tokens`)\n",
    "\n",
    "    Output shape: (`batch_size`, `num_virtual_tokens`, `2*layers*hidden`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.prefix_projection = config.prefix_projection\n",
    "        token_dim = config.token_dim\n",
    "        num_layers = config.num_layers\n",
    "        encoder_hidden_size = config.encoder_hidden_size\n",
    "        num_virtual_tokens = config.num_virtual_tokens\n",
    "        if self.prefix_projection and not config.inference_mode:\n",
    "            # Use a two-layer MLP to encode the prefix\n",
    "            self.embedding = torch.nn.Embedding(num_virtual_tokens, token_dim)\n",
    "            self.transform = torch.nn.Sequential(\n",
    "                torch.nn.Linear(token_dim, encoder_hidden_size),\n",
    "                torch.nn.Tanh(),\n",
    "                torch.nn.Linear(encoder_hidden_size, num_layers * 2 * token_dim),\n",
    "            )\n",
    "        else:\n",
    "            self.embedding = torch.nn.Embedding(num_virtual_tokens, num_layers * 2 * token_dim)\n",
    "\n",
    "    def forward(self, prefix: torch.Tensor):\n",
    "        if self.prefix_projection:\n",
    "            prefix_tokens = self.embedding(prefix)\n",
    "            past_key_values = self.transform(prefix_tokens)\n",
    "        else:\n",
    "            past_key_values = self.embedding(prefix)\n",
    "        return past_key_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
