{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, BitsAndBytesConfig, TrainingArguments, Trainer, get_linear_schedule_with_warmup, pipeline\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "\n",
    "from peft import get_peft_config, get_peft_model, prepare_model_for_kbit_training, PeftConfig, PromptEncoderConfig, LoraConfig, PeftModel, PromptTuningConfig, PromptTuningInit\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "\n",
    "from src import paths\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 10.20 GB\n",
      "   Allocated Memory : 0.00 GB\n",
      "   Reserved Memory : 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        for gpu_id in range(num_gpus):\n",
    "            free_mem, total_mem = torch.cuda.mem_get_info(gpu_id)\n",
    "            gpu_properties = torch.cuda.get_device_properties(gpu_id)\n",
    "            print(f\"GPU {gpu_id}: {gpu_properties.name}\")\n",
    "            print(f\"   Total Memory: {total_mem / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Free Memory: {free_mem / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Allocated Memory : {torch.cuda.memory_allocated(gpu_id) / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Reserved Memory : {torch.cuda.memory_reserved(gpu_id) / (1024 ** 3):.2f} GB\")\n",
    "    else:\n",
    "        print(\"No GPU available.\")\n",
    "\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "def get_artifical_data_for_label(label:str):\n",
    "    label_dict = {\n",
    "        \"rrms\": \"relapsing_remitting_multiple_sclerosis\",\n",
    "        \"ppms\": \"primary_progressive_multiple_sclerosis\",\n",
    "        \"spms\": \"secondary_progressive_multiple_sclerosis\"\n",
    "    }\n",
    "    generated_data = pd.read_csv(paths.DATA_PATH_PREPROCESSED/f'ms-diag/artificial_{label}.csv')\n",
    "    generated_data[\"labels\"] = label_dict[label]\n",
    "    generated_data = generated_data[[\"0\", \"labels\"]].rename(columns = {\"0\":\"text\"})\n",
    "\n",
    "    return generated_data\n",
    "\n",
    "def get_artifical_data_all():\n",
    "    artifical_data = []\n",
    "    for label in [\"rrms\", \"ppms\", \"spms\"]:\n",
    "        try: \n",
    "            artifical_data.append(get_artifical_data_for_label(label))\n",
    "        except:\n",
    "            print(f\"Could not find data for {label}\")\n",
    "    artifical_data = pd.concat(artifical_data)\n",
    "    artifical_data = Dataset.from_pandas(artifical_data).remove_columns('__index_level_0__')\n",
    "    return artifical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_files = {\"train\": \"ms-diag_clean_train.csv\", \"validation\": \"ms-diag_clean_val.csv\", \"test\": \"ms-diag_clean_test.csv\", \"augmented\": \"ms-diag_augmented.csv\"}\n",
    "df = load_dataset(os.path.join(paths.DATA_PATH_PREPROCESSED,'ms-diag'), data_files = data_files)\n",
    "\n",
    "# Samples Generated from medBERT masked\n",
    "df[\"train\"] = concatenate_datasets([df[\"augmented\"], df[\"train\"]])\n",
    "\n",
    "# Samples Generated from Zero Shot\n",
    "# df[\"train\"] = concatenate_datasets([get_artifical_data_all(), df[\"train\"]])\n",
    "\n",
    "# # Oversampling\n",
    "# minority_classes = df['train'].filter(lambda example: example['labels'] != 'relapsing_remitting_multiple_sclerosis')\n",
    "# df[\"train\"] = concatenate_datasets(9*[minority_classes] + [df['train']])\n",
    "\n",
    "\n",
    "# Number of labels\n",
    "num_labels = len(set(df['train']['labels']))\n",
    "\n",
    "# Label to id\n",
    "label2id = {'primary_progressive_multiple_sclerosis': 0,\n",
    "            'relapsing_remitting_multiple_sclerosis': 1,\n",
    "            'secondary_progressive_multiple_sclerosis': 2}\n",
    "id2label = {v:k for k,v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before Model is loaded:\n",
      "\n",
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 10.20 GB\n",
      "   Allocated Memory : 0.00 GB\n",
      "   Reserved Memory : 0.00 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7440ac27e9ac45f9a7994c407f51c9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /cluster/dataset/midatams/inf-extr/resources/models/llama2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory after Model is loaded:\n",
      "\n",
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 6.55 GB\n",
      "   Allocated Memory : 3.45 GB\n",
      "   Reserved Memory : 3.64 GB\n"
     ]
    }
   ],
   "source": [
    "# Low precision config\n",
    "print(\"Memory before Model is loaded:\\n\")\n",
    "check_gpu_memory()\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(paths.MODEL_PATH/'llama2', \n",
    "                                                           device_map=\"auto\", \n",
    "                                                           quantization_config=bnb_config,\n",
    "                                                          num_labels = num_labels)\n",
    "print(\"Memory after Model is loaded:\\n\")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size without Pad Token:  32000\n",
      "Tokenizer pad token ID: 32000\n",
      "Model pad token ID: 32000\n",
      "Model config pad token ID: 32000\n",
      "Vocabulary Size with Pad Token:  32001\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(paths.MODEL_PATH/'llama2', padding_side='left')\n",
    "print(\"Vocabulary Size without Pad Token: \", len(tokenizer))\n",
    "\n",
    "# Check if the pad token is already in the tokenizer vocabulary\n",
    "if '<pad>' not in tokenizer.get_vocab():\n",
    "    # Add the pad token\n",
    "    tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "\n",
    "#Resize the embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#Configure the pad token in the model\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Check if they are equal\n",
    "assert model.config.pad_token_id == tokenizer.pad_token_id, \"The model's pad token ID does not match the tokenizer's pad token ID!\"\n",
    "\n",
    "# Print the pad token ids\n",
    "print('Tokenizer pad token ID:', tokenizer.pad_token_id)\n",
    "print('Model pad token ID:', model.config.pad_token_id)\n",
    "print('Model config pad token ID:', model.config.pad_token_id)\n",
    "print(\"Vocabulary Size with Pad Token: \", len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft_config = PromptEncoderConfig(task_type=\"SEQ_CLS\", num_virtual_tokens=20, encoder_hidden_size=128, encoder_dropout=0.1)\n",
    "# model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# print(peft_config)\n",
    "# peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 5.32 GB\n",
      "   Allocated Memory : 4.08 GB\n",
      "   Reserved Memory : 4.67 GB\n"
     ]
    }
   ],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=20,\n",
    "    prompt_tuning_init_text=\"Klassifiziere als primär, sekundär oder schubförmige MS\",\n",
    "    tokenizer_name_or_path=paths.MODEL_PATH/'llama2-chat',\n",
    ")\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# peft_config = LoraConfig(\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0.1,\n",
    "#     r=8,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"SEQ_CLS\"\n",
    "# )\n",
    "# peft_model = get_peft_model(model, peft_config)\n",
    "# peft_model.to(device)\n",
    "# check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For quantized training need to prepare model\n",
    "# model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# config = {\n",
    "#     \"peft_type\": \"PREFIX_TUNING\",\n",
    "#     \"task_type\": \"SEQ_CLS\",\n",
    "#     \"inference_mode\": False,\n",
    "#     \"num_virtual_tokens\": 0,\n",
    "#     \"token_dim\": model.config.hidden_size,\n",
    "#     \"num_transformer_submodules\": 1,\n",
    "#     \"num_attention_heads\": model.config.num_attention_heads,\n",
    "#     \"num_layers\": model.config.num_hidden_layers,\n",
    "#     \"encoder_hidden_size\": 128,\n",
    "#     \"prefix_projection\": True,\n",
    "# }\n",
    "# peft_config = get_peft_config(config)\n",
    "# print(peft_config)\n",
    "# peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 94,208 || all params: 6,611,648,512 || trainable%: 0.001424879133078755\n",
      "score.modules_to_save.default.weight Shape: torch.Size([3, 4096])\n"
     ]
    }
   ],
   "source": [
    "# Trainable parameters\n",
    "peft_model.print_trainable_parameters()\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, \"Shape:\",param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare dataset\n",
    "# if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "#     tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # We need space for the prefixes, so if the sequence is longer/equal than the max model length we need to truncate to tokenizer.model_max_length - peft_config.num_virtual_tokens\n",
    "    outputs = tokenizer(examples[\"text\"], truncation=True, max_length=256)\n",
    "    outputs[\"labels\"] = [label2id[label] for label in examples[\"labels\"]]\n",
    "    return outputs\n",
    "\n",
    "encoded_dataset = df.map(tokenize_function, batched=True, remove_columns=[\"text\", \"rid\", \"date\"])\n",
    "# # Small train dataset with balanced classes\n",
    "# rmms_id = label2id['relapsing_remitting_multiple_sclerosis']\n",
    "# small_train_dataset = concatenate_datasets([encoded_dataset['train'].filter(lambda example: example['labels'] != rmms_id), encoded_dataset['train'].filter(lambda example: example['labels'] == rmms_id).shuffle(seed=42).select(range(10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "train_batch_size = 8\n",
    "eval_batch_size = 8\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 8\n",
    "gradient_accumulation_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Small train dataset with balanced classes\n",
    "# rmms_id = label2id['relapsing_remitting_multiple_sclerosis']\n",
    "# small_train_dataset = concatenate_datasets([encoded_dataset['train'].filter(lambda example: example['labels'] != rmms_id), encoded_dataset['train'].filter(lambda example: example['labels'] == rmms_id).shuffle(seed=42).select(range(10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding=\"longest\")\n",
    "\n",
    "# DataLoaders creation\n",
    "train_dataloader = DataLoader(\n",
    "    encoded_dataset[\"train\"], shuffle=True, collate_fn=data_collator, batch_size=train_batch_size,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    encoded_dataset[\"validation\"], shuffle=False, collate_fn=data_collator, batch_size=eval_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(peft_model.parameters(), lr=learning_rate)\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")\n",
    "\n",
    "# # Loss\n",
    "# # Class weights\n",
    "# power = 1\n",
    "# class_weights = [1/np.log(len(encoded_dataset['train'].filter(lambda example: example['labels'] == label))) for label in set(encoded_dataset['train']['labels'])]\n",
    "# class_weights = [weight**power for weight in class_weights]\n",
    "# class_weights = torch.tensor(class_weights, dtype=model.dtype).detach().to(device)\n",
    "\n",
    "\n",
    "# loss_fun = torch.nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/43 [00:00<?, ?it/s]/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Loss: 0.0000: 100%|██████████| 43/43 [06:50<00:00,  9.55s/it]\n",
      "100%|██████████| 2/2 [00:05<00:00,  2.94s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type PosixPath is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m     max_f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     48\u001b[0m     min_eval_loss \u001b[38;5;241m=\u001b[39m eval_loss\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mpeft_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL_PATH\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpeft-prompt_llama2_msdiag\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m f1 \u001b[38;5;241m>\u001b[39m max_f1:\n\u001b[1;32m     51\u001b[0m     max_f1 \u001b[38;5;241m=\u001b[39m f1\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/peft/peft_model.py:279\u001b[0m, in \u001b[0;36mPeftModel.save_pretrained\u001b[0;34m(self, save_directory, safe_serialization, selected_adapters, save_embedding_layers, is_main_process, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m     auto_mapping_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_main_process:\n\u001b[0;32m--> 279\u001b[0m     \u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_mapping_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_mapping_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m peft_config\u001b[38;5;241m.\u001b[39minference_mode \u001b[38;5;241m=\u001b[39m inference_mode\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/peft/config.py:80\u001b[0m, in \u001b[0;36mPeftConfigMixin.save_pretrained\u001b[0;34m(self, save_directory, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# save it\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[0;32m---> 80\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwrite(\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/json/__init__.py:234\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/json/encoder.py:201\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterencode(o, _one_shot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(chunks)\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/json/encoder.py:431\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/json/encoder.py:438\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    437\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[0;32m--> 438\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type PosixPath is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# Seed\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Training\n",
    "peft_model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    peft_model.train()\n",
    "    total_loss = 0\n",
    "    bar = tqdm(train_dataloader)\n",
    "    for step, batch in enumerate(bar):\n",
    "        optimizer.zero_grad()\n",
    "        batch.to(device)\n",
    "        outputs = peft_model(**batch)\n",
    "        logits = outputs.logits\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "        bar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    peft_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        eval_loss = 0\n",
    "        eval_preds = []\n",
    "        labels = []\n",
    "        for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "            batch.to(device)\n",
    "            outputs = peft_model(**batch)\n",
    "                \n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            eval_preds.extend(predictions.tolist())\n",
    "            labels.extend(batch['labels'].tolist())\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.detach().float()\n",
    "            \n",
    "    f1 = f1_score(labels, eval_preds, average='macro')\n",
    "\n",
    "    if epoch == 0:\n",
    "        max_f1 = 0\n",
    "        min_eval_loss = eval_loss\n",
    "        peft_model.save_pretrained(paths.MODEL_PATH/'peft-prompt_llama2_msdiag')\n",
    "    elif f1 > max_f1:\n",
    "        max_f1 = f1\n",
    "        if eval_loss < min_eval_loss:\n",
    "            min_eval_loss = eval_loss\n",
    "        print(\"Saved\")\n",
    "        peft_model.save_pretrained(paths.MODEL_PATH/'peft-prompt_llama2_msdiag')\n",
    "    elif (f1 == max_f1) and (eval_loss < min_eval_loss):\n",
    "        print(\"Saved\")\n",
    "        peft_model.save_pretrained(paths.MODEL_PATH/'peft-prompt_llama2_msdiag')\n",
    "        \n",
    "    eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"{epoch=}: {train_epoch_loss=} {eval_epoch_loss=} {f1=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft_model.save_pretrained(paths.MODEL_PATH/'msdiag_llama2_peft_lora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on test set\n",
    "peft_model.eval()\n",
    "test_dataloader = DataLoader(\n",
    "    encoded_dataset[\"test\"], shuffle=False, collate_fn=data_collator, batch_size=1\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "        test_preds = []\n",
    "        labels = []\n",
    "        for step, batch in enumerate(tqdm(test_dataloader)):\n",
    "            batch.to(device)\n",
    "            outputs = peft_model(**batch)\n",
    "                \n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            test_preds.extend(predictions.tolist())\n",
    "            \n",
    "            labels.extend(batch['labels'].tolist())            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn F1, precision, recall, accuracy\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, ConfusionMatrixDisplay\n",
    "print(f\"Accuracy: {accuracy_score(encoded_dataset['test']['labels'], test_preds)}\")\n",
    "print(f\"Precision: {precision_score(encoded_dataset['test']['labels'], test_preds, average='weighted')}\")\n",
    "print(f\"Recall: {recall_score(encoded_dataset['test']['labels'], test_preds, average='weighted')}\")\n",
    "print(f\"F1: {f1_score(encoded_dataset['test']['labels'], test_preds, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_predictions(encoded_dataset[\"test\"][\"labels\"], test_preds, display_labels=label2id, xticks_rotation=\"vertical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv(os.path.join(paths.DATA_PATH_PREPROCESSED,'ms-diag/ms-diag_clean_test.csv'))\n",
    "for i, label in enumerate(labels):\n",
    "    if label != test_preds[i]:\n",
    "        print(\"Entry: \", i)\n",
    "        print(\"Label\", id2label[label])\n",
    "        print(\"Predicted\", id2label[test_preds[i]])\n",
    "        print(\"Text: \", df[\"test\"][i][\"text\"])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "# config = PeftConfig.from_pretrained(paths.MODEL_PATH/'peft_llama2_msdiag')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class PromptLearningConfig(PeftConfig):\n",
    "    \"\"\"\n",
    "    This is the base configuration class to store the configuration of [`PrefixTuning`], [`PromptEncoder`], or\n",
    "    [`PromptTuning`].\n",
    "\n",
    "    Args:\n",
    "        num_virtual_tokens (`int`): The number of virtual tokens to use.\n",
    "        token_dim (`int`): The hidden embedding dimension of the base transformer model.\n",
    "        num_transformer_submodules (`int`): The number of transformer submodules in the base transformer model.\n",
    "        num_attention_heads (`int`): The number of attention heads in the base transformer model.\n",
    "        num_layers (`int`): The number of layers in the base transformer model.\n",
    "    \"\"\"\n",
    "\n",
    "    num_virtual_tokens: int = field(default=None, metadata={\"help\": \"Number of virtual tokens\"})\n",
    "    token_dim: int = field(\n",
    "        default=None, metadata={\"help\": \"The hidden embedding dimension of the base transformer model\"}\n",
    "    )\n",
    "    num_transformer_submodules: Optional[int] = field(\n",
    "        default=None, metadata={\"help\": \"Number of transformer submodules\"}\n",
    "    )\n",
    "    num_attention_heads: Optional[int] = field(default=None, metadata={\"help\": \"Number of attention heads\"})\n",
    "    num_layers: Optional[int] = field(default=None, metadata={\"help\": \"Number of transformer layers\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "configuratio interpretation:\n",
    "- num_virtual_tokens: number of prefix tokens to append to (each layer?)\n",
    "- num_transformer_submodules: 1 or 2? for decoder/encoder? or is it number of layers? I think decoder encoder architecture because if it is not set, they set it to 1 except in a Seq2Seq task. And they create embeddings for the virtual tokens only once for each submodule?\n",
    "- num_layers: to how many layers we should add the prefix tuning?\n",
    "- The peft model will automatically try to infer all the model specification from model.config in the `get_peft_model()` call. It will also automatically construct the PEFT Model class from the task type: return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model, peft_config, adapter_name=adapter_name)\n",
    "- you can add multiple adapters (must be all of same peft type though) to a mode each with their own configurations. You must give them unique names, otherwise gives name \"default\". Each prompt encoding and the whole process will be done for every single adapter I think. It first prepares the peft config by matching non specified elements with the model config. It then sets up the prompt encoder (_set_up_prompt_encoder(adapter_name)) using the created config for the adapter\n",
    "- If specification is PROMPT_TUNING we just create a trainable word embeddings matrix. By inserting an init text you can initialize the embedding from the base tokenizer and base model. \n",
    "- I think the prompt_token ids are initialized separately from the existing ids for the tokenizer. Because we have a different embedding matrix we can initialize from 0 to 19 say and then put it through, then concatenate the weights again to the embeddings of the base model without the virtual tokens. As they don't have any meaning and we will always have the same order of them (first will be virtual token 0, then 1 etc.) we also don't really neew to match them in the forward pass order will always be same.\n",
    "\n",
    "``` python\n",
    "if config.num_transformer_submodules is None:\n",
    "            config.num_transformer_submodules = 2 if config.task_type == TaskType.SEQ_2_SEQ_LM else 1\n",
    "total_virtual_tokens = config.num_virtual_tokens * config.num_transformer_submodules\n",
    "```\n",
    "\n",
    "```\n",
    "def _prepare_prompt_learning_config(peft_config, model_config):\n",
    "    if peft_config.num_layers is None:\n",
    "        if \"num_hidden_layers\" in model_config:\n",
    "            num_layers = model_config[\"num_hidden_layers\"]\n",
    "        elif \"num_layers\" in model_config:\n",
    "            num_layers = model_config[\"num_layers\"]\n",
    "        elif \"n_layer\" in model_config:\n",
    "            num_layers = model_config[\"n_layer\"]\n",
    "        else:\n",
    "            raise ValueError(\"Please specify `num_layers` in `peft_config`\")\n",
    "        peft_config.num_layers = num_layers\n",
    "\n",
    "    if peft_config.token_dim is None:\n",
    "        if \"hidden_size\" in model_config:\n",
    "            token_dim = model_config[\"hidden_size\"]\n",
    "        elif \"n_embd\" in model_config:\n",
    "            token_dim = model_config[\"n_embd\"]\n",
    "        elif \"d_model\" in model_config:\n",
    "            token_dim = model_config[\"d_model\"]\n",
    "        else:\n",
    "            raise ValueError(\"Please specify `token_dim` in `peft_config`\")\n",
    "        peft_config.token_dim = token_dim\n",
    "\n",
    "    if peft_config.num_attention_heads is None:\n",
    "        if \"num_attention_heads\" in model_config:\n",
    "            num_attention_heads = model_config[\"num_attention_heads\"]\n",
    "        elif \"n_head\" in model_config:\n",
    "            num_attention_heads = model_config[\"n_head\"]\n",
    "        elif \"num_heads\" in model_config:\n",
    "            num_attention_heads = model_config[\"num_heads\"]\n",
    "        elif \"encoder_attention_heads\" in model_config:\n",
    "            num_attention_heads = model_config[\"encoder_attention_heads\"]\n",
    "        else:\n",
    "            raise ValueError(\"Please specify `num_attention_heads` in `peft_config`\")\n",
    "        peft_config.num_attention_heads = num_attention_heads\n",
    "\n",
    "    if getattr(peft_config, \"encoder_hidden_size\", None) is None:\n",
    "        setattr(peft_config, \"encoder_hidden_size\", peft_config.token_dim)\n",
    "\n",
    "    return peft_config\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prefix tuning forward:\n",
    "- The prefixes are inserted into the model using the past_key_values which is the cached keys and values of the whole sequence before the current token in the autoregressive generation procedure. Instead of computing these keys and values with a model forward pass, we simply compute them directly with our MLP model. This is also why we need the number of attention heads (as we need a pair for every head).\n",
    "- The prompt encoder returns num_layers * 2 * token_dim past key values, so for every layer a key and value. Reading this one could think only \"one token\" (one key value pair) is generated. But later this is reshaped into:\n",
    "\n",
    "past_key_values = past_key_values.view(\n",
    "                batch_size,\n",
    "                peft_config.num_virtual_tokens,\n",
    "                peft_config.num_layers * 2,\n",
    "                peft_config.num_attention_heads,\n",
    "                peft_config.token_dim // peft_config.num_attention_heads,\n",
    "            )\n",
    "\n",
    "so we again have num_virtual_tokens as a dimension. This matrix is then permuted and split which I don't know the specifics of but it ensures that it is in the correct format.\n",
    "\n",
    "def _prefix_tuning_forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        batch_size = _get_batch_size(input_ids, inputs_embeds)\n",
    "        past_key_values = self.get_prompt(batch_size)\n",
    "        fwd_params = list(inspect.signature(self.base_model.forward).parameters.keys())\n",
    "        kwargs.update(\n",
    "            {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"inputs_embeds\": inputs_embeds,\n",
    "                \"output_attentions\": output_attentions,\n",
    "                \"output_hidden_states\": output_hidden_states,\n",
    "                \"return_dict\": return_dict,\n",
    "                \"past_key_values\": past_key_values,\n",
    "            }\n",
    "        )\n",
    "        if \"past_key_values\" in fwd_params:\n",
    "            return self.base_model(labels=labels, **kwargs)\n",
    "        else:\n",
    "            transformer_backbone_name = self.base_model.get_submodule(self.transformer_backbone_name)\n",
    "            fwd_params = list(inspect.signature(transformer_backbone_name.forward).parameters.keys())\n",
    "            if \"past_key_values\" not in fwd_params:\n",
    "                raise ValueError(\"Model does not support past key values which are required for prefix tuning.\")\n",
    "            outputs = transformer_backbone_name(**kwargs)\n",
    "            pooled_output = outputs[1] if len(outputs) > 1 else outputs[0]\n",
    "            if \"dropout\" in [name for name, _ in list(self.base_model.named_children())]:\n",
    "                pooled_output = self.base_model.dropout(pooled_output)\n",
    "            logits = self.base_model.get_submodule(self.cls_layer_name)(pooled_output)\n",
    "\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                if self.config.problem_type is None:\n",
    "                    if self.base_model.num_labels == 1:\n",
    "                        self.config.problem_type = \"regression\"\n",
    "                    elif self.base_model.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                        self.config.problem_type = \"single_label_classification\"\n",
    "                    else:\n",
    "                        self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "                if self.config.problem_type == \"regression\":\n",
    "                    loss_fct = MSELoss()\n",
    "                    if self.base_model.num_labels == 1:\n",
    "                        loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                    else:\n",
    "                        loss = loss_fct(logits, labels)\n",
    "                elif self.config.problem_type == \"single_label_classification\":\n",
    "                    loss_fct = CrossEntropyLoss()\n",
    "                    loss = loss_fct(logits.view(-1, self.base_model.num_labels), labels.view(-1))\n",
    "                elif self.config.problem_type == \"multi_label_classification\":\n",
    "                    loss_fct = BCEWithLogitsLoss()\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            if not return_dict:\n",
    "                output = (logits,) + outputs[2:]\n",
    "                return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "            return SequenceClassifierOutput(\n",
    "                loss=loss,\n",
    "                logits=logits,\n",
    "                hidden_states=outputs.hidden_states,\n",
    "                attentions=outputs.attentions,\n",
    "            )\n",
    "\n",
    "def get_prompt(self, batch_size: int, task_ids: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the virtual prompts to use for Peft. Only applicable when using a prompt learning method.\n",
    "        \"\"\"\n",
    "        peft_config = self.active_peft_config\n",
    "        prompt_encoder = self.prompt_encoder[self.active_adapter]\n",
    "        prompt_tokens = (\n",
    "            self.prompt_tokens[self.active_adapter]\n",
    "            .unsqueeze(0)\n",
    "            .expand(batch_size, -1)\n",
    "            .to(prompt_encoder.embedding.weight.device)\n",
    "        )\n",
    "        if peft_config.peft_type == PeftType.PREFIX_TUNING:\n",
    "            prompt_tokens = prompt_tokens[:, : peft_config.num_virtual_tokens]\n",
    "            if peft_config.inference_mode:\n",
    "                past_key_values = prompt_encoder.embedding.weight.repeat(batch_size, 1, 1)\n",
    "            else:\n",
    "                past_key_values = prompt_encoder(prompt_tokens)\n",
    "            if self.base_model_torch_dtype is not None:\n",
    "                past_key_values = past_key_values.to(self.base_model_torch_dtype)\n",
    "            past_key_values = past_key_values.view(\n",
    "                batch_size,\n",
    "                peft_config.num_virtual_tokens,\n",
    "                peft_config.num_layers * 2,\n",
    "                peft_config.num_attention_heads,\n",
    "                peft_config.token_dim // peft_config.num_attention_heads,\n",
    "            )\n",
    "            if peft_config.num_transformer_submodules == 2:\n",
    "                past_key_values = torch.cat([past_key_values, past_key_values], dim=2)\n",
    "            past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(\n",
    "                peft_config.num_transformer_submodules * 2\n",
    "            )\n",
    "            if TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING.get(self.config.model_type, None) is not None:\n",
    "                post_process_fn = TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING[self.config.model_type]\n",
    "                past_key_values = post_process_fn(past_key_values)\n",
    "            return past_key_values\n",
    "        else:\n",
    "            if peft_config.peft_type == PeftType.MULTITASK_PROMPT_TUNING:\n",
    "                prompts = prompt_encoder(prompt_tokens, task_ids)\n",
    "            else:\n",
    "                if peft_config.inference_mode:\n",
    "                    prompts = prompt_encoder.embedding.weight.repeat(batch_size, 1, 1)\n",
    "                else:\n",
    "                    prompts = prompt_encoder(prompt_tokens)\n",
    "            return prompts\n",
    "\n",
    "\n",
    "class PrefixEncoder(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    The `torch.nn` model to encode the prefix.\n",
    "\n",
    "    Args:\n",
    "        config ([`PrefixTuningConfig`]): The configuration of the prefix encoder.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    ```py\n",
    "    >>> from peft import PrefixEncoder, PrefixTuningConfig\n",
    "\n",
    "    >>> config = PrefixTuningConfig(\n",
    "    ...     peft_type=\"PREFIX_TUNING\",\n",
    "    ...     task_type=\"SEQ_2_SEQ_LM\",\n",
    "    ...     num_virtual_tokens=20,\n",
    "    ...     token_dim=768,\n",
    "    ...     num_transformer_submodules=1,\n",
    "    ...     num_attention_heads=12,\n",
    "    ...     num_layers=12,\n",
    "    ...     encoder_hidden_size=768,\n",
    "    ... )\n",
    "    >>> prefix_encoder = PrefixEncoder(config)\n",
    "    ```\n",
    "\n",
    "    **Attributes**:\n",
    "        - **embedding** (`torch.nn.Embedding`) -- The embedding layer of the prefix encoder.\n",
    "        - **transform** (`torch.nn.Sequential`) -- The two-layer MLP to transform the prefix embeddings if\n",
    "          `prefix_projection` is `True`.\n",
    "        - **prefix_projection** (`bool`) -- Whether to project the prefix embeddings.\n",
    "\n",
    "    Input shape: (`batch_size`, `num_virtual_tokens`)\n",
    "\n",
    "    Output shape: (`batch_size`, `num_virtual_tokens`, `2*layers*hidden`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.prefix_projection = config.prefix_projection\n",
    "        token_dim = config.token_dim\n",
    "        num_layers = config.num_layers\n",
    "        encoder_hidden_size = config.encoder_hidden_size\n",
    "        num_virtual_tokens = config.num_virtual_tokens\n",
    "        if self.prefix_projection and not config.inference_mode:\n",
    "            # Use a two-layer MLP to encode the prefix\n",
    "            self.embedding = torch.nn.Embedding(num_virtual_tokens, token_dim)\n",
    "            self.transform = torch.nn.Sequential(\n",
    "                torch.nn.Linear(token_dim, encoder_hidden_size),\n",
    "                torch.nn.Tanh(),\n",
    "                torch.nn.Linear(encoder_hidden_size, num_layers * 2 * token_dim),\n",
    "            )\n",
    "        else:\n",
    "            self.embedding = torch.nn.Embedding(num_virtual_tokens, num_layers * 2 * token_dim)\n",
    "\n",
    "    def forward(self, prefix: torch.Tensor):\n",
    "        if self.prefix_projection:\n",
    "            prefix_tokens = self.embedding(prefix)\n",
    "            past_key_values = self.transform(prefix_tokens)\n",
    "        else:\n",
    "            past_key_values = self.embedding(prefix)\n",
    "        return past_key_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
