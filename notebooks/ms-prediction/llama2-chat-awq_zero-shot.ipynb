{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 10.19 GB\n",
      "   Allocated Memory : 0.00 GB\n",
      "   Reserved Memory : 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, DataCollatorWithPadding\n",
    "\n",
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "from src import paths\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from typing import Tuple\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "MODEL_PATH = paths.MODEL_PATH/'llama2-chat'\n",
    "QUANTIZATION = \"4bit\"\n",
    "\n",
    "SPLIT = \"train\"\n",
    "\n",
    "BASE_PROMPT = \"<s>[INST]\\n<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{user_prompt}[/INST]\\n\\n{answer_init}\"\n",
    "SYSTEM_PROMP = \"Is the MS diagnosis in the text of type \\\"Sekundär progrediente Multiple Sklerose (SPMS)\\\", \\\"primäre progrediente Multiple Sklerose (PPMS)\\\" or \\\"schubförmig remittierende Multiple Sklerose (RRMS)\\\"?\"\n",
    "ANSWER_INIT = \"Based on the information provided in the text, the most likely diagnosis for the patient is: \"\n",
    "TRUNCATION_SIZE = 300\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "DO_SAMPLE = False\n",
    "NUM_BEAMS = 1\n",
    "MAX_NEW_TOKENS = 20\n",
    "TEMPERATURE = 1\n",
    "TOP_P = 1\n",
    "TOP_K = 4\n",
    "PENALTY_ALPHA = 0.0\n",
    "\n",
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        for gpu_id in range(num_gpus):\n",
    "            free_mem, total_mem = torch.cuda.mem_get_info(gpu_id)\n",
    "            gpu_properties = torch.cuda.get_device_properties(gpu_id)\n",
    "            print(f\"GPU {gpu_id}: {gpu_properties.name}\")\n",
    "            print(f\"   Total Memory: {total_mem / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Free Memory: {free_mem / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Allocated Memory : {torch.cuda.memory_allocated(gpu_id) / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Reserved Memory : {torch.cuda.memory_reserved(gpu_id) / (1024 ** 3):.2f} GB\")\n",
    "    else:\n",
    "        print(\"No GPU available.\")\n",
    "\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 17:51:17 config.py:179] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 12-26 17:51:17 llm_engine.py:73] Initializing an LLM engine with config: model=PosixPath('/cluster/dataset/midatams/inf-extr/resources/models/Llama-2-7B-Chat-AWQ'), tokenizer=PosixPath('/cluster/dataset/midatams/inf-extr/resources/models/Llama-2-7B-Chat-AWQ'), tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=awq, enforce_eager=False, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W socket.cpp:436] [c10d] The server socket cannot be initialized on [::]:44215 (errno: 97 - Address family not supported by protocol).\n",
      "[W socket.cpp:663] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:44215 (errno: 97 - Address family not supported by protocol).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-26 17:51:44 llm_engine.py:223] # GPU blocks: 359, # CPU blocks: 512\n",
      "INFO 12-26 17:51:48 model_runner.py:394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-26 17:51:57 model_runner.py:437] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "# Create an LLM.\n",
    "llm = LLM(model=paths.MODEL_PATH/\"Llama-2-7B-Chat-AWQ\", quantization=\"AWQ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling parameters for text generation.\n",
    "\n",
    "    Overall, we follow the sampling parameters from the OpenAI text completion\n",
    "    API (https://platform.openai.com/docs/api-reference/completions/create).\n",
    "    In addition, we support beam search, which is not supported by OpenAI.\n",
    "\n",
    "    Args:\n",
    "        n: Number of output sequences to return for the given prompt.\n",
    "        best_of: Number of output sequences that are generated from the prompt.\n",
    "            From these `best_of` sequences, the top `n` sequences are returned.\n",
    "            `best_of` must be greater than or equal to `n`. This is treated as\n",
    "            the beam width when `use_beam_search` is True. By default, `best_of`\n",
    "            is set to `n`.\n",
    "        presence_penalty: Float that penalizes new tokens based on whether they\n",
    "            appear in the generated text so far. Values > 0 encourage the model\n",
    "            to use new tokens, while values < 0 encourage the model to repeat\n",
    "            tokens.\n",
    "        frequency_penalty: Float that penalizes new tokens based on their\n",
    "            frequency in the generated text so far. Values > 0 encourage the\n",
    "            model to use new tokens, while values < 0 encourage the model to\n",
    "            repeat tokens.\n",
    "        repetition_penalty: Float that penalizes new tokens based on whether\n",
    "            they appear in the prompt and the generated text so far. Values > 1\n",
    "            encourage the model to use new tokens, while values < 1 encourage\n",
    "            the model to repeat tokens.\n",
    "        temperature: Float that controls the randomness of the sampling. Lower\n",
    "            values make the model more deterministic, while higher values make\n",
    "            the model more random. Zero means greedy sampling.\n",
    "        top_p: Float that controls the cumulative probability of the top tokens\n",
    "            to consider. Must be in (0, 1]. Set to 1 to consider all tokens.\n",
    "        top_k: Integer that controls the number of top tokens to consider. Set\n",
    "            to -1 to consider all tokens.\n",
    "        min_p: Float that represents the minimum probability for a token to be\n",
    "            considered, relative to the probability of the most likely token.\n",
    "            Must be in [0, 1]. Set to 0 to disable this.\n",
    "        use_beam_search: Whether to use beam search instead of sampling.\n",
    "        length_penalty: Float that penalizes sequences based on their length.\n",
    "            Used in beam search.\n",
    "        early_stopping: Controls the stopping condition for beam search. It\n",
    "            accepts the following values: `True`, where the generation stops as\n",
    "            soon as there are `best_of` complete candidates; `False`, where an\n",
    "            heuristic is applied and the generation stops when is it very\n",
    "            unlikely to find better candidates; `\"never\"`, where the beam search\n",
    "            procedure only stops when there cannot be better candidates\n",
    "            (canonical beam search algorithm).\n",
    "        stop: List of strings that stop the generation when they are generated.\n",
    "            The returned output will not contain the stop strings.\n",
    "        stop_token_ids: List of tokens that stop the generation when they are\n",
    "            generated. The returned output will contain the stop tokens unless\n",
    "            the stop tokens are special tokens.\n",
    "        include_stop_str_in_output: Whether to include the stop strings in output\n",
    "            text. Defaults to False.\n",
    "        ignore_eos: Whether to ignore the EOS token and continue generating\n",
    "            tokens after the EOS token is generated.\n",
    "        max_tokens: Maximum number of tokens to generate per output sequence.\n",
    "        logprobs: Number of log probabilities to return per output token.\n",
    "            Note that the implementation follows the OpenAI API: The return\n",
    "            result includes the log probabilities on the `logprobs` most likely\n",
    "            tokens, as well the chosen tokens. The API will always return the\n",
    "            log probability of the sampled token, so there  may be up to\n",
    "            `logprobs+1` elements in the response.\n",
    "        prompt_logprobs: Number of log probabilities to return per prompt token.\n",
    "        skip_special_tokens: Whether to skip special tokens in the output.\n",
    "        spaces_between_special_tokens: Whether to add spaces between special\n",
    "            tokens in the output.  Defaults to True.\n",
    "        logits_processors: List of functions that modify logits based on\n",
    "            previously generated tokens.\n",
    "\n",
    "    Defaults:\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        n: int = 1,\r\n",
    "        best_of: Optional[int] = None,\r\n",
    "        presence_penalty: float = 0.0,\r\n",
    "        frequency_penalty: float = 0.0,\r\n",
    "        repetition_penalty: float = 1.0,\r\n",
    "        temperature: float = 1.0,\r\n",
    "        top_p: float = 1.0,\r\n",
    "        top_k: int = -1,\r\n",
    "        min_p: int = 0.0,\r\n",
    "        use_beam_search: bool = False,\r\n",
    "        length_penalty: float = 1.0,\r\n",
    "        early_stopping: Union[bool, str] = False,\r\n",
    "        stop: Optional[Union[str, List[str]]] = None,\r\n",
    "        stop_token_ids: Optional[List[int]] = None,\r\n",
    "        include_stop_str_in_output: bool = False,\r\n",
    "        ignore_eos: bool = False,\r\n",
    "        max_tokens: int = 16,\r\n",
    "        logprobs: Optional[int] = None,\r\n",
    "        prompt_logprobs: Optional[int] = None,\r\n",
    "        skip_special_tokens: bool = True,\r\n",
    "        spaces_between_special_tokens: bool = True,\r\n",
    "        logits_processors: Optional[List[LogitsProcessor]] = None,\n",
    "\n",
    "Output args:\n",
    "\n",
    "\"\"\"The output data of a request to the LLM.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        request_id: The unique ID of the request.\r\n",
    "        prompt: The prompt string of the request.\r\n",
    "        prompt_token_ids: The token IDs of the prompt.\r\n",
    "        prompt_logprobs: The log probabilities to return per prompt token.\r\n",
    "        outputs: The output sequences of the request.\r\n",
    "        finished: Whether the whole request is finished.\r\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 4/4 [00:00<00:00, 13.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Hello, my name is, Generated text: ' Unknown Doctor, and I am here to review this product, K2 Dream'\n",
      "Prompt: The president of the United States is, Generated text: ' a very busy person. They work long hours, often making decisions that impact'\n",
      "Prompt: The capital of France is, Generated text: ' Paris, which is located in the northern central part of the country. It is'\n",
      "Prompt: The future of AI is, Generated text: ' Fed by the present and past\\n\\nArtificial Intelligence (AI)'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a sampling params object.\n",
    "sampling_params = SamplingParams(best_of = 8, use_beam_search = True, temperature = 0)\n",
    "\n",
    "# Sample prompts.\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "### Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, padding_side=\"left\")\n",
    "prompt_token_ids = tokenizer(prompts)[\"input_ids\"]\n",
    "\n",
    "outputs = llm.generate(prompt_token_ids = prompt_token_ids)\n",
    "\n",
    "for idx, output in enumerate(outputs):\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompts[idx]}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 2.05 GB\n",
      "   Allocated Memory : 6.52 GB\n",
      "   Reserved Memory : 6.76 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Model and tokenizer\n",
    "\n",
    "def load_model_and_tokenizer(model_path:os.PathLike, quantization:str = QUANTIZATION)->Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"Loads the model and tokenizer from the given path and returns the compiled model and tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        model_path (os.PathLike): Path to the model\n",
    "        quantization (str, optional): Quantization. Must be one of 4bit or bfloat16. Defaults to QUANTIZATION.\n",
    "\n",
    "        Returns:\n",
    "            tuple(AutoModelForCausalLM, AutoTokenizer): Returns the compiled model and tokenizer\n",
    "            \n",
    "    \"\"\"\n",
    "    # ### Model\n",
    "    if quantization == \"bfloat16\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "    elif quantization == \"4bit\":\n",
    "        bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                        bnb_4bit_use_double_quant=True,\n",
    "                                        bnb_4bit_quant_type=\"nf4\",\n",
    "                                        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", quantization_config=bnb_config)\n",
    "    else:\n",
    "        raise ValueError(\"Quantization must be one of 4bit or bfloat16\")\n",
    "    \n",
    "    ### Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"left\")\n",
    "\n",
    "    # Check if the pad token is already in the tokenizer vocabulary\n",
    "    if '<pad>' not in tokenizer.get_vocab():\n",
    "        # Add the pad token\n",
    "        tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "    \n",
    "\n",
    "    #Resize the embeddings\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    #Configure the pad token in the model\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Check if they are equal\n",
    "    assert model.config.pad_token_id == tokenizer.pad_token_id, \"The model's pad token ID does not match the tokenizer's pad token ID!\"\n",
    "\n",
    "    # Print the pad token ids\n",
    "    print('Tokenizer pad token ID:', tokenizer.pad_token_id)\n",
    "    print('Model pad token ID:', model.config.pad_token_id)\n",
    "    print('Model config pad token ID:', model.config.pad_token_id)\n",
    "    print(\"Vocabulary Size with Pad Token: \", len(tokenizer))\n",
    "\n",
    "    return torch.compile(model), tokenizer # Compile Model for faster inference. # To-Do https://pytorch.org/blog/pytorch-compile-to-speed-up-inference/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data()->DatasetDict:\n",
    "    \"\"\"Loads the data for MS-Diag task and returns the dataset dictionary\n",
    "    \n",
    "    Returns:\n",
    "        DatasetDict: Returns the dataset dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    data_files = {\"train\": \"ms-diag_clean_train.csv\", \"validation\": \"ms-diag_clean_val.csv\", \"test\": \"ms-diag_clean_test.csv\"}\n",
    "\n",
    "    df = load_dataset(os.path.join(paths.DATA_PATH_PREPROCESSED,'ms-diag'), data_files = data_files)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_data(df:DatasetDict, tokenizer:AutoTokenizer, split:str=SPLIT, truncation_size:int = TRUNCATION_SIZE)->list[str]:\n",
    "    \"\"\"Returns a list of input texts for the classification task\n",
    "    \n",
    "    Args:\n",
    "        df (DatasetDict): Dataset dictionary\n",
    "        tokenizer (AutoTokenizer): Tokenizer\n",
    "        split (str, optional): Split. Defaults to SPLIT.\n",
    "        truncation_size (int, optional): Truncation size. Defaults to TRUNCATION_SIZE.\n",
    "        \n",
    "    Returns:\n",
    "        list(str): Returns a list of input texts for the classification task\n",
    "    \"\"\"\n",
    "\n",
    "    def format_prompt(text:str)->str:\n",
    "        \"\"\"Truncates the text to the given truncation size and formats the prompt.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text\n",
    "        \n",
    "        Returns:\n",
    "            str: Returns the formatted prompt\n",
    "        \"\"\"\n",
    "        if len(text) > truncation_size:\n",
    "            text = text[:truncation_size]\n",
    "        else:\n",
    "            text = text\n",
    "        input = BASE_PROMPT.format(system_prompt = SYSTEM_PROMP,\n",
    "                                user_prompt = text,\n",
    "                                answer_init = ANSWER_INIT)\n",
    "\n",
    "        return input\n",
    "\n",
    "    \n",
    "    # Tokenize the text\n",
    "    if split == \"all\":\n",
    "        text = df[\"train\"][\"text\"] + df[\"validation\"][\"text\"] + df[\"test\"][\"text\"]\n",
    "    else:\n",
    "        text = df[split][\"text\"]\n",
    "\n",
    "    tokens = [tokenizer(format_prompt(t)) for t in text]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def get_DataLoader(tokens:list[str], tokenizer:AutoTokenizer, batch_size:int = BATCH_SIZE, padding:bool = True)->DataLoader:\n",
    "    \"\"\"Returns a DataLoader for the given dataset dictionary\n",
    "    \n",
    "    Args:\n",
    "        tokens (List(str)): List of input texts\n",
    "        tokenizer (AutoTokenizer): Tokenizer\n",
    "        batch_size (int, optional): Batch size. Defaults to BATCH_SIZE.\n",
    "        padding (bool, optional): Padding. Defaults to True.\n",
    "        \n",
    "    Returns:\n",
    "        DataLoader: Returns a DataLoader for the given dataset dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    # Default collate function \n",
    "    collate_fn = DataCollatorWithPadding(tokenizer, padding=padding)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset=tokens, collate_fn=collate_fn, batch_size=batch_size, shuffle = False) \n",
    "\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before Model is loaded:\n",
      "\n",
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 2.05 GB\n",
      "   Allocated Memory : 6.52 GB\n",
      "   Reserved Memory : 6.76 GB\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_model_and_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 59\u001b[0m\n\u001b[1;32m     55\u001b[0m     pd\u001b[38;5;241m.\u001b[39mSeries(results)\u001b[38;5;241m.\u001b[39mto_csv(paths\u001b[38;5;241m.\u001b[39mRESULTS_PATH\u001b[38;5;241m/\u001b[39mfile_name)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU Memory before Model is loaded:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m check_gpu_memory()\n\u001b[0;32m----> 8\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_and_tokenizer\u001b[49m(MODEL_PATH, quantization\u001b[38;5;241m=\u001b[39mQUANTIZATION)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU Memory after Model is loaded:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m check_gpu_memory()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_model_and_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "\n",
    "    # Load Data, Model and Tokenizer\n",
    "    df = load_data()\n",
    "\n",
    "    print(\"GPU Memory before Model is loaded:\\n\")\n",
    "    check_gpu_memory()\n",
    "    model, tokenizer = load_model_and_tokenizer(MODEL_PATH, quantization=QUANTIZATION)\n",
    "    print(\"GPU Memory after Model is loaded:\\n\")\n",
    "    check_gpu_memory()\n",
    "\n",
    "    # Prepare Data\n",
    "    tokens = prepare_data(df, tokenizer, split=\"all\", truncation_size=TRUNCATION_SIZE)\n",
    "\n",
    "    # Get DataLoader\n",
    "    dataloader = get_DataLoader(tokens, tokenizer, batch_size=BATCH_SIZE, padding=True)\n",
    "\n",
    "    # Inference\n",
    "    outputs = []\n",
    "\n",
    "    for idx, batch in enumerate(tqdm.tqdm(dataloader)):\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        print(input_ids)\n",
    "        break\n",
    "        with torch.inference_mode():\n",
    "            generated_ids = llm.generate(input_ids = input_ids, \n",
    "                                           attention_mask = attention_mask,\n",
    "                                            max_new_tokens=MAX_NEW_TOKENS, \n",
    "                                            num_beams=NUM_BEAMS, \n",
    "                                            do_sample=DO_SAMPLE, \n",
    "                                            temperature = TEMPERATURE, \n",
    "                                            num_return_sequences = 1, \n",
    "                                            top_p = TOP_P,\n",
    "                                            top_k = TOP_K,\n",
    "                                            penalty_alpha = PENALTY_ALPHA).to(\"cpu\")\n",
    "    \n",
    "        outputs.append(tokenizer.batch_decode(generated_ids, skip_special_tokens=True))\n",
    "        print(\"Memory after batch {}:\\n\".format(idx))\n",
    "        check_gpu_memory()\n",
    "\n",
    "    # Save results\n",
    "    return\n",
    "    outputs = list(chain.from_iterable(outputs))\n",
    "    results = [out.split(ANSWER_INIT)[1] for out in outputs]\n",
    "    \n",
    "    # Add Arguments as last row to the results\n",
    "    results.append(str(args))\n",
    "\n",
    "    file_name = f\"ms_diag-llama2-chat_zero-shot_{JOB_ID}.csv\"\n",
    "    pd.Series(results).to_csv(paths.RESULTS_PATH/file_name)\n",
    "\n",
    "    return\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
