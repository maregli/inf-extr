{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "from src import paths\n",
    "\n",
    "from src.utils import get_default_pydantic_model, SideEffect, SideEffectList\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "default_model = get_default_pydantic_model(\"side_effects\")\n",
    "model = BertModel.from_pretrained(os.path.join(paths.MODEL_PATH, \"medbert-512\")).to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(os.path.join(paths.MODEL_PATH, \"medbert-512\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_cosine_similarity(ground_truth:str, prediction:str, model:BertModel, tokenizer:BertTokenizer, threshhold:float = 0.7):\n",
    "    \"\"\"\n",
    "    Calculate if the cosine similarity between the ground truth and the prediction is higher than the threshhold.\n",
    "\n",
    "    Args:\n",
    "        ground_truth (str): The ground truth text.\n",
    "        prediction (str): The predicted text.\n",
    "        model (BertModel): BertModel object to calculate the cosine similarity.\n",
    "        tokenizer (BertTokenizer): BertTokenizer object to tokenize the text.\n",
    "        threshhold (float, optional): The threshhold for the cosine similarity. Defaults to 0.7.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the cosine similarity is higher than the threshhold, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ground_truth = model(**tokenizer(ground_truth, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device))[\"last_hidden_state\"].to(\"cpu\")\n",
    "        prediction = model(**tokenizer(prediction, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device))[\"last_hidden_state\"].to(\"cpu\")\n",
    "        ground_truth = ground_truth.mean(dim=1)\n",
    "        prediction = prediction.mean(dim=1)\n",
    "        return cosine_similarity(ground_truth, prediction).item() > threshhold\n",
    "\n",
    "def preprocess_group(grouped_df: pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess a group of rows (grouped by obs index) from the dataframe. The preprocessing consists of:\n",
    "    - Fusing all the unique side effects per medication with \",\" (so only one row per medication is left)\n",
    "    - Splitting the medications by \"/\" if necessary\n",
    "\n",
    "    Args:\n",
    "        grouped_df (pd.DataFrame): A group of rows from the dataframe.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with the group preprocessed.\n",
    "    \"\"\"\n",
    "\n",
    "    df = []\n",
    "    \n",
    "    # group by medication name\n",
    "    for med, group in grouped_df.groupby(\"medication\"):\n",
    "       # Fuse all the unique side effects with \",\"\n",
    "        side_effects = group[\"side_effect\"].unique()\n",
    "        side_effects = \", \".join(side_effects)\n",
    "        text = group[\"text\"].iloc[0]\n",
    "        original_text = group[\"original_text\"].iloc[0]\n",
    "        index = group[\"index\"].iloc[0]\n",
    "\n",
    "        new_row = {\"medication\": med, \"side_effect\": side_effects, \"text\": text, \"original_text\": original_text, \"index\": index, \"successful\": group[\"successful\"].iloc[0]}\n",
    "        # Append the new row to the dataframe\n",
    "        df.append(new_row)\n",
    "\n",
    "    return pd.DataFrame(df)\n",
    "\n",
    "def prepare_results(path: str)->pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Prepare the results from the model to be compared with the labels. The preprocessing consists of:\n",
    "    - Fixing the model answers, replacing them with the default model answer if they are invalid (in regards to JSON structure).\n",
    "    - Grouping the results by index and preprocessing the group. (See preprocess_group() function for more details.)\n",
    "    - Lowercasing and converting everything to string.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the results file.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with the results preprocessed.\n",
    "    \"\"\"\n",
    "    results = torch.load(path)\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    df[\"successful\"] = True\n",
    "\n",
    "    # Fix model_answers\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            # Does model answer have key \"side_effects\"?\n",
    "            answer = json.loads(row[\"model_answers\"])\n",
    "\n",
    "            # Is answer a valid SideEffectList?\n",
    "            SideEffectList(**answer)\n",
    "\n",
    "            # Is the element an empty list?\n",
    "            if len(answer[\"side_effects\"]) == 0:\n",
    "                raise ValueError(\"Empty list\")\n",
    "            \n",
    "            # Are all the elements valid SideEffect?\n",
    "            for med in answer[\"side_effects\"]:\n",
    "                SideEffect(**med)\n",
    "            \n",
    "            df.at[idx, \"model_answers\"] = json.dumps(answer)\n",
    "\n",
    "            \n",
    "        except:\n",
    "            df.at[idx, \"model_answers\"] = default_model.model_dump_json()\n",
    "            # print(f\"Error at index {idx}\")\n",
    "            df.at[idx, \"successful\"] = False\n",
    "    \n",
    "    dfs = []\n",
    "    for idx, row in df.iterrows():\n",
    "        row = row.to_dict()\n",
    "        answer = json.loads(row[\"model_answers\"])\n",
    "        medications = answer[\"side_effects\"]\n",
    "        for med in medications:\n",
    "            med[\"text\"] = row.get(\"text\", \"\")\n",
    "            med[\"original_text\"] = row.get(\"original_text\", \"\")\n",
    "            med[\"index\"] = row.get(\"index\", \"\")\n",
    "            med[\"successful\"] = row.get(\"successful\", \"\")\n",
    "            dfs.append((med))\n",
    "\n",
    "    res = pd.DataFrame(dfs)\n",
    "\n",
    "    res = res.sort_values(by=\"index\")\n",
    "\n",
    "    # Group by index and preprocess\n",
    "    _dfs = []\n",
    "\n",
    "    for index, group in res.groupby(\"index\"):\n",
    "        _dfs.append(preprocess_group(group))\n",
    "\n",
    "    res = pd.concat(_dfs)\n",
    "\n",
    "    # Convert everything to string and lowercase\n",
    "    res = res.map(lambda x: str(x).lower())\n",
    "\n",
    "    return res\n",
    "\n",
    "def prepare_labels(path: str)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare the labels to be compared with the results. The preprocessing consists of:\n",
    "    - Lowercasing and converting everything to string.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the labels file.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        pd.DataFrame: A dataframe with the labels preprocessed.\n",
    "    \"\"\"\n",
    "    labels = pd.read_excel(path) \n",
    "\n",
    "    # Convert everything to string and lowercase\n",
    "    labels = labels.map(lambda x: str(x).lower())\n",
    "\n",
    "    return labels\n",
    "\n",
    "def calculate_precision_recall_f1(ground_truth:list[str], predicted:list[str])->tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculate precision and recall from two lists of strings, allowing for partial string matches.\n",
    "    A string does not have to be exactly the same to be considered a match, it is enough if one string is a substring of the other.\n",
    "    This function is used to evaluate the medication names.\n",
    "\n",
    "    Args:\n",
    "        ground_truth (list[str]): List of ground truth medication names\n",
    "        predicted (list[str]): List of predicted medication names\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, float, float]: A tuple with the precision, recall and f1 score.\n",
    "    \"\"\"\n",
    "    # Convert lists to sets for easier comparison\n",
    "    ground_truth_set = set(ground_truth)\n",
    "    predicted_set = set(predicted)\n",
    "\n",
    "    # Initialize counters\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "\n",
    "    # Calculate True Positives and False Positives\n",
    "    for pred_med in predicted_set:\n",
    "        found_match = False\n",
    "        for truth_med in ground_truth_set:\n",
    "            if pred_med in truth_med or truth_med in pred_med:\n",
    "                true_positives += 1\n",
    "                found_match = True\n",
    "                break\n",
    "        if not found_match:\n",
    "            false_positives += 1\n",
    "\n",
    "    # Calculate False Negatives\n",
    "    false_negatives = len(ground_truth_set) - true_positives\n",
    "\n",
    "    # Calculate Precision\n",
    "    if true_positives + false_positives == 0:\n",
    "        precision = 0  # Handle division by zero\n",
    "    else:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "\n",
    "    # Calculate Recall\n",
    "    if true_positives + false_negatives == 0:\n",
    "        recall = 0  # Handle division by zero\n",
    "    else:\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    # Calculate F1 Score\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def string_match(ground_truth:list[dict], predicted:list[dict], model:BertModel, tokenizer:BertTokenizer, threshold:float=0.9)->tuple[float, float, float]:\n",
    "    \"\"\" \n",
    "    Calculate if the strings match (based on cosine similarity) from two lists of dictionaries. This function is used to evaluate the side effects.\n",
    "\n",
    "    Args:\n",
    "        ground_truth (list): List of ground truth medication names and their side effect.\n",
    "        predicted (list): List of predicted medication names and their side effect.\n",
    "        model (BertModel): BertModel object to calculate the cosine similarity.\n",
    "        tokenizer (BertTokenizer): BertTokenizer object to tokenize the text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Match the prediction to the ground truth with medication name\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "\n",
    "    for pred_id, pred in enumerate(predicted):\n",
    "        for gt_id, gt in enumerate(ground_truth):\n",
    "            if pred[\"medication\"] in gt[\"medication\"] or gt[\"medication\"] in pred[\"medication\"]:\n",
    "                if high_cosine_similarity([pred[\"side_effect\"]], [gt[\"side_effect\"]], model, tokenizer, threshold):\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "                    fn += 1\n",
    "                # Remove the ground truth and prediction from the list\n",
    "                ground_truth.pop(gt_id)\n",
    "                predicted.pop(pred_id)\n",
    "\n",
    "    # For remaining ground truth\n",
    "    fn += len(ground_truth)\n",
    "    fp += len(predicted)\n",
    "\n",
    "    # Calculate Precision\n",
    "    if tp + fp == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "\n",
    "    # Calculate Recall\n",
    "    if tp + fn == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = tp / (tp + fn)\n",
    "\n",
    "    # Calculate F1 Score\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def create_metrics_df(labels_path:str, results_path:str, *args, **kwargs)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a dataframe with the metrics for each index. The metrics are:\n",
    "    - Precision, Recall and F1 score for medication names\n",
    "    - Precision, Recall and F1 score for side effects\n",
    "\n",
    "    Args:\n",
    "        labels_path (str): Path to the labels file.\n",
    "        results_path (str): Path to the results file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with the metrics for each index.\n",
    "    \"\"\"\n",
    "    \n",
    "    labels = prepare_labels(labels_path)\n",
    "    res = prepare_results(results_path)\n",
    "    model = kwargs.get(\"model\")\n",
    "    tokenizer = kwargs.get(\"tokenizer\")\n",
    "    threshold = kwargs.get(\"threshold\", 0.9)\n",
    "\n",
    "    dfs = []\n",
    "    for index, group in res.groupby(\"index\"):\n",
    "        text = group[\"text\"].iloc[0]\n",
    "        original_text = group[\"original_text\"].iloc[0]\n",
    "        index = group[\"index\"].iloc[0]\n",
    "        preds_med = group[\"medication\"].to_list()\n",
    "        preds_se = group[\"side_effect\"].to_list()\n",
    "        labels_med = labels[labels[\"index\"] == index][\"medication\"].to_list()\n",
    "        labels_se = labels[labels[\"index\"] == index][\"side_effect\"].to_list()\n",
    "\n",
    "        # Side effects\n",
    "        ground_truth = labels[labels[\"index\"] == index].to_dict(orient=\"records\")\n",
    "        predicted = group.to_dict(orient=\"records\")\n",
    "        assert model is not None and tokenizer is not None, \"Model and tokenizer must be provided as keyword arguments for side effects evaluation.\"\n",
    "        side_effects_precision, side_effects_recall, side_effects_f1 = string_match(ground_truth, predicted, model, tokenizer, threshold)\n",
    "\n",
    "        # Medication\n",
    "        ground_truth = labels[labels[\"index\"] == index][\"medication\"].to_list()\n",
    "        predicted = group[\"medication\"].to_list()\n",
    "        medication_precision, medication_recall, medication_f1 = calculate_precision_recall_f1(ground_truth, predicted)\n",
    "\n",
    "        \n",
    "        if group[\"successful\"].iloc[0] == \"false\":\n",
    "            successful = False\n",
    "        else:\n",
    "            successful = True\n",
    "\n",
    "        dfs.append({\"text\": text, \n",
    "                    \"original_text\": original_text, \n",
    "                    \"index\": index, \n",
    "                    \"preds_med\": preds_med,\n",
    "                    \"labels_med\": labels_med,\n",
    "                    \"preds_se\": preds_se,\n",
    "                    \"labels_se\": labels_se,\n",
    "                    \"successful\": successful,\n",
    "                    \"side_effects_precision\": side_effects_precision, \"side_effects_recall\": side_effects_recall, \"side_effects_f1\": side_effects_f1, \"medication_precision\": medication_precision, \"medication_recall\": medication_recall, \"medication_f1\": medication_f1})\n",
    "\n",
    "    return pd.DataFrame(dfs)\n",
    "\n",
    "def create_agg_metrics_df(labels_path:str, results_path:str, *args, **kwargs)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a dataframe with the aggregated metrics for the whole dataset. The metrics are:\n",
    "    - Precision, Recall and F1 score for medication names\n",
    "    - Precision, Recall and F1 score for side effects\n",
    "\n",
    "    Args:\n",
    "        labels_path (str): Path to the labels file.\n",
    "        results_path (str): Path to the results file.\n",
    "        scorer (BERTScorer): BERTScorer object to score the similarity between the side effects.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with the aggregated metrics for the whole dataset.\n",
    "    \"\"\"\n",
    "    metrics = create_metrics_df(labels_path, results_path, *args, **kwargs)\n",
    "\n",
    "    # Calculate the average of the metrics\n",
    "    agg_metrics = metrics.agg({\"side_effects_precision\": \"mean\", \"side_effects_recall\": \"mean\", \"side_effects_f1\": \"mean\", \"medication_precision\": \"mean\", \"medication_recall\": \"mean\", \"medication_f1\": \"mean\", \"successful\": \"sum\"})\n",
    "\n",
    "    return agg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(paths:list[str], labels:str, *args, **kwargs)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarize the results from the model. The summary consists of:\n",
    "    - Precision, Recall and F1 score for medication names\n",
    "    - Precision, Recall and F1 score for side effects\n",
    "    - Aggregated metrics for the whole dataset\n",
    "\n",
    "    Args:\n",
    "        paths (list): List of paths to the results files.\n",
    "        labels (str): Path to the labels file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with the summary of the results.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for path in paths:\n",
    "        metrics = create_agg_metrics_df(labels, path, *args, **kwargs)\n",
    "        path = str(path)\n",
    "        if path.endswith(\"rag.pt\"):\n",
    "            metrics[\"approach\"] = \"S2A-1\"\n",
    "        elif path.endswith(\"s2a.pt\"):\n",
    "            metrics[\"approach\"] = \"S2A-2\"\n",
    "        else:\n",
    "            metrics[\"approach\"] = \"Base\"\n",
    "        \n",
    "        if \"few_shot_vanilla\" in path:\n",
    "            metrics[\"strategy\"] = \"Few-Shot Base\"\n",
    "        elif \"few_shot_instruction\" in path:\n",
    "            metrics[\"strategy\"] = \"Few-Shot Instruction\"\n",
    "        elif \"zero_shot_vanilla\" in path:\n",
    "            metrics[\"strategy\"] = \"Zero-Shot Base\"\n",
    "        elif \"zero_shot_instruction\" in path:\n",
    "            metrics[\"strategy\"] = \"Zero-Shot Instruction\"\n",
    "        else:\n",
    "            ValueError(\"Unknown strategy\")\n",
    "        dfs.append(metrics)\n",
    "    return pd.DataFrame(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_vanilla.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot instruction\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_instruction.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_vanilla_10_examples.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_10_examples.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_vanilla_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot instruction\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_instruction_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_vanilla_10_examples_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_10_examples_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_vanilla_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot instruction\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_instruction_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_vanilla_10_examples_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Instruction\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_10_examples_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latex_metrics(metrics:pd.DataFrame)->str:\n",
    "    \"\"\"\n",
    "    Convert the metrics dataframe to a latex table.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for strat, group in metrics.groupby(\"strategy\"):\n",
    "        cols = [col for col in metrics.columns if col not in [\"strategy\", \"approach\"]]\n",
    "        df_base = group[group[\"approach\"] == \"Base\"][cols].reset_index(drop=True)\n",
    "        df_base.columns = [f\"{col}_base\" for col in cols]\n",
    "        df_rag = group[group[\"approach\"] == \"S2A-1\"][cols].reset_index(drop=True)\n",
    "        df_rag.columns = [f\"{col}_rag\" for col in cols]\n",
    "        df_s2a = group[group[\"approach\"] == \"S2A-2\"][cols].reset_index(drop=True)\n",
    "        df_s2a.columns = [f\"{col}_s2a\" for col in cols]\n",
    "        df = pd.concat([df_base, df_rag, df_s2a], axis=1)\n",
    "        df[\"strategy\"] = strat\n",
    "        dfs.append(df)\n",
    "\n",
    "    return pd.concat(dfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_vanilla.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_instruction.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_vanilla_10_examples.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_10_examples.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_vanilla_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_instruction_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_vanilla_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_vanilla_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_instruction_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_vanilla_10_examples_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_10_examples_s2a.pt\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = summarize(file_paths, paths.RESULTS_PATH/\"side-effects/labels.xlsx\", model=model, tokenizer=tokenizer, threshold = 0.7)\n",
    "results = results.round(2)\n",
    "# results.to_csv(paths.RESULTS_PATH/\"side-effects/summary13b.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesis13b = latex_metrics(results)\n",
    "thesis13b_precision = thesis13b[[\"strategy\"] + [col for col in thesis13b.columns if \"precision\" in col]]\n",
    "thesis13b_recall = thesis13b[[\"strategy\"] + [col for col in thesis13b.columns if \"recall\" in col]]\n",
    "thesis13b_f1 = thesis13b[[\"strategy\"] + [col for col in thesis13b.columns if \"f1\" in col]]\n",
    "\n",
    "# thesis13b_precision.to_csv(paths.RESULTS_PATH/\"side-effects/thesis13b_precision.csv\", index=False)\n",
    "# thesis13b_recall.to_csv(paths.RESULTS_PATH/\"side-effects/thesis13b_recall.csv\", index=False)\n",
    "# thesis13b_f1.to_csv(paths.RESULTS_PATH/\"side-effects/thesis13b_f1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model seems to have difficulties if there is no medication mentioned with side effects. The tuned model does not have this problem I think, show example\n",
    "- also split medications that contain \"/\" into two lists\n",
    "- concatenate the side effects for the same medications with \",\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_vanilla.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot instruction\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_instruction.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_vanilla_10_examples.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_instruction_10_examples.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_vanilla_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot instruction\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_instruction_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_vanilla_10_examples_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_instruction_10_examples_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_vanilla_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot instruction\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_instruction_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_vanilla_10_examples_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_instruction_10_examples_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths7b = [paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_vanilla.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_instruction.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_vanilla_10_examples.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_instruction_10_examples.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_vanilla_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_instruction_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_vanilla_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_instruction_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_vanilla_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_instruction_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_vanilla_10_examples_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_instruction_10_examples_s2a.pt\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results7b = summarize(file_paths7b, paths.RESULTS_PATH/\"side-effects/labels.xlsx\", model=model, tokenizer=tokenizer, threshold = 0.5)\n",
    "results7b = results7b.round(2)\n",
    "# results7b.to_csv(paths.RESULTS_PATH/\"side-effects/summary7b.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesis7b = latex_metrics(results7b)\n",
    "thesis7b_precision = thesis7b[[\"strategy\"] + [col for col in thesis7b.columns if \"precision\" in col]]\n",
    "thesis7b_recall = thesis7b[[\"strategy\"] + [col for col in thesis7b.columns if \"recall\" in col]]\n",
    "thesis7b_f1 = thesis7b[[\"strategy\"] + [col for col in thesis7b.columns if \"f1\" in col]]\n",
    "\n",
    "# thesis7b_precision.to_csv(paths.RESULTS_PATH/\"side-effects/thesis7b_precision.csv\", index=False)\n",
    "# thesis7b_recall.to_csv(paths.RESULTS_PATH/\"side-effects/thesis7b_recall.csv\", index=False)\n",
    "# thesis7b_f1.to_csv(paths.RESULTS_PATH/\"side-effects/thesis7b_f1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13B Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths13b_lora_1024 = [\n",
    "                paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_zero_shot_vanilla.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_zero_shot_instruction.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_few_shot_vanilla_10_examples.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_few_shot_instruction_10_examples.pt\",\n",
    "              # paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_zero_shot_vanilla_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_zero_shot_instruction_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_few_shot_vanilla_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_few_shot_instruction_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_zero_shot_vanilla_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_zero_shot_instruction_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_few_shot_vanilla_10_examples_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_few_shot_instruction_10_examples_s2a.pt\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results13b_lora_1024 = summarize(file_paths13b_lora_1024, paths.RESULTS_PATH/\"side-effects/labels.xlsx\", model=model, tokenizer=tokenizer, threshold = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results13b_lora_1024 = results13b_lora_1024.round(2)\n",
    "results13b_lora_1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths13b_lora_512 = [\n",
    "              #   paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_zero_shot_vanilla.pt\",\n",
    "              # paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_zero_shot_instruction.pt\",\n",
    "              # paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_few_shot_vanilla_10_examples.pt\",\n",
    "            #   paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_few_shot_instruction_10_examples.pt\",\n",
    "              # paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_zero_shot_vanilla_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_zero_shot_instruction_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_few_shot_vanilla_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_few_shot_instruction_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_zero_shot_vanilla_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_zero_shot_instruction_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_few_shot_vanilla_10_examples_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_few_shot_instruction_10_examples_s2a.pt\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results13b_lora_512 = summarize(file_paths13b_lora_512, paths.RESULTS_PATH/\"side-effects/labels.xlsx\", model=model, tokenizer=tokenizer, threshold = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results13b_lora_512 = results13b_lora_512.round(2)\n",
    "results13b_lora_512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold\n",
    "Will compare effects of threshold for few shot instruction s2a on results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_files = [\n",
    "    paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_10_examples_s2a.pt\",\n",
    "    paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_few_shot_instruction_10_examples_s2a.pt\",\n",
    "    paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_few_shot_instruction_10_examples_s2a.pt\",\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 11)\n",
    "dfs = []\n",
    "for threshold in thresholds:\n",
    "    _df = summarize(threshold_files, paths.RESULTS_PATH/\"side-effects/labels.xlsx\", model=model, tokenizer=tokenizer, threshold = threshold)\n",
    "    _df[\"model\"] = [\"Llama2-MedTuned-13B\", \"Llama2-MedTuned-13B-lora-512\", \"Llama2-MedTuned-13B-lora-1024\"]\n",
    "    _df[\"threshold\"] = threshold\n",
    "    dfs.append(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thresholds = pd.concat(dfs, axis=0)\n",
    "df_thresholds.drop(columns=[\"successful\", \"medication_precision\", \"medication_recall\", \"medication_f1\",\t\"successful\",\"approach\", \"strategy\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line_plot_group(x:pd.Series, y:pd.Series, groups:pd.Series, title:str = \"Line Plot\",\n",
    "                             xlabel:str = \"Threshold\", ylabel:str = \"Recall\"):\n",
    "    \"\"\"\n",
    "    Plot a line plot with different color for group.\n",
    "\n",
    "    Args:\n",
    "        x (pd.Series): x values.\n",
    "        y (pd.Series): y values.\n",
    "        groups (pd.Series): group values.\n",
    "        title (str): Title of the plot.\n",
    "        color_palette (np.ndarray): Color palette to use.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(x) == len(y) == len(groups), \"Length of x, y and group must be the same.\"\n",
    "\n",
    "    unique_groups = groups.unique()\n",
    "\n",
    "    data = pd.DataFrame({\"x\": x, \"y\": y, \"groups\": groups})\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    sns.lineplot(data=data, x=\"x\", y=\"y\", hue=\"groups\", palette=\"viridis\")\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_line_plot_group(df_thresholds[\"threshold\"], df_thresholds[\"side_effects_recall\"], df_thresholds[\"model\"], title=\"Recall vs Threshold\", ylabel=\"Recall\", xlabel=\"Threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_line_plot_group(df_thresholds[\"threshold\"], df_thresholds[\"side_effects_precision\"], df_thresholds[\"model\"], title=\"Precision vs Threshold\", ylabel=\"Precision\", xlabel=\"Threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_line_plot_group(df_thresholds[\"threshold\"], df_thresholds[\"side_effects_f1\"], df_thresholds[\"model\"], title=\"F1 vs Threshold\", ylabel=\"F1\", xlabel=\"Threshold\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
