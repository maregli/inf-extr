{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying text lines\n",
    "\n",
    "This notebook is used to create the dataset to train and evaluate the line classifier of the project. Be sure to run:\n",
    "\n",
    "1. notebooks/00_preprocessing_old_project\n",
    "\n",
    "first.\n",
    "\n",
    "A classifier is trained to classify single text lines of a report. For example, the lines can be classified as containing a diagnosis (`\"dm\"`) or history (`his`) of a patient. This task was used as a preprocessing step to later steps of structured information extraction. So that only lines classified as containing a diagnosis will be fed to a downstream classifier, extracting the exact diagnosis. This step might be unnecessary with modern transformers that can handle longer text inputs. But it could still help by only feeding relevant input. Even more important old approach only trained and evaluated classifier 2 (MS-Diag) on reports containing \"dm\", which gives a more accurate idea of the whole pipeline.\n",
    "\n",
    "The files containing the necessary information are inside the `data/raw/labelling` directory. It contains manually labelled reports from different sessions.\n",
    "\n",
    "The original classes per label are:\n",
    "\n",
    "| category    | subcategory       | abbreviation |\n",
    "|-------------|-------------------|--------------|\n",
    "| diagnosis   | MS diagnosis      | dm           |\n",
    "|             | other             | do           |\n",
    "| current state     |              | cu           |\n",
    "| history     |                   | his          |\n",
    "| symptoms    | MS related        | sym          |\n",
    "|             | other             | so           |\n",
    "| MRI | results                  | mr           |\n",
    "| lab | results                  | labr         |\n",
    "|             | other             | labo         |\n",
    "| medication  | MS related        | medms        |\n",
    "|             | other             | medo         |\n",
    "| test, treatment        | results | tr           |\n",
    "| header      |                   | head         |\n",
    "| unknown     |                   | unk          |\n",
    "\n",
    "I will group the classes according to the original approach, and drop observations with no text or label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "\n",
    "from src import paths\n",
    "from src.utils import line_label_label2id, prepare_pd_dataset_for_lineclass\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Level Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_line_labelling():\n",
    "    \"\"\"Loading the data from the nested csv files in the different \"imported_time\" directories. Labelled reports have a \"rev.csv\" ending\n",
    "    and are in a \"_Marc\" subdirectory. There should only be one entry per rid, that is labelled. Duplicates will be removed.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with columns: \"text\", \"class\", \"rid\"\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    rid_list = []\n",
    "\n",
    "    for root, dirs, files in os.walk(paths.DATA_PATH_LABELLED):\n",
    "        if \"Marc\" not in root:\n",
    "            continue\n",
    "        for file in files:\n",
    "\n",
    "            # Get the research id from filename\n",
    "            rid = file.split(\"_\")[0]\n",
    "            \n",
    "            # Check if the file is a labelled report (and not mri) and if the rid is already in the list\n",
    "            if (file.endswith(\"rev.csv\") and \"mri\" not in file and rid not in rid_list):\n",
    "                \n",
    "                # Append rid to rid list to keep track of which files have been added\n",
    "                rid_list.append(rid)\n",
    "\n",
    "                # Create a dataframe from the csv file\n",
    "                _df = pd.read_csv(os.path.join(root, file))\n",
    "                \n",
    "                # Add the rid to the dataframe\n",
    "                _df = _df.rename(columns={\"text\": \"text\", \"class\": \"class\"})\n",
    "                _df['rid'] = rid\n",
    "                _df['index_within_rid'] = _df.index\n",
    "                \n",
    "                # Append the dataframe to the main dataframe\n",
    "                try: \n",
    "                    df_list.append(_df)\n",
    "                except:\n",
    "                    print(\"Error with file: \", file)\n",
    "                    print(\"df head: \", _df.head(5))\n",
    "                    print(\"_df head: \", _df.head(5))\n",
    "                    continue\n",
    "    print(\"Number of reports: \", len(df_list))\n",
    "    return pd.concat(df_list)[[\"text\", \"class\", \"rid\", \"index_within_rid\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_line_text(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    For Transformer to work, there has to be text in the text column. If there is no text, the text column is removed.\n",
    "    \"\"\"\n",
    "    df = df.dropna(subset=[\"text\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_line_class(df: pd.DataFrame):\n",
    "    \"\"\"Cleans the dataframe labels in \"class\".\n",
    "    1) Removes whitespace from beginning and end of text\n",
    "    2) Correct spelling mistakes\n",
    "    3) Fill NaN values with \"unk\"\n",
    "    4) Create a new column \"class_agg\" with the aggregated classes of the original approach.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Class mapping spelling mistakes\n",
    "    class_mapping_spelling = {\n",
    "        'memds': 'medms',\n",
    "        'hs': 'his',\n",
    "        'm': 'mr',\n",
    "    }\n",
    "    class_mapping_agg = {\n",
    "        'his': 'his_sym_cu',\n",
    "        'sym': 'his_sym_cu',\n",
    "        'cu': 'his_sym_cu',\n",
    "        'labr': 'labr_labo',\n",
    "        'labo': 'labr_labo',\n",
    "        'to': 'to_tr',\n",
    "        'tr': 'to_tr',\n",
    "        'medo': 'medo_unk_do_so',\n",
    "        'unk': 'medo_unk_do_so',\n",
    "        'do': 'medo_unk_do_so',\n",
    "        'so': 'medo_unk_do_so',\n",
    "    }\n",
    "\n",
    "    # Cleaning the class column\n",
    "    df['class'] = df['class'].str.strip() \\\n",
    "                             .replace(class_mapping_spelling) \\\n",
    "                             .fillna(\"unk\") \n",
    "\n",
    "    # Creating a new column with the aggregated classes\n",
    "    df['class_agg'] = df['class'].replace(class_mapping_agg)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_clean_line_df():\n",
    "    \"\"\"Loads and cleans the dataframe from the load_line_labelling function.\n",
    "    \"\"\"\n",
    "    df = load_line_labelling()\n",
    "    df = clean_line_text(df)\n",
    "    df = clean_line_class(df)\n",
    "    print(df.columns)\n",
    "\n",
    "    return df[[\"rid\", \"index_within_rid\",\"text\", \"class\", \"class_agg\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_clean_line_df()\n",
    "display(df.head(5))\n",
    "\n",
    "# Class distribution\n",
    "print(\"class_agg values: \", df.class_agg.value_counts())\n",
    "print(\"class values: \", df[\"class\"].value_counts())\n",
    "\n",
    "# Number of reports with missing values in class_agg\n",
    "print(\"Number of reports with missing values in class_agg: \", df[df[\"class_agg\"].isnull()].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory\n",
    "os.makedirs(os.path.join(paths.DATA_PATH_PREPROCESSED, \"line-label\"), exist_ok=True)\n",
    "# Save the dataframe\n",
    "df.to_csv(os.path.join(paths.DATA_PATH_PREPROCESSED, \"line-label/line-label_clean.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train test split by rid\n",
    "rids = df.rid.unique()\n",
    "rids_train, rids_test = train_test_split(rids, test_size=0.2, random_state=42)\n",
    "rids_train, rids_val = train_test_split(rids_train, test_size=0.2, random_state=42)\n",
    "\n",
    "df_train = df[df.rid.isin(rids_train)]\n",
    "df_val = df[df.rid.isin(rids_val)]\n",
    "df_test = df[df.rid.isin(rids_test)]\n",
    "\n",
    "# Save the train, val and test dataframes\n",
    "df_train.to_csv(os.path.join(paths.DATA_PATH_PREPROCESSED, \"line-label/line-label_clean_train.csv\"))\n",
    "\n",
    "df_val.to_csv(os.path.join(paths.DATA_PATH_PREPROCESSED, \"line-label/line-label_clean_val.csv\"))\n",
    "\n",
    "df_test.to_csv(os.path.join(paths.DATA_PATH_PREPROCESSED, \"line-label/line-label_clean_test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all of the data for dataset clf2 construction\n",
    "df_all = pd.read_csv(paths.DATA_PATH_PREPROCESSED/\"midatams/seantis_kisim.csv\")\n",
    "df_all = df_all[[\"research_id\", \"index_within_rid\",\"text\"]].rename(columns={\"research_id\": \"rid\"})\n",
    "df_all[\"class_agg\"] = None\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HuggingFace Dataset\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train),\n",
    "    \"val\": Dataset.from_pandas(df_val),\n",
    "    \"test\": Dataset.from_pandas(df_test),\n",
    "})\n",
    "\n",
    "# Drop unnecessary columns\n",
    "dataset = dataset.remove_columns([\"class\", \"__index_level_0__\"])\n",
    "\n",
    "# Add the all dataset\n",
    "dataset[\"all\"] = Dataset.from_pandas(df_all)\n",
    "\n",
    "# Create labels \n",
    "dataset = dataset.map(lambda example: {\"labels\":[line_label_label2id.get(e, None) for e in example[\"class_agg\"]]}, batched=True)\n",
    "\n",
    "# Save the dataset\n",
    "dataset.save_to_disk(os.path.join(paths.DATA_PATH_PREPROCESSED, \"line-label/line-label_clean_dataset\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of reports\n",
    "print(\"Number of reports: \", len(df[\"rid\"].unique()))\n",
    "\n",
    "# Number of lines\n",
    "print(\"Number of lines: \", df.shape[0])\n",
    "\n",
    "# Value counts of class_agg\n",
    "print(\"Value counts of class: \", df[\"class\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Level Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a preprocessing step to label all the observations in seantis_kisim.csv, the dataset containing the reports in line splitted format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset needs columns rid, text, class_agg. The text column contains one line of a report per row. \n",
    "# The class_agg column can have all Null values, it will be filled by the lineclass model.\n",
    "\n",
    "df_all = pd.read_csv(paths.DATA_PATH_PREPROCESSED/\"midatams/seantis_kisim.csv\")\n",
    "df_all = df_all[[\"research_id\", \"text\"]].rename(columns={\"research_id\": \"rid\"})\n",
    "df_all[\"class_agg\"] = None\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(paths.DATA_PATH_PREPROCESSED/\"line-label/line-label_clean_train.csv\")\n",
    "df_val = pd.read_csv(paths.DATA_PATH_PREPROCESSED/\"line-label/line-label_clean_val.csv\")\n",
    "df_test = pd.read_csv(paths.DATA_PATH_PREPROCESSED/\"line-label/line-label_clean_test.csv\")\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train),\n",
    "    \"val\": Dataset.from_pandas(df_val),\n",
    "    \"test\": Dataset.from_pandas(df_test),\n",
    "    \"all\": Dataset.from_pandas(df_all)\n",
    "})\n",
    "\n",
    "df_token_train = prepare_pd_dataset_for_lineclass(df_train)\n",
    "df_token_val = prepare_pd_dataset_for_lineclass(df_val)\n",
    "df_token_test = prepare_pd_dataset_for_lineclass(df_test)\n",
    "df_token_all = prepare_pd_dataset_for_lineclass(df_all)\n",
    "\n",
    "dataset_token = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_token_train),\n",
    "    \"val\": Dataset.from_pandas(df_token_val),\n",
    "    \"test\": Dataset.from_pandas(df_token_test),\n",
    "    \"all\": Dataset.from_pandas(df_token_all)\n",
    "})\n",
    "\n",
    "# Add a few of the validation examples to the training set\n",
    "dataset_token[\"train\"] = concatenate_datasets([dataset_token[\"train\"], dataset_token[\"val\"].select(range(8))])\n",
    "dataset_token[\"val\"] = dataset_token[\"val\"].select(range(8, len(dataset_token[\"val\"])))\n",
    "\n",
    "# Save the tokenized datasets\n",
    "dataset_token.save_to_disk(paths.DATA_PATH_PREPROCESSED/\"line-label/line-label_for_token_classification\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
