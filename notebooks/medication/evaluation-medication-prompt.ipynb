{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "9e927d5c-5913-48ef-a16e-6b0dfcca48ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "from src import paths\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from src.utils import get_default_pydantic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "08f3baf4-accd-4b0c-a245-f61d6d985586",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "results = torch.load(paths.RESULTS_PATH/\"medication/medication_outlines_Llama2-MedTuned-7b_4bit__shot_instruction_examples_10.pt\")\n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "a14ec3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to fix broken JSON by searching for last \"}\" then adding \"]\"\n",
    "default_model = get_default_pydantic_model(\"medication\")\n",
    "def fix_json(json_str):\n",
    "    last_index = json_str.rfind(\"}\")\n",
    "    if last_index != -1:  # If \"},\" is found\n",
    "        fixed_json = json_str[:last_index + 1] + \"]}\"\n",
    "\n",
    "        return fixed_json\n",
    "    else:\n",
    "        # If \"},\" is not found, return the default_model\n",
    "        return default_model.model_dump_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "7c347c79-741c-420b-a8ae-5f857df84861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix model_answers wherever successful is False\n",
    "_df_fixed = df[~df[\"successful\"]].apply(lambda row: fix_json(row[\"model_answers\"]), axis=1)\n",
    "df_fixed = df.copy()\n",
    "df_fixed.loc[~df[\"successful\"], \"model_answers\"] = _df_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ed62cfe4-bd02-47d2-a29d-c9ebeaa69cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for idx, (answer, text) in enumerate(zip(df_fixed[\"model_answers\"], df_fixed[\"text\"])):\n",
    "    try:\n",
    "        answer = json.loads(answer)\n",
    "        medications = answer[\"medications\"]\n",
    "        for med in medications:\n",
    "            med[\"text\"] = text\n",
    "            med[\"id\"] = idx\n",
    "            dfs.append((med))\n",
    "    except:\n",
    "        print(f\"Error at index {idx}\")\n",
    "res = pd.DataFrame(dfs)\n",
    "\n",
    "# If all the values of morning, noon, evening, night are 0, then set them all to -99\n",
    "def check_and_replace(df, cols_to_check):\n",
    "    \"\"\"\n",
    "    Check if 4 specified columns in each row are all 0,\n",
    "    then replace those 4 columns with -99.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): Input DataFrame.\n",
    "        cols_to_check (list): List of column names to check.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with replacements.\n",
    "    \"\"\"\n",
    "    for index, row in df.iterrows():\n",
    "        if (row[cols_to_check] == 0).sum() >= 4:\n",
    "            df.loc[index, cols_to_check] = -99\n",
    "    return df\n",
    "\n",
    "res = check_and_replace(res, [\"morning\", \"noon\", \"evening\", \"night\"])\n",
    "\n",
    "# Convert everything to string and lowercase\n",
    "res = res.map(lambda x: str(x).lower())\n",
    "\n",
    "# Remove .0+ from every string\n",
    "expression = r\"\\.0+$\"\n",
    "res = res.replace(expression, \"\", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faca820",
   "metadata": {},
   "source": [
    "- False Positives (FP): Predicted but not in ground truth. So if a dose for a tp medication is predicted wrongly, or if a medication was predicted that is not in the ground truth (and thus also a dose was predicted that is not in the ground truth)\n",
    "- True Positives (TP): Predicted and in ground truth.\n",
    "- False Negatives (FN): Not predicted but in ground truth. This can only happen if the medication is not predicted as if it is predicted it will always also predict a unit.\n",
    "- True Negatives (TN): Not predicted and not in ground truth (not relevant for this task)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28a0fbf",
   "metadata": {},
   "source": [
    "Stuff it doesn't catch\n",
    "- Spelling mistakes: defalgan instead of dafalgan. Example 73 for E/ml vs IE/ml. Example 79 for wrong schema (but I would also not know what to do)\n",
    "- Sometimes didn't write full name or splits it up (Irfen Dolo; Excipial U Lipolotion)\n",
    "- A lot of times if no schema was there it just put 0 0 0 0 for the intake. It had a tendency to write 0 (instead of -99) for stuff it didn't find. Also 1-0-0-0 not really safe. The rest seem robust.\n",
    "    - Could convert to -99 if all of them are 0 which doesn't make sense anyways. Check if better metrics.\n",
    "- Couldn't catch split stuff in dose like 80/10, 800/160\n",
    "- Text 48 and 60 is good examples of what rule based dosis intake would not catch\n",
    "- Also example 63 for dosis (2 f√ºr beideseitig)\n",
    "- Example 68 for something you had to google as well (no mg, so model puts Filmtabl)\n",
    "- Example 77 for something that is very hard to catch for a model like this\n",
    "- Example 78 for hallucination of medication (because Insurance sounds like medication)\n",
    "- Example 87 for a hard case (with no medication implicit), could probably be alleviated with correct example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feaf290",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "dbbee3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_excel(paths.RESULTS_PATH/\"medication/labels.xlsx\") \n",
    "\n",
    "# Convert everything to string and lowercase\n",
    "labels = labels.map(lambda x: str(x).lower())\n",
    "\n",
    "# Remove .0+ from every string\n",
    "expression = r\"\\.0+$\"\n",
    "labels = labels.replace(expression, \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "66095e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose only the relevant columns\n",
    "relevant_columns = [\"name\", \"dose\", \"dose_unit\", \"morning\", \"noon\", \"evening\", \"night\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "d57c19ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall(ground_truth, predicted):\n",
    "    ground_truth = ground_truth.copy()\n",
    "    predicted = predicted.copy()\n",
    "    true_positives = {}\n",
    "    false_positives = {}\n",
    "    false_negatives = {}\n",
    "    \n",
    "    for pred in predicted:\n",
    "        pred_name = pred[\"name\"]\n",
    "        true_positives.setdefault(\"name\", 0)\n",
    "        matched = False\n",
    "        for i, truth in enumerate(ground_truth):\n",
    "            if truth[\"name\"] in pred_name or pred_name in truth[\"name\"]: # First we match the medication to the corresponding ground truth\n",
    "                matched = True\n",
    "                pred.pop(\"name\") # Remove name and put true positive\n",
    "                true_positives[\"name\"] += 1\n",
    "                for key in pred: # Then iterate over the keys and count the true positives and false positives\n",
    "                    if pred[key] == truth[key]:\n",
    "                        true_positives.setdefault(key, 0)\n",
    "                        true_positives[key] += 1\n",
    "                    else:\n",
    "                        false_positives.setdefault(key, 0)\n",
    "                        false_positives[key] += 1\n",
    "                        \n",
    "                del ground_truth[i]  # Remove the matched item\n",
    "                break  # Move to the next predicted item\n",
    "        \n",
    "        if not matched: # If there is no medication in the ground truth that matches, then it is a false positive for all keys\n",
    "            for key in pred:\n",
    "                false_positives.setdefault(key, 0)\n",
    "                false_positives[key] += 1\n",
    "    for truth in ground_truth:\n",
    "        for key in truth:\n",
    "            false_negatives.setdefault(key, 0)\n",
    "            false_negatives[key] += 1\n",
    "    \n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    f1_score = {}\n",
    "\n",
    "    if len(predicted) == 0:\n",
    "        true_positives = {key: 0 for key in ground_truth[0].keys()}\n",
    "        false_positives = {key: 0 for key in ground_truth[0].keys()}\n",
    "\n",
    "    for key in relevant_columns:\n",
    "        # Precision: TP / (TP + FP)\n",
    "        precision[key] = true_positives.get(key, 0) / (true_positives.get(key, 0) + false_positives.get(key, 0) + 1e-10)\n",
    "\n",
    "        # Recall: TP / (TP + FN)\n",
    "        recall[key] = true_positives.get(key, 0) / (true_positives.get(key, 0) + false_negatives.get(key, 0) + 1e-10)\n",
    "\n",
    "        # F1 Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "        f1_score[key] = 2 * (precision[key] * recall[key]) / (precision[key] + recall[key] + 1e-10)\n",
    "    \n",
    "    # Catch NA values in true_positives\n",
    "    \n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "evaluated = []\n",
    "assert len(labels.id.unique()) == len(res.id.unique())\n",
    "\n",
    "for idx in labels.id.unique():\n",
    "    ground_truth = labels[labels.id == idx][relevant_columns].to_dict(\"records\")\n",
    "    predicted = res[res.id == idx][relevant_columns].to_dict(\"records\")\n",
    "    precision, recall, f1_score = calculate_precision_recall(ground_truth, predicted)\n",
    "    precision_dict = {f\"precision_{key}\": value for key, value in precision.items()}\n",
    "    recall_dict = {f\"recall_{key}\": value for key, value in recall.items()}\n",
    "    f1_score_dict = {f\"f1_score_{key}\": value for key, value in f1_score.items()}\n",
    "    merged = {**precision_dict, **recall_dict, **f1_score_dict}\n",
    "    merged[\"text\"] = res[res.id == idx][\"text\"].values[0]\n",
    "    merged[\"id\"] = idx\n",
    "    evaluated.append(merged)\n",
    "\n",
    "evaluated_df = pd.DataFrame(evaluated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "89dd3c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = evaluated_df.drop(columns=[\"id\", \"text\"]).mean()\n",
    "agg_df[\"intake_metrics_precision\"] = agg_df[[\"precision_morning\", \"precision_noon\", \"precision_evening\", \"precision_night\"]].mean()\n",
    "agg_df[\"intake_metrics_recall\"] = agg_df[[\"recall_morning\", \"recall_noon\", \"recall_evening\", \"recall_night\"]].mean()\n",
    "agg_df[\"intake_metrics_f1_score\"] = agg_df[[\"f1_score_morning\", \"f1_score_noon\", \"f1_score_evening\", \"f1_score_night\"]].mean()\n",
    "agg_df.drop([\"precision_morning\", \"precision_noon\", \"precision_evening\", \"precision_night\", \"recall_morning\", \"recall_noon\", \"recall_evening\", \"recall_night\", \"f1_score_morning\", \"f1_score_noon\", \"f1_score_evening\", \"f1_score_night\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "25ba3f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision_name              0.911667\n",
       "precision_dose              0.707667\n",
       "precision_dose_unit         0.676667\n",
       "recall_name                 0.842143\n",
       "recall_dose                 0.651000\n",
       "recall_dose_unit            0.635143\n",
       "f1_score_name               0.856270\n",
       "f1_score_dose               0.660603\n",
       "f1_score_dose_unit          0.639603\n",
       "intake_metrics_precision    0.592292\n",
       "intake_metrics_recall       0.560083\n",
       "intake_metrics_f1_score     0.561190\n",
       "dtype: float64"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d3a671",
   "metadata": {},
   "source": [
    "precision_name              0.978333\n",
    "precision_dose              0.868333\n",
    "precision_dose_unit         0.908333\n",
    "recall_name                 0.988238\n",
    "recall_dose                 0.886905\n",
    "recall_dose_unit            0.926905\n",
    "f1_score_name               0.981120\n",
    "f1_score_dose               0.874612\n",
    "f1_score_dose_unit          0.914612\n",
    "intake_metrics_precision    0.455167\n",
    "intake_metrics_recall       0.473750\n",
    "intake_metrics_f1_score     0.461512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba099b20",
   "metadata": {},
   "source": [
    "precision_name              0.978333\n",
    "precision_dose              0.868333\n",
    "precision_dose_unit         0.908333\n",
    "recall_name                 0.988238\n",
    "recall_dose                 0.886905\n",
    "recall_dose_unit            0.926905\n",
    "f1_score_name               0.981120\n",
    "f1_score_dose               0.874612\n",
    "f1_score_dose_unit          0.914612\n",
    "intake_metrics_precision    0.601417\n",
    "intake_metrics_recall       0.622750\n",
    "intake_metrics_f1_score     0.608456\n",
    "dtype: float64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
