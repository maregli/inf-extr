{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, DataCollatorWithPadding\n",
    "\n",
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path(__file__).parent.parent.parent.parent))\n",
    "\n",
    "from src import paths\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from typing import Tuple\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "MODEL_PATH = paths.MODEL_PATH/'llama2-chat'\n",
    "QUANTIZATION = \"4bit\"\n",
    "\n",
    "SPLIT = \"train\"\n",
    "\n",
    "BASE_PROMPT = \"<s>[INST]\\n<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{user_prompt}[/INST]\\n\\n{answer_init}\"\n",
    "SYSTEM_PROMP = \"Is the MS diagnosis in the text of type \\\"Sekundär progrediente Multiple Sklerose (SPMS)\\\", \\\"primäre progrediente Multiple Sklerose (PPMS)\\\" or \\\"schubförmig remittierende Multiple Sklerose (RRMS)\\\"?\"\n",
    "ANSWER_INIT = \"Based on the information provided in the text, the most likely diagnosis for the patient is: \"\n",
    "TRUNCATION_SIZE = 300\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "DO_SAMPLE = False\n",
    "NUM_BEAMS = 1\n",
    "MAX_NEW_TOKENS = 20\n",
    "TEMPERATURE = 1\n",
    "TOP_P = 1\n",
    "TOP_K = 4\n",
    "PENALTY_ALPHA = 0.0\n",
    "\n",
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        for gpu_id in range(num_gpus):\n",
    "            free_mem, total_mem = torch.cuda.mem_get_info(gpu_id)\n",
    "            gpu_properties = torch.cuda.get_device_properties(gpu_id)\n",
    "            print(f\"GPU {gpu_id}: {gpu_properties.name}\")\n",
    "            print(f\"   Total Memory: {total_mem / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Free Memory: {free_mem / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Allocated Memory : {torch.cuda.memory_allocated(gpu_id) / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Reserved Memory : {torch.cuda.memory_reserved(gpu_id) / (1024 ** 3):.2f} GB\")\n",
    "    else:\n",
    "        print(\"No GPU available.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample prompts.\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "# Create a sampling params object.\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "\n",
    "# Create an LLM.\n",
    "llm = LLM(model=, quantization=\"AWQ\")\n",
    "# Generate texts from the prompts. The output is a list of RequestOutput objects\n",
    "# that contain the prompt, generated text, and other information.\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Model and tokenizer\n",
    "\n",
    "def load_model_and_tokenizer(model_path:os.PathLike, quantization:str = QUANTIZATION)->Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"Loads the model and tokenizer from the given path and returns the compiled model and tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        model_path (os.PathLike): Path to the model\n",
    "        quantization (str, optional): Quantization. Must be one of 4bit or bfloat16. Defaults to QUANTIZATION.\n",
    "\n",
    "        Returns:\n",
    "            tuple(AutoModelForCausalLM, AutoTokenizer): Returns the compiled model and tokenizer\n",
    "            \n",
    "    \"\"\"\n",
    "    # ### Model\n",
    "    if quantization == \"bfloat16\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "    elif quantization == \"4bit\":\n",
    "        bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                        bnb_4bit_use_double_quant=True,\n",
    "                                        bnb_4bit_quant_type=\"nf4\",\n",
    "                                        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", quantization_config=bnb_config)\n",
    "    else:\n",
    "        raise ValueError(\"Quantization must be one of 4bit or bfloat16\")\n",
    "    \n",
    "    ### Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"left\")\n",
    "\n",
    "    # Check if the pad token is already in the tokenizer vocabulary\n",
    "    if '<pad>' not in tokenizer.get_vocab():\n",
    "        # Add the pad token\n",
    "        tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "    \n",
    "\n",
    "    #Resize the embeddings\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    #Configure the pad token in the model\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Check if they are equal\n",
    "    assert model.config.pad_token_id == tokenizer.pad_token_id, \"The model's pad token ID does not match the tokenizer's pad token ID!\"\n",
    "\n",
    "    # Print the pad token ids\n",
    "    print('Tokenizer pad token ID:', tokenizer.pad_token_id)\n",
    "    print('Model pad token ID:', model.config.pad_token_id)\n",
    "    print('Model config pad token ID:', model.config.pad_token_id)\n",
    "    print(\"Vocabulary Size with Pad Token: \", len(tokenizer))\n",
    "\n",
    "    return torch.compile(model), tokenizer # Compile Model for faster inference. # To-Do https://pytorch.org/blog/pytorch-compile-to-speed-up-inference/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data()->DatasetDict:\n",
    "    \"\"\"Loads the data for MS-Diag task and returns the dataset dictionary\n",
    "    \n",
    "    Returns:\n",
    "        DatasetDict: Returns the dataset dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    data_files = {\"train\": \"ms-diag_clean_train.csv\", \"validation\": \"ms-diag_clean_val.csv\", \"test\": \"ms-diag_clean_test.csv\"}\n",
    "\n",
    "    df = load_dataset(os.path.join(paths.DATA_PATH_PREPROCESSED,'ms-diag'), data_files = data_files)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_data(df:DatasetDict, tokenizer:AutoTokenizer, split:str=SPLIT, truncation_size:int = TRUNCATION_SIZE)->list[str]:\n",
    "    \"\"\"Returns a list of input texts for the classification task\n",
    "    \n",
    "    Args:\n",
    "        df (DatasetDict): Dataset dictionary\n",
    "        tokenizer (AutoTokenizer): Tokenizer\n",
    "        split (str, optional): Split. Defaults to SPLIT.\n",
    "        truncation_size (int, optional): Truncation size. Defaults to TRUNCATION_SIZE.\n",
    "        \n",
    "    Returns:\n",
    "        list(str): Returns a list of input texts for the classification task\n",
    "    \"\"\"\n",
    "\n",
    "    def format_prompt(text:str)->str:\n",
    "        \"\"\"Truncates the text to the given truncation size and formats the prompt.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text\n",
    "        \n",
    "        Returns:\n",
    "            str: Returns the formatted prompt\n",
    "        \"\"\"\n",
    "        if len(text) > truncation_size:\n",
    "            text = text[:truncation_size]\n",
    "        else:\n",
    "            text = text\n",
    "        input = BASE_PROMPT.format(system_prompt = SYSTEM_PROMP,\n",
    "                                user_prompt = text,\n",
    "                                answer_init = ANSWER_INIT)\n",
    "\n",
    "        return input\n",
    "\n",
    "    \n",
    "    # Tokenize the text\n",
    "    if split == \"all\":\n",
    "        text = df[\"train\"][\"text\"] + df[\"validation\"][\"text\"] + df[\"test\"][\"text\"]\n",
    "    else:\n",
    "        text = df[split][\"text\"]\n",
    "\n",
    "    tokens = [tokenizer(format_prompt(t)) for t in text]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def get_DataLoader(tokens:list[str], tokenizer:AutoTokenizer, batch_size:int = BATCH_SIZE, padding:bool = True)->DataLoader:\n",
    "    \"\"\"Returns a DataLoader for the given dataset dictionary\n",
    "    \n",
    "    Args:\n",
    "        tokens (List(str)): List of input texts\n",
    "        tokenizer (AutoTokenizer): Tokenizer\n",
    "        batch_size (int, optional): Batch size. Defaults to BATCH_SIZE.\n",
    "        padding (bool, optional): Padding. Defaults to True.\n",
    "        \n",
    "    Returns:\n",
    "        DataLoader: Returns a DataLoader for the given dataset dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    # Default collate function \n",
    "    collate_fn = DataCollatorWithPadding(tokenizer, padding=padding)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset=tokens, collate_fn=collate_fn, batch_size=batch_size, shuffle = False) \n",
    "\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "\n",
    "    # Load Data, Model and Tokenizer\n",
    "    df = load_data()\n",
    "\n",
    "    print(\"GPU Memory before Model is loaded:\\n\")\n",
    "    check_gpu_memory()\n",
    "    model, tokenizer = load_model_and_tokenizer(MODEL_PATH, quantization=QUANTIZATION)\n",
    "    print(\"GPU Memory after Model is loaded:\\n\")\n",
    "    check_gpu_memory()\n",
    "\n",
    "    # Prepare Data\n",
    "    tokens = prepare_data(df, tokenizer, split=SPLIT, truncation_size=TRUNCATION_SIZE)\n",
    "\n",
    "    # Get DataLoader\n",
    "    dataloader = get_DataLoader(tokens, tokenizer, batch_size=BATCH_SIZE, padding=True)\n",
    "\n",
    "    # Inference\n",
    "    outputs = []\n",
    "\n",
    "    for idx, batch in enumerate(tqdm.tqdm(dataloader)):\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "        with torch.inference_mode():\n",
    "            generated_ids = model.generate(input_ids = input_ids, \n",
    "                                           attention_mask = attention_mask,\n",
    "                                            max_new_tokens=MAX_NEW_TOKENS, \n",
    "                                            num_beams=NUM_BEAMS, \n",
    "                                            do_sample=DO_SAMPLE, \n",
    "                                            temperature = TEMPERATURE, \n",
    "                                            num_return_sequences = 1, \n",
    "                                            top_p = TOP_P,\n",
    "                                            top_k = TOP_K,\n",
    "                                            penalty_alpha = PENALTY_ALPHA).to(\"cpu\")\n",
    "    \n",
    "        outputs.append(tokenizer.batch_decode(generated_ids, skip_special_tokens=True))\n",
    "        print(\"Memory after batch {}:\\n\".format(idx))\n",
    "        check_gpu_memory()\n",
    "\n",
    "    # Save results\n",
    "    outputs = list(chain.from_iterable(outputs))\n",
    "    results = [out.split(ANSWER_INIT)[1] for out in outputs]\n",
    "    \n",
    "    # Add Arguments as last row to the results\n",
    "    results.append(str(args))\n",
    "\n",
    "    file_name = f\"ms_diag-llama2-chat_zero-shot_{JOB_ID}.csv\"\n",
    "    pd.Series(results).to_csv(paths.RESULTS_PATH/file_name)\n",
    "\n",
    "    return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf-extr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
