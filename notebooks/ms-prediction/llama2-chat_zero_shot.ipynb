{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "from src import paths\n",
    "\n",
    "from src.utils import (load_model_and_tokenizer, \n",
    "                       load_ms_data,  \n",
    "                       check_gpu_memory,\n",
    "                        get_format_fun,\n",
    "                        format_prompt,\n",
    ")\n",
    "\n",
    "import argparse\n",
    "\n",
    "from transformers import DataCollatorWithPadding, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, AutoModel, TextStreamer\n",
    "\n",
    "\n",
    "from datasets import concatenate_datasets, Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "import json\n",
    "\n",
    "import outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import outlines.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLAMA MedTuned 13B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Llama2-MedTuned-13b-LoRa-merged\"\n",
    "# MODEL_NAME = \"Llama2-MedTuned-13b\"\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(model_name = MODEL_NAME,\n",
    "                                            task_type = \"clm\",\n",
    "                                            quantization = \"4bit\",\n",
    "                                           attn_implementation = \"flash_attention_2\"\n",
    "                                           )\n",
    "model.config.use_cache = True\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Task instructions, system prompt and examples\n",
    "with open(paths.DATA_PATH_PREPROCESSED/\"ms-diag/task_instruction.txt\", \"r\") as f:\n",
    "    task_instruction = f.read()\n",
    "\n",
    "with open(paths.DATA_PATH_PREPROCESSED/\"ms-diag/system_prompt.txt\", \"r\") as f:\n",
    "    system_prompt = f.read()\n",
    "\n",
    "with open(paths.DATA_PATH_PREPROCESSED/\"ms-diag/examples.json\", \"r\") as f:\n",
    "    examples = json.load(f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_ms_data(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"train\"][\"text\"][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_fun = get_format_fun(\"zero_shot_instruction\")\n",
    "test = format_prompt([df[\"train\"][\"text\"][9]], format_fun, examples = examples, task_instruction = task_instruction, system_prompt = system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(test, return_tensors = \"pt\").to(device)\n",
    "streamer = TextStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.generate(**inputs, streamer=streamer, max_new_tokens=200, num_beams = 1, do_sample = False, top_p = 1, temperature = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.generate(**inputs, streamer=streamer, max_new_tokens=200, num_beams = 1, do_sample = False, top_p = 1, temperature = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = [\"[INST]Schreibe einen medizinischen Bericht: [/INST]\"]\n",
    "inputs1 = tokenizer(test1, return_tensors = \"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.generate(**inputs1, streamer=streamer, max_new_tokens=200, num_beams = 1, do_sample = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"primary progressive multiple sclerosis\", \"secondary progressive multiple sclerosis\",\n",
    "          \"relapsing remitting multiple sclerosis\",\"not enough info\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = get_hidden_state([\" \"], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_line = load_ms_data(\"line\")\n",
    "df_all = load_ms_data(\"all\")\n",
    "df_first_last = load_ms_data(\"all_first_line_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to make output more consistent by stopping on MS, https://github.com/huggingface/transformers/issues/26959\n",
    "class EosListStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, eos_sequence = [835, 2799, 4080, 29901]):\n",
    "        self.eos_sequence = eos_sequence\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        last_ids = input_ids[:,-len(self.eos_sequence):].tolist()\n",
    "        return self.eos_sequence in last_ids\n",
    "ms_stop = EosListStoppingCriteria(tokenizer(\"multiple sclerosis\", add_special_tokens = False)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(bos_token_id = tokenizer.bos_token_id,\n",
    "                                     eos_token_id = tokenizer.eos_token_id,\n",
    "                                     pad_token_id = tokenizer.pad_token_id,\n",
    "                                     use_cache = True,\n",
    "                                     max_new_tokens = 20,\n",
    "                                     temperature=1,\n",
    "                                     top_p=1,\n",
    "                                     do_sample=False,\n",
    "                                     output_hidden_states = False,\n",
    "                                     return_dict_in_generate = True,\n",
    "                                    )\n",
    "\n",
    "def single_round_inference(reports:list[str], \n",
    "                           model:AutoModelForCausalLM, \n",
    "                           tokenizer:AutoTokenizer, \n",
    "                           format_fun:Callable[[str],str], \n",
    "                           generation_config:GenerationConfig = None,\n",
    "                           prefix:str = \"[/INST]\",\n",
    "                           device:torch.device = torch.device(\"cpu\"),\n",
    "                           batch_size:int=1,\n",
    "                           output_hidden_states:bool = True)->dict:\n",
    "    \n",
    "    \"\"\" Single round inference for the MS extraction task\n",
    "    \n",
    "    Args:\n",
    "        reports (list[str]): list of medical reports\n",
    "        model (AutoModelForCausalLM): model\n",
    "        tokenizer (AutoTokenizer): tokenizer\n",
    "        format_fun (Callable[[str],str]): function to convert input text to desired prompt format\n",
    "        generation_config (GenerationConfig): generation config. Defaults to None. If None, default config is used.\n",
    "        prefix (str): prefix that separates input from output. Defaults to \"[/INST]\".\n",
    "        device (torch.device): device. Defaults to torch.device(\"cpu\").\n",
    "        batch_size (int): batch size. Defaults to 1.\n",
    "        output_hidden_states (bool); whether hidden states should be calculated for model answers. Defaults to True\n",
    "        \n",
    "    Returns:\n",
    "        dict: dictionary with keys report, prediction, last_hidden_states, input_lengths, whole_prompt\n",
    "            \n",
    "    \"\"\"\n",
    "    print(\"Starting Inference\")\n",
    "    tokens = [tokenizer(format_fun(t), add_special_tokens = False, truncation = True) for t in reports]\n",
    "    \n",
    "    collate_fn = DataCollatorWithPadding(tokenizer, padding=True)\n",
    "\n",
    "    dataloader = DataLoader(dataset=tokens, collate_fn=collate_fn, batch_size=batch_size, shuffle = False) \n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "    whole_prompt = []\n",
    "    last_hidden_states = []\n",
    "    input_lengths = [len(t[\"input_ids\"]) for t in tokens]\n",
    "\n",
    "    for idx, batch in enumerate(tqdm(dataloader)):\n",
    "        batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **batch,\n",
    "                generation_config=generation_config,\n",
    "            )\n",
    "\n",
    "        # Check GPU memory every 5 batches\n",
    "        if idx % 5 == 0:\n",
    "            check_gpu_memory()\n",
    "\n",
    "\n",
    "        return_tokens = outputs[\"sequences\"].to(\"cpu\")\n",
    "        batch_result = tokenizer.batch_decode(return_tokens, skip_special_tokens=True)\n",
    "        batch_result = [\" \" if result == \"\" else result for result in batch_result]\n",
    "        whole_prompt.extend(batch_result)\n",
    "        batch_result = [\" \" if result == \"\" else result.split(prefix)[-1].lower().strip() for result in batch_result]\n",
    "        print(\"batch results \", batch_result)\n",
    "\n",
    "        print(batch_result)\n",
    "\n",
    "        results.extend(batch_result)\n",
    "        del outputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Finished Inference\")\n",
    "\n",
    "    if output_hidden_states:\n",
    "        # This separate pass is not time but memory efficient as if the hidden states are calculated during inference,\n",
    "        # hidden states are calculated for every token in the batch and the generated sequence.\n",
    "        print(\"Starting Hidden State Calculation\")\n",
    "        last_hidden_states = get_hidden_state(results, model, tokenizer, device, batch_size)\n",
    "        print(\"Finished Hidden State Calculation\")\n",
    "\n",
    "    else:\n",
    "        last_hidden_states = None\n",
    "\n",
    "        \n",
    "    return {\"report\": reports, \n",
    "            \"prediction\": results, \n",
    "            \"last_hidden_states\": last_hidden_states, \n",
    "            \"input_lengths\":input_lengths,\n",
    "            \"whole_prompt\": whole_prompt}\n",
    "\n",
    "def multi_round_inference(reports:list[str], \n",
    "                          model:AutoModelForCausalLM, \n",
    "                          tokenizer:AutoTokenizer, \n",
    "                          format_fun1:Callable[[str],str],\n",
    "                          format_fun2:Callable[[str],str],\n",
    "                          generation_config:GenerationConfig = None,\n",
    "                          prefix:str = \"[/INST]\",\n",
    "                          device:torch.device = torch.device(\"cpu\"),\n",
    "                          batch_size:int = 1)->dict:\n",
    "    \n",
    "    \"\"\"Multi Round inference for the MS extraction task\n",
    "    \n",
    "    Args:\n",
    "        reports (list[str]): list of medical reports\n",
    "        model (AutoModelForCausalLM): model\n",
    "        tokenizer (AutoTokenizer): tokenizer\n",
    "        format_fun1 (Callable[str,str]): function to convert input text to desired prompt format\n",
    "        format_fun2 (Callable[str,str]): function to convert chat history to desired prompt format\n",
    "        generation_config (GenerationConfig): generation config. Defaults to None. If None, default config is used.\n",
    "        prefix (str): prefix that separates input from output. Defaults to \"[/INST]\".\n",
    "        device (torch.device): device. Defaults to torch.device(\"cpu\").\n",
    "        batch_size (int): batch size. Defaults to 1.\n",
    "        \n",
    "    Returns:\n",
    "        dict: dictionary with keys report, prediction, last_hidden_states, input_lengths, whole_prompt\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    max_new_tokens = generation_config.max_new_tokens\n",
    "\n",
    "    # For first round don't calculate hidden states and use long max_new_tokens, and low batch size\n",
    "    generation_config.max_new_tokens = 100\n",
    "    output_round1 = single_round_inference(reports=reports,\n",
    "                                           model=model, \n",
    "                                           tokenizer=tokenizer, \n",
    "                                           format_fun=format_fun1,\n",
    "                                           generation_config=generation_config,\n",
    "                                           device=device,\n",
    "                                           batch_size=1,\n",
    "                                           output_hidden_states=False)\n",
    "\n",
    "    # For second round calculate hidden states if desired\n",
    "    generation_config.max_new_tokens = max_new_tokens\n",
    "    chat_history = output_round1[\"whole_prompt\"]\n",
    "    chat_history = [text.split(\"[/INST]\")[-1] for text in chat_history]\n",
    "\n",
    "    return single_round_inference(reports=chat_history,\n",
    "                                  model=model, \n",
    "                                  tokenizer=tokenizer, \n",
    "                                  format_fun=format_fun2,\n",
    "                                  generation_config=generation_config,\n",
    "                                  prefix=prefix,\n",
    "                                  device=device,\n",
    "                                  batch_size=batch_size,\n",
    "                                  output_hidden_states=True)\n",
    "\n",
    "\n",
    "\n",
    "def get_hidden_state(text: list[str], model:AutoModel, tokenizer:AutoTokenizer, device:torch.device=torch.device(\"cpu\"), batch_size:int = 16)->list[torch.Tensor]:\n",
    "    \"\"\"Get hidden state of last layer of model for each prediction in results. Per default the last \n",
    "    \n",
    "    Args:\n",
    "        text (list[str]): a list of input strings to be encoded.\n",
    "        model (AutoModel): model\n",
    "        tokenizer (AutoTokenizer): tokenizer\n",
    "        device (torch.device): device. Defaults to torch.device(\"cpu\").\n",
    "        batch_size (int): batch size. Defaults to 16.\n",
    "        \n",
    "    Returns:\n",
    "        results (dict): results of prompting return with keys report, prediction, last_hidden_states, input_lengths, whole_prompt, encodings\n",
    "            \n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    dataset = Dataset.from_dict(tokenizer(text, add_special_tokens = False))\n",
    "    collate_fn = DataCollatorWithPadding(tokenizer = tokenizer, padding = \"longest\")\n",
    "\n",
    "    dataloader = DataLoader(dataset = dataset, batch_size = batch_size, collate_fn = collate_fn)\n",
    "    \n",
    "    encodings = []\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch, output_hidden_states = True)\n",
    "\n",
    "        last_hidden_state = outputs[\"hidden_states\"][-1]\n",
    "\n",
    "        # For decoder architectures the last token of the sequence contains information about the whole sequence\n",
    "        last_hidden_state = last_hidden_state[:, -1, :]\n",
    "        encodings.append(last_hidden_state)\n",
    "\n",
    "        del last_hidden_state\n",
    "        del outputs\n",
    "        del batch\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return torch.cat(encodings, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla\n",
    "\n",
    "Using the original prompt template of meta Llama2 creators. \\<s>[INST]<\\<SYS>>{system_prompt}<\\</SYS>>{instruction}{input}[/INST]\n",
    "You should set add special tokens to false for the tokenizer otherwise you will have double bos in the beginning of the prompt, if you state it. Gives more control.\n",
    "\n",
    "@misc{touvron2023llama,\r\n",
    "      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, \r\n",
    "      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},\r\n",
    "      year={2023},\r\n",
    "      eprint={2307.09288},\r\n",
    "      archivePrefix={arXiv},\r\n",
    "      primaryClarbage.\n",
    "\n",
    "This prompt template builds the foundation to all further strategies, otherwise the model's answers are kinda garbage.\n",
    "\n",
    "Hidden states is of format hidden_states (tuple(tuple(torch.FloatTensor)), optional, returned when output_hidden_states=True is passed or when config.output_hidden_states=True) — Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of torch.FloatTensor of shape (batch_size, generated_length, hidden_size). I will try working with the last hidden state of the first generated token as this is where the model will start it's generation/prediction from..CL}\r\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def zero_shot_base(report:str)->str:\n",
    "    \"\"\"Zero-shot base for the MS extraction task\n",
    "\n",
    "    Args:\n",
    "        report (str): medical report\n",
    "\n",
    "    Returns:\n",
    "        str: reformatted medical report with base\n",
    "\n",
    "    \"\"\"\n",
    "    base_prompt = \"<s>[INST]<<SYS>>{system_prompt}<</SYS>>\\n\\n{instruction}{input}[/INST]\\nThe type of multiple sclerosis stated in the german medical report is: \"\n",
    "    system_prompt =  (\"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \"\n",
    "                      \"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \"\n",
    "                       \"Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make \"\n",
    "                        \"any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t \"\n",
    "                        \"know the answer to a question, please don’t share false information.\\n\"\n",
    "                        )\n",
    "    instruction = (\"Your task is to extract the type of multiple Sclerosis (MS) stated in a German medical report. There are 3 types: \"\n",
    "                        \"\\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" and \\\"schubförmig remittierende Multiple Sklerose (RRMS)\\\".\"\n",
    "                        \"The type is provided in the text you just have to extract it. If you cannot match a type exactly answer with \\\"not enough info\\\".\"\n",
    "                        \"Your answer should solely consist of either \\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" \"\n",
    "                        \"\\schubförmige remittierende Multiple Sklerose (RRMS)\\\", or \\\"not enough info\\\".\"\n",
    "                        \"\\nHere is the medical report:\\n\"\n",
    "                    )\n",
    "    input = base_prompt.format(system_prompt = system_prompt, instruction = instruction, input =  report)\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = single_round_inference(reports = df_line[\"train\"].select(range(10))[\"text\"],\n",
    "                                 model=model, \n",
    "                                 tokenizer= tokenizer,\n",
    "                                 format_fun = zero_shot_base,\n",
    "                                generation_config = generation_config,\n",
    "                                 prefix = \"[/INST]\\nThe type of multiple sclerosis stated in the german medical report is: \",\n",
    "                                device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction\n",
    "\n",
    "Based on the paper of the creators of Llama2-MedTuned\n",
    "\n",
    "@misc{rohanian2023exploring,\r\n",
    "      title={Exploring the Effectiveness of Instruction Tuning in Biomedical Language Processing}, \r\n",
    "      author={Omid Rohanian and Mohammadmahdi Nouriborji and David A. Clifton},\r\n",
    "      year={2023},\r\n",
    "      eprint={2401.00579},\r\n",
    "      archivePrefix={arXiv},\r\n",
    "      primaryClass={cs\n",
    "\n",
    "Formulating the task as an instruction is closer to the fine-tuning of the model..CL}\r\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_instruction(report:str)->str:\n",
    "    \"\"\"Zero-shot instruction for the MS extraction task\n",
    "    \n",
    "    Args:\n",
    "        report (str): medical report\n",
    "        \n",
    "        Returns:\n",
    "            str: reformatted medical report with instruction\n",
    "            \n",
    "            \"\"\"\n",
    "    instruction_base_prompt = \"<s>[INST]\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}[/INST]\\n\\n### Output:\\n\"\n",
    "    task_instruction = (\"Your task is to extract the type of multiple Sclerosis (MS) stated in a German medical report. There are 3 types: \"\n",
    "                        \"\\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" and \\\"schubförmig remittierende Multiple Sklerose (RRMS)\\\".\"\n",
    "                        \"The type is provided in the text you just have to extract it. If you cannot match a type exactly answer with \\\"not enough info\\\".\"\n",
    "                        \"Your answer should solely consist of either \\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" \"\n",
    "                        \"\\schubförmige remittierende Multiple Sklerose (RRMS)\\\", or \\\"not enough info\\\".\"\n",
    "                        \"\\nHere is the medical report:\\n\"\n",
    "                    )\n",
    "    input = instruction_base_prompt.format(instruction = task_instruction, input =  report)\n",
    "\n",
    "    return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = single_round_inference(reports = df_line[\"train\"].select(range(10))[\"text\"],\n",
    "                                 model=model, \n",
    "                                 tokenizer= tokenizer,\n",
    "                                 format_fun = zero_shot_instruction,\n",
    "                                generation_config = generation_config,\n",
    "                                 prefix = \"[/INST]\\n\\n### Output:\\n\",\n",
    "                                device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot\n",
    "\n",
    "Original Paper suggesting this:\n",
    "\n",
    "@misc{brown2020language,\r\n",
    "      title={Language Models are Few-Shot Learners}, \r\n",
    "      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},\r\n",
    "      year={2020},\r\n",
    "      eprint={2005.14165},\r\n",
    "      archivePrefix={arXiv},\r\n",
    "      primaryClass={cs.CL}\r\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_base(report:str)->str:\n",
    "    \"\"\"Few Shot base for the MS extraction task\n",
    "\n",
    "    Args:\n",
    "        report (str): medical report\n",
    "\n",
    "    Returns:\n",
    "        str: reformatted medical report with base\n",
    "\n",
    "    \"\"\"\n",
    "    base_prompt = \"<s>[INST]<<SYS>>{system_prompt}<</SYS>>\\n\\n{instruction}Report:\\n{input}[/INST]\\nDiagnosis:\\n\"\n",
    "\n",
    "    rrms = 'Schubförmig-remittierende Multiple Sklerose, EM 01/2013, ED 10/2015\\nINDENT EDSS 05/2020: 2.0 [...]'\n",
    "    spms = '1. Sekundär progrediente schubförmige Multiple Sklerose [...]'\n",
    "    ppms = '1. Primär progrediente Multiple Sklerose, EM 1992, ED 1996, aktuell EDSS 7.0 [...]'\n",
    "    no_ms = '[...] INDENT MRI 07/2014: Progrediente supratentorielle MS-Plaques mit Befund-Progredienz im Bereich der Radiatio optica beidseits. [...]'\n",
    "\n",
    "    examples = [ppms, spms, rrms, no_ms]\n",
    "\n",
    "    labels = [\"Schubförmig remittierende Multiple Sklerose (RRMS)\",\n",
    "              \"Sekundär progrediente Multiple Sklerose (SPMS)\",\n",
    "              \"Primär progrediente Multiple Sklerosis (PPMS)\", \n",
    "              \"not enough info\"]\n",
    "    \n",
    "    system_prompt = (\n",
    "    \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \"\n",
    "    \"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \"\n",
    "    \"Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make \"\n",
    "    \"any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t \"\n",
    "    \"know the answer to a question, please don’t share false information.\\n\"\n",
    "    )\n",
    "\n",
    "    instruction = (\n",
    "       \"Your task is to extract the type of multiple Sclerosis (MS) stated in a German medical report. There are 3 types: \"\n",
    "        \"\\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" and \\\"schubförmig remittierende Multiple Sklerose (RRMS)\\\".\"\n",
    "        \"The type is provided in the text you just have to extract it. If you cannot match a type exactly answer with \\\"not enough info\\\".\"\n",
    "        \"Your answer should solely consist of either \\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" \"\n",
    "        \"\\\"schubförmige remittierende Multiple Sklerose (RRMS)\\\", or \\\"not enough info\\\".\"\n",
    "        \"To help you with your task, here are a few excerpts from reports that indiciate what output you should produce:\\n\\n\"\n",
    "        )\n",
    "    \n",
    "    for example, label in zip(examples, labels):\n",
    "        instruction += f\"Report:\\n{example}\\nDiagnosis:\\n{label}\\n\\n\"\n",
    "    \n",
    "    input = base_prompt.format(system_prompt = system_prompt, instruction = instruction, input =  report)\n",
    "    input + \"Diagnosis:\\n\"\n",
    "\n",
    "    return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = single_round_inference(df_line[\"train\"].filter(lambda e: e[\"labels\"] != 3).select(range(2))[\"text\"], model, tokenizer, few_shot_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_instruct(report:str)->str:\n",
    "    \"\"\"Few Shot base for the MS extraction task\n",
    "\n",
    "    Args:\n",
    "        report (str): medical report\n",
    "\n",
    "    Returns:\n",
    "        str: reformatted medical report with base\n",
    "\n",
    "    \"\"\"\n",
    "    base_prompt = \"<s>[INST]### Instruction:\\n{instruction}### Input:\\n{input}[/INST]\\n### Output:\\n\"\n",
    "\n",
    "    rrms = 'Schubförmig-remittierende Multiple Sklerose, EM 01/2013, ED 10/2015\\nINDENT EDSS 05/2020: 2.0 [...]'\n",
    "    spms = '1. Sekundär progrediente schubförmige Multiple Sklerose [...]'\n",
    "    ppms = '1. Primär progrediente Multiple Sklerose, EM 1992, ED 1996, aktuell EDSS 7.0 [...]'\n",
    "    no_ms = '[...] INDENT MRI 07/2014: Progrediente supratentorielle MS-Plaques mit Befund-Progredienz im Bereich der Radiatio optica beidseits. [...]'\n",
    "\n",
    "    examples = [ppms, spms, rrms, no_ms]\n",
    "\n",
    "    labels = [\"Schubförmig remittierende Multiple Sklerose (RRMS)\",\n",
    "              \"Sekundär progrediente Multiple Sklerose (SPMS)\",\n",
    "              \"Primär progrediente Multiple Sklerosis (PPMS)\", \n",
    "              \"not enough info\"]\n",
    "\n",
    "    instruction = (\n",
    "        \"Your task is to extract the type of multiple Sclerosis (MS) stated in a German medical report. There are 3 types: \"\n",
    "        \"\\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" and \\\"schubförmig remittierende Multiple Sklerose (RRMS)\\\".\"\n",
    "        \"The type is provided in the text you just have to extract it. If you cannot match a type exactly answer with \\\"not enough info\\\".\"\n",
    "        \"Your answer should solely consist of either \\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" \"\n",
    "        \"\\\"schubförmige remittierende Multiple Sklerose (RRMS)\\\", or \\\"not enough info\\\".\"\n",
    "        \"To help you with your task, here are a few excerpts from reports that indiciate what output you should produce:\\n\\n\"\n",
    "        )\n",
    "    \n",
    "    for example, label in zip(examples, labels):\n",
    "        instruction += f\"### Input:\\n{example}\\n### Output:\\n{label}\\n\\n\"\n",
    "    \n",
    "    input = base_prompt.format(instruction = instruction, input =  report)\n",
    "\n",
    "    return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = single_round_inference(df_line[\"train\"].filter(lambda e: e[\"labels\"] != 3).select(range(2))[\"text\"], model, tokenizer, few_shot_instruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_steps_one(report: str)->str:\n",
    "    \"\"\"Two Steps One for the MS extraction task. Encodes the report for first turn of the dialogue.\n",
    "\n",
    "    Args:\n",
    "        report (str): medical report\n",
    "\n",
    "    Returns:\n",
    "        str: reformatted medical report with base\n",
    "\n",
    "    \"\"\"\n",
    "    base_prompt = \"<s>[INST]<<SYS>>{system_prompt}<</SYS>>\\n\\n{instruction}{input}[/INST]\"\n",
    "    system_prompt =  (\"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \"\n",
    "                      \"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \"\n",
    "                       \"Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make \"\n",
    "                        \"any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t \"\n",
    "                        \"know the answer to a question, please don’t share false information.\\n\"\n",
    "                        )\n",
    "    instruction = (\"Your task is to extract relevant information about the multiple sclerosis diagnosis from the provided German medical report. \"\n",
    "                   \"Identify and summarize all sections discussing \\\"Multiple Sklerose\\\" paying attention to the exact type of multiple sclerosis. \"\n",
    "                   \"There are three types:\\n\"\n",
    "                   \"primär progrediente Multiple Sklerose (PPMS)\\n\"\n",
    "                   \"sekundär progrediente Multiple Sklerose (SPMS)\\n\"\n",
    "                   \"schubförmige Multiple Sklerose (RRMS)\\n\"\n",
    "                   \"If the report lacks information about multiple sclerosis, respond with \\\"not enough info\\\". \"\n",
    "                   \"\\nHere is the Medical Report:\\n \"\n",
    "                   )\n",
    "                   \n",
    "    input = base_prompt.format(system_prompt = system_prompt, instruction = instruction, input =  report)\n",
    "    return input\n",
    "\n",
    "def two_steps_two(chat_history: str)->str:\n",
    "    \"\"\"Two Steps Two for the MS extraction task. Encodes the chat history for second turn of the dialogue.\n",
    "\n",
    "    Args:\n",
    "        chat_history (str): chat history\n",
    "\n",
    "    Returns:\n",
    "        str: reformatted medical report with base\n",
    "\n",
    "    \"\"\"\n",
    "    base_prompt = \"<s>[INST]\\n\\n{instruction}\\n{summary}[/INST]\\nGiven the summary the most likely diagnosis is: \"\n",
    "    instruction = (\n",
    "                   \"Given a summary of a medical report describing a patient's condition related to multiple sclerosis, provide the most likely diagnosis. The possible diagnoses are:\\n\"\n",
    "                   \"- primär progrediente Multiple Sklerose (PPMS)\\n\"\n",
    "                   \"- sekundär progrediente Multiple Sklerose (SPMS)\\n\"\n",
    "                   \"- schubförmige Multiple Sklerose (RRMS)\\n\"\n",
    "                   \"- not enough info\\n\"\n",
    "                   \"Consider the information provided in the summary and select the diagnosis that best fits the patient's condition. If the summary does not contain sufficient information to make a diagnosis, choose \\\"not enough info.\\\" \"\n",
    "                   \"Here is the summary:\\n\"\n",
    "                   )\n",
    "    input = base_prompt.format(instruction = instruction, summary = chat_history)\n",
    "\n",
    "    return input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_round_inference(reports:list[str], \n",
    "                           model:AutoModelForCausalLM, \n",
    "                           tokenizer:AutoTokenizer, \n",
    "                           format_fun1:Callable[str,str],\n",
    "                          format_fun2:Callable[str,str],\n",
    "                           output_hidden_states:bool = True,\n",
    "                          max_new_tokens:int = 20)->pd.DataFrame:\n",
    "    \n",
    "    \"\"\"Multi Round inference for the MS extraction task\n",
    "    \n",
    "    Args:\n",
    "        reports (list[str]): list of medical reports\n",
    "        model (AutoModelForCausalLM): model\n",
    "        tokenizer (AutoTokenizer): tokenizer\n",
    "        format_fun1 (Callable[str,str]): function to convert input text to desired prompt format\n",
    "        format_fun2 (Callable[str,str]): function to convert chat history to desired prompt format\n",
    "        output_hidden_states (bool); whether hidden states should be calculated. Defaults to True\n",
    "        max_new_tokens (int): The number of tokens to be generated.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: results of inference\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    output_round1 = single_round_inference(reports, model, tokenizer, format_fun1, output_hidden_states = False, max_new_tokens = 2)\n",
    "    chat_history = output_round1[\"whole_prompt\"]\n",
    "\n",
    "    return single_round_inference(chat_history, model, tokenizer, format_fun2, output_hidden_states = output_hidden_states, max_new_tokens = max_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = multi_round_inference(df_line[\"train\"].filter(lambda e: e[\"labels\"] != 3).select(range(2))[\"text\"], model, tokenizer, two_steps_one, two_steps_two, max_new_tokens = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LLAMA MedTuned 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Llama2-MedTuned-7b\"\n",
    "model, tokenizer = load_model_and_tokenizer(model_name = MODEL_NAME,\n",
    "                                            task_type = \"clm\",\n",
    "                                            quantization = \"4bit\",\n",
    "                                           # attn_implementation = \"flash_attention_2\"\n",
    "                                           )\n",
    "model.config.use_cache = True\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(bos_token_id = tokenizer.bos_token_id,\n",
    "                                     eos_token_id = tokenizer.eos_token_id,\n",
    "                                     pad_token_id = tokenizer.pad_token_id,\n",
    "                                     use_cache = True,\n",
    "                                     max_new_tokens = 20,\n",
    "                                     temperature=1,\n",
    "                                     top_p=1,\n",
    "                                     do_sample=False,\n",
    "                                     output_hidden_states = False,\n",
    "                                     return_dict_in_generate = True,\n",
    "                                    )\n",
    "\n",
    "def single_round_inference(reports:list[str], \n",
    "                           model:AutoModelForCausalLM, \n",
    "                           tokenizer:AutoTokenizer, \n",
    "                           format_fun:Callable[[str],str], \n",
    "                           generation_config:GenerationConfig = None,\n",
    "                           prefix:str = \"[/INST]\",\n",
    "                           device:torch.device = torch.device(\"cpu\"),\n",
    "                           batch_size:int=1,\n",
    "                           output_hidden_states:bool = True)->dict:\n",
    "    \n",
    "    \"\"\" Single round inference for the MS extraction task\n",
    "    \n",
    "    Args:\n",
    "        reports (list[str]): list of medical reports\n",
    "        model (AutoModelForCausalLM): model\n",
    "        tokenizer (AutoTokenizer): tokenizer\n",
    "        format_fun (Callable[[str],str]): function to convert input text to desired prompt format\n",
    "        generation_config (GenerationConfig): generation config. Defaults to None. If None, default config is used.\n",
    "        prefix (str): prefix that separates input from output. Defaults to \"[/INST]\".\n",
    "        device (torch.device): device. Defaults to torch.device(\"cpu\").\n",
    "        batch_size (int): batch size. Defaults to 1.\n",
    "        output_hidden_states (bool); whether hidden states should be calculated for model answers. Defaults to True\n",
    "        \n",
    "    Returns:\n",
    "        dict: dictionary with keys report, prediction, last_hidden_states, input_lengths, whole_prompt\n",
    "            \n",
    "    \"\"\"\n",
    "    print(\"Starting Inference\")\n",
    "    tokens = [tokenizer(format_fun(t), add_special_tokens = False, truncation = True) for t in reports]\n",
    "    \n",
    "    collate_fn = DataCollatorWithPadding(tokenizer, padding=True)\n",
    "\n",
    "    dataloader = DataLoader(dataset=tokens, collate_fn=collate_fn, batch_size=batch_size, shuffle = False) \n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "    whole_prompt = []\n",
    "    last_hidden_states = []\n",
    "    input_lengths = [len(t[\"input_ids\"]) for t in tokens]\n",
    "\n",
    "    for idx, batch in enumerate(tqdm(dataloader)):\n",
    "        batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **batch,\n",
    "                generation_config=generation_config,\n",
    "            )\n",
    "\n",
    "        # Check GPU memory every 5 batches\n",
    "        if idx % 5 == 0:\n",
    "            check_gpu_memory()\n",
    "\n",
    "\n",
    "        return_tokens = outputs[\"sequences\"].to(\"cpu\")\n",
    "        batch_result = tokenizer.batch_decode(return_tokens, skip_special_tokens=True)\n",
    "        whole_prompt.extend(batch_result)\n",
    "        batch_result = [result.split(prefix)[-1].lower().strip() for result in batch_result]\n",
    "\n",
    "        results.extend(batch_result)\n",
    "        del outputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Finished Inference\")\n",
    "\n",
    "    if output_hidden_states:\n",
    "        # This separate pass is not time but memory efficient as if the hidden states are calculated during inference,\n",
    "        # hidden states are calculated for every token in the batch and the generated sequence.\n",
    "        print(results)\n",
    "        # If there are \"\" results need to pass \" \"\n",
    "        results_hs = [result if result != \"\" else \" \" for result in results]\n",
    "        print(results_hs)\n",
    "        print(\"Starting Hidden State Calculation\")\n",
    "        last_hidden_states = get_hidden_state(results_hs, model, tokenizer, device, batch_size)\n",
    "        print(\"Finished Hidden State Calculation\")\n",
    "\n",
    "    else:\n",
    "        last_hidden_states = None\n",
    "\n",
    "        \n",
    "    return {\"report\": reports, \n",
    "            \"prediction\": results, \n",
    "            \"last_hidden_states\": last_hidden_states, \n",
    "            \"input_lengths\":input_lengths,\n",
    "            \"whole_prompt\": whole_prompt}\n",
    "\n",
    "def multi_round_inference(reports:list[str], \n",
    "                          model:AutoModelForCausalLM, \n",
    "                          tokenizer:AutoTokenizer, \n",
    "                          format_fun1:Callable[[str],str],\n",
    "                          format_fun2:Callable[[str],str],\n",
    "                          generation_config:GenerationConfig = None,\n",
    "                          prefix:str = \"[/INST]\",\n",
    "                          device:torch.device = torch.device(\"cpu\"),\n",
    "                          batch_size:int = 1)->dict:\n",
    "    \n",
    "    \"\"\"Multi Round inference for the MS extraction task\n",
    "    \n",
    "    Args:\n",
    "        reports (list[str]): list of medical reports\n",
    "        model (AutoModelForCausalLM): model\n",
    "        tokenizer (AutoTokenizer): tokenizer\n",
    "        format_fun1 (Callable[str,str]): function to convert input text to desired prompt format\n",
    "        format_fun2 (Callable[str,str]): function to convert chat history to desired prompt format\n",
    "        generation_config (GenerationConfig): generation config. Defaults to None. If None, default config is used.\n",
    "        prefix (str): prefix that separates input from output. Defaults to \"[/INST]\".\n",
    "        device (torch.device): device. Defaults to torch.device(\"cpu\").\n",
    "        batch_size (int): batch size. Defaults to 1.\n",
    "        \n",
    "    Returns:\n",
    "        dict: dictionary with keys report, prediction, last_hidden_states, input_lengths, whole_prompt\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    max_new_tokens = generation_config.max_new_tokens\n",
    "\n",
    "    # For first round don't calculate hidden states and use long max_new_tokens, and low batch size\n",
    "    generation_config.max_new_tokens = 100\n",
    "    output_round1 = single_round_inference(reports=reports,\n",
    "                                           model=model, \n",
    "                                           tokenizer=tokenizer, \n",
    "                                           format_fun=format_fun1,\n",
    "                                           generation_config=generation_config,\n",
    "                                           device=device,\n",
    "                                           batch_size=1,\n",
    "                                           output_hidden_states=False)\n",
    "\n",
    "    # For second round calculate hidden states if desired\n",
    "    generation_config.max_new_tokens = max_new_tokens\n",
    "    chat_history = output_round1[\"whole_prompt\"]\n",
    "    chat_history = [text.split(\"[/INST]\")[-1] for text in chat_history]\n",
    "\n",
    "    return single_round_inference(reports=chat_history,\n",
    "                                  model=model, \n",
    "                                  tokenizer=tokenizer, \n",
    "                                  format_fun=format_fun2,\n",
    "                                  generation_config=generation_config,\n",
    "                                  prefix=prefix,\n",
    "                                  device=device,\n",
    "                                  batch_size=batch_size,\n",
    "                                  output_hidden_states=True)\n",
    "\n",
    "\n",
    "\n",
    "def get_hidden_state(text: list[str], model:AutoModel, tokenizer:AutoTokenizer, device:torch.device=torch.device(\"cpu\"), batch_size:int = 16)->list[torch.Tensor]:\n",
    "    \"\"\"Get hidden state of last layer of model for each prediction in results. Per default the last \n",
    "    \n",
    "    Args:\n",
    "        text (list[str]): a list of input strings to be encoded.\n",
    "        model (AutoModel): model\n",
    "        tokenizer (AutoTokenizer): tokenizer\n",
    "        device (torch.device): device. Defaults to torch.device(\"cpu\").\n",
    "        batch_size (int): batch size. Defaults to 16.\n",
    "        \n",
    "    Returns:\n",
    "        results (dict): results of prompting return with keys report, prediction, last_hidden_states, input_lengths, whole_prompt, encodings\n",
    "            \n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    dataset = Dataset.from_dict(tokenizer(text, add_special_tokens = False))\n",
    "    collate_fn = DataCollatorWithPadding(tokenizer = tokenizer, padding = \"longest\")\n",
    "\n",
    "    dataloader = DataLoader(dataset = dataset, batch_size = batch_size, collate_fn = collate_fn)\n",
    "    \n",
    "    encodings = []\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch, output_hidden_states = True)\n",
    "\n",
    "        last_hidden_state = outputs[\"hidden_states\"][-1]\n",
    "\n",
    "        # For decoder architectures the last token of the sequence contains information about the whole sequence\n",
    "        last_hidden_state = last_hidden_state[:, -1, :]\n",
    "        encodings.append(last_hidden_state)\n",
    "\n",
    "        del last_hidden_state\n",
    "        del outputs\n",
    "        del batch\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return torch.cat(encodings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"primary progressive multiple sclerosis\", \"secondary progressive multiple sclerosis\",\n",
    "          \"relapsing remitting multiple sclerosis\",\"not enough info\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = get_hidden_state([\" \"], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_line = load_ms_data(\"line\")\n",
    "df_all = load_ms_data(\"all\")\n",
    "df_first_last = load_ms_data(\"all_first_line_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to make output more consistent by stopping on MS, https://github.com/huggingface/transformers/issues/26959\n",
    "class EosListStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, eos_sequence = [835, 2799, 4080, 29901]):\n",
    "        self.eos_sequence = eos_sequence\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        last_ids = input_ids[:,-len(self.eos_sequence):].tolist()\n",
    "        return self.eos_sequence in last_ids\n",
    "ms_stop = EosListStoppingCriteria(tokenizer(\"multiple sclerosis\", add_special_tokens = False)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla\n",
    "\n",
    "Using the original prompt template of meta Llama2 creators. \\<s>[INST]<\\<SYS>>{system_prompt}<\\</SYS>>{instruction}{input}[/INST]\n",
    "You should set add special tokens to false for the tokenizer otherwise you will have double bos in the beginning of the prompt, if you state it. Gives more control.\n",
    "\n",
    "@misc{touvron2023llama,\r\n",
    "      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, \r\n",
    "      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},\r\n",
    "      year={2023},\r\n",
    "      eprint={2307.09288},\r\n",
    "      archivePrefix={arXiv},\r\n",
    "      primaryClarbage.\n",
    "\n",
    "This prompt template builds the foundation to all further strategies, otherwise the model's answers are kinda garbage.\n",
    "\n",
    "Hidden states is of format hidden_states (tuple(tuple(torch.FloatTensor)), optional, returned when output_hidden_states=True is passed or when config.output_hidden_states=True) — Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of torch.FloatTensor of shape (batch_size, generated_length, hidden_size). I will try working with the last hidden state of the first generated token as this is where the model will start it's generation/prediction from..CL}\r\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def zero_shot_base(report:str)->str:\n",
    "    \"\"\"Zero-shot base for the MS extraction task\n",
    "\n",
    "    Args:\n",
    "        report (str): medical report\n",
    "\n",
    "    Returns:\n",
    "        str: reformatted medical report with base\n",
    "\n",
    "    \"\"\"\n",
    "    base_prompt = \"<s>[INST]<<SYS>>{system_prompt}<</SYS>>\\n\\n{instruction}{input}[/INST]\\nThe type of multiple sclerosis stated in the german medical report is: \"\n",
    "    system_prompt =  (\"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \"\n",
    "                      \"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \"\n",
    "                       \"Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make \"\n",
    "                        \"any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t \"\n",
    "                        \"know the answer to a question, please don’t share false information.\\n\"\n",
    "                        )\n",
    "    instruction = (\"Your task is to extract the type of multiple Sclerosis (MS) stated in a German medical report. There are 3 types: \"\n",
    "                        \"\\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" and \\\"schubförmig remittierende Multiple Sklerose (RRMS)\\\".\"\n",
    "                        \"The type is provided in the text you just have to extract it. If you cannot match a type exactly answer with \\\"not enough info\\\".\"\n",
    "                        \"Your answer should solely consist of either \\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" \"\n",
    "                        \"\\schubförmige remittierende Multiple Sklerose (RRMS)\\\", or \\\"not enough info\\\".\"\n",
    "                        \"\\nHere is the medical report:\\n\"\n",
    "                    )\n",
    "    input = base_prompt.format(system_prompt = system_prompt, instruction = instruction, input =  report)\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = single_round_inference(reports = df_line[\"train\"].select(range(2))[\"text\"],\n",
    "                                 model=model, \n",
    "                                 tokenizer= tokenizer,\n",
    "                                 format_fun = zero_shot_base,\n",
    "                                generation_config = generation_config,\n",
    "                                 prefix = \"[/INST]\\nThe type of multiple sclerosis stated in the german medical report is: \",\n",
    "                                device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction\n",
    "\n",
    "Based on the paper of the creators of Llama2-MedTuned\n",
    "\n",
    "@misc{rohanian2023exploring,\r\n",
    "      title={Exploring the Effectiveness of Instruction Tuning in Biomedical Language Processing}, \r\n",
    "      author={Omid Rohanian and Mohammadmahdi Nouriborji and David A. Clifton},\r\n",
    "      year={2023},\r\n",
    "      eprint={2401.00579},\r\n",
    "      archivePrefix={arXiv},\r\n",
    "      primaryClass={cs\n",
    "\n",
    "Formulating the task as an instruction is closer to the fine-tuning of the model..CL}\r\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_instruction(report:str)->str:\n",
    "    \"\"\"Zero-shot instruction for the MS extraction task\n",
    "    \n",
    "    Args:\n",
    "        report (str): medical report\n",
    "        \n",
    "        Returns:\n",
    "            str: reformatted medical report with instruction\n",
    "            \n",
    "            \"\"\"\n",
    "    instruction_base_prompt = \"<s>[INST]\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}[/INST]\\n\\n### Output:\\n\"\n",
    "    task_instruction = (\"Your task is to extract the type of multiple Sclerosis (MS) stated in a German medical report. There are 3 types: \"\n",
    "                        \"\\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" and \\\"schubförmig remittierende Multiple Sklerose (RRMS)\\\".\"\n",
    "                        \"The type is provided in the text you just have to extract it. If you cannot match a type exactly answer with \\\"not enough info\\\".\"\n",
    "                        \"Your answer should solely consist of either \\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" \"\n",
    "                        \"\\schubförmige remittierende Multiple Sklerose (RRMS)\\\", or \\\"not enough info\\\".\"\n",
    "                        \"\\nHere is the medical report:\\n\"\n",
    "                    )\n",
    "    input = instruction_base_prompt.format(instruction = task_instruction, input =  report)\n",
    "\n",
    "    return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = single_round_inference(reports = df_line[\"train\"].select(range(2))[\"text\"],\n",
    "                                 model=model, \n",
    "                                 tokenizer= tokenizer,\n",
    "                                 format_fun = zero_shot_instruction,\n",
    "                                generation_config = generation_config,\n",
    "                                 prefix = \"[/INST]\\n\\n### Output:\\n\",\n",
    "                                device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot\n",
    "\n",
    "Original Paper suggesting this:\n",
    "\n",
    "@misc{brown2020language,\r\n",
    "      title={Language Models are Few-Shot Learners}, \r\n",
    "      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},\r\n",
    "      year={2020},\r\n",
    "      eprint={2005.14165},\r\n",
    "      archivePrefix={arXiv},\r\n",
    "      primaryClass={cs.CL}\r\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_base(report:str)->str:\n",
    "    \"\"\"Few Shot base for the MS extraction task\n",
    "\n",
    "    Args:\n",
    "        report (str): medical report\n",
    "\n",
    "    Returns:\n",
    "        str: reformatted medical report with base\n",
    "\n",
    "    \"\"\"\n",
    "    base_prompt = \"<s>[INST]<<SYS>>{system_prompt}<</SYS>>\\n\\n{instruction}Report:\\n{input}[/INST]\\nDiagnosis:\\n\"\n",
    "\n",
    "    rrms = 'Schubförmig-remittierende Multiple Sklerose, EM 01/2013, ED 10/2015\\nINDENT EDSS 05/2020: 2.0 [...]'\n",
    "    spms = '1. Sekundär progrediente schubförmige Multiple Sklerose [...]'\n",
    "    ppms = '1. Primär progrediente Multiple Sklerose, EM 1992, ED 1996, aktuell EDSS 7.0 [...]'\n",
    "    no_ms = '[...] INDENT MRI 07/2014: Progrediente supratentorielle MS-Plaques mit Befund-Progredienz im Bereich der Radiatio optica beidseits. [...]'\n",
    "\n",
    "    examples = [ppms, spms, rrms, no_ms]\n",
    "\n",
    "    labels = [\"Schubförmig remittierende Multiple Sklerose (RRMS)\",\n",
    "              \"Sekundär progrediente Multiple Sklerose (SPMS)\",\n",
    "              \"Primär progrediente Multiple Sklerosis (PPMS)\", \n",
    "              \"not enough info\"]\n",
    "    \n",
    "    system_prompt = (\n",
    "    \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \"\n",
    "    \"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \"\n",
    "    \"Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make \"\n",
    "    \"any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t \"\n",
    "    \"know the answer to a question, please don’t share false information.\\n\"\n",
    "    )\n",
    "\n",
    "    instruction = (\n",
    "       \"Your task is to extract the type of multiple Sclerosis (MS) stated in a German medical report. There are 3 types: \"\n",
    "        \"\\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" and \\\"schubförmig remittierende Multiple Sklerose (RRMS)\\\".\"\n",
    "        \"The type is provided in the text you just have to extract it. If you cannot match a type exactly answer with \\\"not enough info\\\".\"\n",
    "        \"Your answer should solely consist of either \\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" \"\n",
    "        \"\\\"schubförmige remittierende Multiple Sklerose (RRMS)\\\", or \\\"not enough info\\\".\"\n",
    "        \"To help you with your task, here are a few excerpts from reports that indiciate what output you should produce:\\n\\n\"\n",
    "        )\n",
    "    \n",
    "    for example, label in zip(examples, labels):\n",
    "        instruction += f\"Report:\\n{example}\\nDiagnosis:\\n{label}\\n\\n\"\n",
    "    \n",
    "    input = base_prompt.format(system_prompt = system_prompt, instruction = instruction, input =  report)\n",
    "    input + \"Diagnosis:\\n\"\n",
    "\n",
    "    return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = single_round_inference(df_line[\"train\"].filter(lambda e: e[\"labels\"] != 3).select(range(2))[\"text\"], model, tokenizer, few_shot_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_instruct(report:str)->str:\n",
    "    \"\"\"Few Shot base for the MS extraction task\n",
    "\n",
    "    Args:\n",
    "        report (str): medical report\n",
    "\n",
    "    Returns:\n",
    "        str: reformatted medical report with base\n",
    "\n",
    "    \"\"\"\n",
    "    base_prompt = \"<s>[INST]### Instruction:\\n{instruction}### Input:\\n{input}[/INST]\\n### Output:\\n\"\n",
    "\n",
    "    rrms = 'Schubförmig-remittierende Multiple Sklerose, EM 01/2013, ED 10/2015\\nINDENT EDSS 05/2020: 2.0 [...]'\n",
    "    spms = '1. Sekundär progrediente schubförmige Multiple Sklerose [...]'\n",
    "    ppms = '1. Primär progrediente Multiple Sklerose, EM 1992, ED 1996, aktuell EDSS 7.0 [...]'\n",
    "    no_ms = '[...] INDENT MRI 07/2014: Progrediente supratentorielle MS-Plaques mit Befund-Progredienz im Bereich der Radiatio optica beidseits. [...]'\n",
    "\n",
    "    examples = [ppms, spms, rrms, no_ms]\n",
    "\n",
    "    labels = [\"Schubförmig remittierende Multiple Sklerose (RRMS)\",\n",
    "              \"Sekundär progrediente Multiple Sklerose (SPMS)\",\n",
    "              \"Primär progrediente Multiple Sklerosis (PPMS)\", \n",
    "              \"not enough info\"]\n",
    "\n",
    "    instruction = (\n",
    "        \"Your task is to extract the type of multiple Sclerosis (MS) stated in a German medical report. There are 3 types: \"\n",
    "        \"\\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" and \\\"schubförmig remittierende Multiple Sklerose (RRMS)\\\".\"\n",
    "        \"The type is provided in the text you just have to extract it. If you cannot match a type exactly answer with \\\"not enough info\\\".\"\n",
    "        \"Your answer should solely consist of either \\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" \"\n",
    "        \"\\\"schubförmige remittierende Multiple Sklerose (RRMS)\\\", or \\\"not enough info\\\".\"\n",
    "        \"To help you with your task, here are a few excerpts from reports that indiciate what output you should produce:\\n\\n\"\n",
    "        )\n",
    "    \n",
    "    for example, label in zip(examples, labels):\n",
    "        instruction += f\"### Input:\\n{example}\\n### Output:\\n{label}\\n\\n\"\n",
    "    \n",
    "    input = base_prompt.format(instruction = instruction, input =  report)\n",
    "\n",
    "    return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = single_round_inference(df_line[\"train\"].filter(lambda e: e[\"labels\"] != 3).select(range(2))[\"text\"], model, tokenizer, few_shot_instruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_steps_one(report: str)->str:\n",
    "    \"\"\"Two Steps One for the MS extraction task. Encodes the report for first turn of the dialogue.\n",
    "\n",
    "    Args:\n",
    "        report (str): medical report\n",
    "\n",
    "    Returns:\n",
    "        str: reformatted medical report with base\n",
    "\n",
    "    \"\"\"\n",
    "    base_prompt = \"<s>[INST]<<SYS>>{system_prompt}<</SYS>>\\n\\n{instruction}{input}[/INST]\"\n",
    "    system_prompt =  (\"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \"\n",
    "                      \"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \"\n",
    "                       \"Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make \"\n",
    "                        \"any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t \"\n",
    "                        \"know the answer to a question, please don’t share false information.\\n\"\n",
    "                        )\n",
    "    instruction = (\"Your task is to extract relevant information about the multiple sclerosis diagnosis from the provided German medical report. \"\n",
    "                   \"Identify and summarize all sections discussing \\\"Multiple Sklerose\\\" paying attention to the exact type of multiple sclerosis. \"\n",
    "                   \"There are three types:\\n\"\n",
    "                   \"primär progrediente Multiple Sklerose (PPMS)\\n\"\n",
    "                   \"sekundär progrediente Multiple Sklerose (SPMS)\\n\"\n",
    "                   \"schubförmige Multiple Sklerose (RRMS)\\n\"\n",
    "                   \"If the report lacks information about multiple sclerosis, respond with \\\"not enough info\\\". \"\n",
    "                   \"\\nHere is the Medical Report:\\n \"\n",
    "                   )\n",
    "                   \n",
    "    input = base_prompt.format(system_prompt = system_prompt, instruction = instruction, input =  report)\n",
    "    return input\n",
    "\n",
    "def two_steps_two(chat_history: str)->str:\n",
    "    \"\"\"Two Steps Two for the MS extraction task. Encodes the chat history for second turn of the dialogue.\n",
    "\n",
    "    Args:\n",
    "        chat_history (str): chat history\n",
    "\n",
    "    Returns:\n",
    "        str: reformatted medical report with base\n",
    "\n",
    "    \"\"\"\n",
    "    base_prompt = \"<s>[INST]\\n\\n{instruction}\\n{summary}[/INST]\\nGiven the summary the most likely diagnosis is: \"\n",
    "    instruction = (\n",
    "                   \"Given a summary of a medical report describing a patient's condition related to multiple sclerosis, provide the most likely diagnosis. The possible diagnoses are:\\n\"\n",
    "                   \"- primär progrediente Multiple Sklerose (PPMS)\\n\"\n",
    "                   \"- sekundär progrediente Multiple Sklerose (SPMS)\\n\"\n",
    "                   \"- schubförmige Multiple Sklerose (RRMS)\\n\"\n",
    "                   \"- not enough info\\n\"\n",
    "                   \"Consider the information provided in the summary and select the diagnosis that best fits the patient's condition. If the summary does not contain sufficient information to make a diagnosis, choose \\\"not enough info.\\\" \"\n",
    "                   \"Here is the summary:\\n\"\n",
    "                   )\n",
    "    input = base_prompt.format(instruction = instruction, summary = chat_history)\n",
    "\n",
    "    return input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_round_inference(reports:list[str], \n",
    "                           model:AutoModelForCausalLM, \n",
    "                           tokenizer:AutoTokenizer, \n",
    "                           format_fun1:Callable[str,str],\n",
    "                          format_fun2:Callable[str,str],\n",
    "                           output_hidden_states:bool = True,\n",
    "                          max_new_tokens:int = 20)->pd.DataFrame:\n",
    "    \n",
    "    \"\"\"Multi Round inference for the MS extraction task\n",
    "    \n",
    "    Args:\n",
    "        reports (list[str]): list of medical reports\n",
    "        model (AutoModelForCausalLM): model\n",
    "        tokenizer (AutoTokenizer): tokenizer\n",
    "        format_fun1 (Callable[str,str]): function to convert input text to desired prompt format\n",
    "        format_fun2 (Callable[str,str]): function to convert chat history to desired prompt format\n",
    "        output_hidden_states (bool); whether hidden states should be calculated. Defaults to True\n",
    "        max_new_tokens (int): The number of tokens to be generated.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: results of inference\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    output_round1 = single_round_inference(reports, model, tokenizer, format_fun1, output_hidden_states = False, max_new_tokens = 2)\n",
    "    chat_history = output_round1[\"whole_prompt\"]\n",
    "\n",
    "    return single_round_inference(chat_history, model, tokenizer, format_fun2, output_hidden_states = output_hidden_states, max_new_tokens = max_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = multi_round_inference(df_line[\"train\"].filter(lambda e: e[\"labels\"] != 3).select(range(2))[\"text\"], model, tokenizer, two_steps_one, two_steps_two, max_new_tokens = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from outlines import samplers\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                            bnb_4bit_use_double_quant=True,\n",
    "                                            bnb_4bit_quant_type=\"nf4\",\n",
    "                                            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                            )\n",
    "model = models.transformers(\n",
    "    model_name=paths.MODEL_PATH/\"Llama2-MedTuned-13b\",\n",
    "    device=\"cuda\",\n",
    "    model_kwargs={\n",
    "        \"device_map\": \"auto\",\n",
    "        \"quantization_config\": quantization_config,\n",
    "        \"attn_implementation\": \"flash_attention_2\",\n",
    "    },\n",
    ")\n",
    "sampler = samplers.greedy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_input = torch.load(paths.RESULTS_PATH/\"ms-diag/ms-diag_outlines_Llama2-MedTuned-7b_4bit_all_test_few_shot_vanilla_rag.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Task instructions, system prompt and examples\n",
    "with open(paths.DATA_PATH_PREPROCESSED/\"ms-diag/task_instruction.txt\", \"r\") as f:\n",
    "    task_instruction = f.read()\n",
    "\n",
    "with open(paths.DATA_PATH_PREPROCESSED/\"ms-diag/system_prompt.txt\", \"r\") as f:\n",
    "    system_prompt = f.read()\n",
    "\n",
    "with open(paths.DATA_PATH_PREPROCESSED/\"ms-diag/examples.json\", \"r\") as f:\n",
    "    examples = json.load(f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_input[\"text\"][:-12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_instruction(input:str)->str:\n",
    "    \"\"\"Zero-shot instruction for the MS extraction task\n",
    "    \n",
    "    Args:\n",
    "        input (str): user input (medical report)\n",
    "        system_prompt (str): system prompt\n",
    "        task_instruction (str): instruction for the task\n",
    "        \n",
    "        Returns:\n",
    "            str: reformatted medical report with instruction\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    instruction_base_prompt = \"[INST]<<SYS>>{system_prompt}<</SYS>>\\n### Instruction:\\n{task_instruction}\\n\\n### Input:\\n{input}[/INST]\\n\\n### Output:\\n\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.\n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not makeany sense, or is not factually coherent, explain why instead of answering something not correct. \n",
    "If you don’t know the answer to a question, please don’t share false information.\n",
    "\"\"\"\n",
    "    task_instruction = \"\"\"Your task is to extract the diagnosis corresponding to a type of multiple sclerosis (MS) stated in a German medical report. The input for this task is a German medical report, and the output should be the type of MS.\n",
    "There are three types of multiple sclerosis in German:\n",
    "- primär progrediente Multiple Sklerose (PPMS)\n",
    "- sekundär progrediente Multiple Sklerose (SPMS)\n",
    "- schubförmig remittierende Multiple Sklerose (RRMS)\n",
    "\n",
    "The type is provided in the text, and your task is to extract it. If you cannot match a type exactly, please answer with 'not enough info'.\n",
    "Your answer should solely consist of one of the following:\n",
    "- primär progrediente Multiple Sklerose\n",
    "- sekundär progrediente Multiple Sklerose\n",
    "- schubförmige remittierende Multiple Sklerose\n",
    "- not enough info\n",
    "\"\"\"\n",
    "    \n",
    "    input = instruction_base_prompt.format(system_prompt = system_prompt, task_instruction = task_instruction, input = input)\n",
    "\n",
    "    return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_instruction(input:str, task_instruction:str, examples:list[dict], *args, **kwargs)->str:\n",
    "    \"\"\"Few Shot Instruction for the MS extraction task\n",
    "\n",
    "    Args:\n",
    "        input (str): user input (medical report)\n",
    "        system_prompt (str): system prompt\n",
    "        task_instruction (str): instruction for the task\n",
    "        examples (list[dict]): examples for the task. Each dict contains a text and a label.\n",
    "\n",
    "    Returns:\n",
    "        str: reformatted medical report with examples and instruction\n",
    "\n",
    "    \"\"\"\n",
    "    base_prompt = (\"[INST]n### Instruction:\\n{task_instruction}\\n\"\n",
    "                   \"Here are some examples to help you understand the task:\\n{examples}\\n\"\n",
    "                   \"Please provide your answer for the following Input.\\n\\n\"\n",
    "                   \"### Input:\\n{input}\\n\\n### Output:\\n[/INST]\")\n",
    "\n",
    "    insert_examples = \"\"\n",
    "\n",
    "    for example in examples:\n",
    "        text = example[\"text\"]\n",
    "        label = example[\"labels\"]\n",
    "        insert_examples += f\"Input:\\n{text}\\nOutput:\\n{label}\\n\\n\"\n",
    "    \n",
    "    input = base_prompt.format(task_instruction = task_instruction, examples = insert_examples, input = input)\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_base(input:str)->str:\n",
    "    \"\"\"Zero-shot base for Llama prompting\n",
    "\n",
    "    Args:\n",
    "        input (str): user input (medical report)\n",
    "        system_prompt (str): system prompt\n",
    "        task_instruction (str): instruction for the task\n",
    "\n",
    "    Returns:\n",
    "        str: reformatted medical report\n",
    "\n",
    "    \"\"\"\n",
    "    base_prompt = \"[INST]{task_instruction}\\nHere is the Input:\\n{input}[/INST] Output:\\n\"\n",
    "\n",
    "    task_instruction = \"\"\"Your task is to extract the diagnosis corresponding to a type of multiple sclerosis (MS) stated in a German medical report. The input for this task is a German medical report, and the output should be the type of MS.\n",
    "There are three types of multiple sclerosis in German:\n",
    "- primär progrediente Multiple Sklerose (PPMS)\n",
    "- sekundär progrediente Multiple Sklerose (SPMS)\n",
    "- schubförmig remittierende Multiple Sklerose (RRMS)\n",
    "\n",
    "The type is provided in the text, and your task is to extract it. If you cannot match a type exactly, please answer with 'not enough info'.\n",
    "Your answer should solely consist of one of the following:\n",
    "- primär progrediente Multiple Sklerose\n",
    "- sekundär progrediente Multiple Sklerose\n",
    "- schubförmige remittierende Multiple Sklerose\n",
    "- not enough info\n",
    "\"\"\"\n",
    "    \n",
    "    input = base_prompt.format(task_instruction = task_instruction, input =  input)\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_instruction(report:str, task_instruction)->str:\n",
    "    \"\"\"Zero-shot instruction for the MS extraction task\n",
    "    \n",
    "    Args:\n",
    "        report (str): medical report\n",
    "        \n",
    "        Returns:\n",
    "            str: reformatted medical report with instruction\n",
    "            \n",
    "            \"\"\"\n",
    "    instruction_base_prompt = \"[INST]\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}[/INST]\\n\\n### Output:\\n\"\n",
    "\n",
    "    input = instruction_base_prompt.format(instruction = task_instruction, input =  report)\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_instruction(report:str)->str:\n",
    "    \"\"\"Zero-shot instruction for the MS extraction task\n",
    "    \n",
    "    Args:\n",
    "        report (str): medical report\n",
    "        \n",
    "        Returns:\n",
    "            str: reformatted medical report with instruction\n",
    "            \n",
    "            \"\"\"\n",
    "    instruction_base_prompt = \"<s>[INST]\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}[/INST]\\n\\n### Output:\\n\"\n",
    "    task_instruction = (\"Your task is to extract the type of multiple Sclerosis (MS) stated in a German medical report. There are 3 types: \"\n",
    "                        \"\\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" and \\\"schubförmig remittierende Multiple Sklerose (RRMS)\\\".\"\n",
    "                        \"The type is provided in the text you just have to extract it. If you cannot match a type exactly answer with \\\"not enough info\\\".\"\n",
    "                        \"Your answer should solely consist of either \\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" \"\n",
    "                        \"\\schubförmige remittierende Multiple Sklerose (RRMS)\\\", or \\\"not enough info\\\".\"\n",
    "                        \"\\nHere is the medical report:\\n\"\n",
    "                    )\n",
    "    input = instruction_base_prompt.format(instruction = task_instruction, input =  report)\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ms_data_diag = load_ms_data(\"line\")[\"test\"].filter(lambda e: e[\"labels\"] != 3)\n",
    "# ms_data_no_diag = load_ms_data(\"line\")[\"test\"].filter(lambda e: e[\"labels\"] == 3).select(range(15))\n",
    "# ms_data = concatenate_datasets([ms_data_diag, ms_data_no_diag])\n",
    "ms_data = load_ms_data(\"all\")[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.utils\n",
    "importlib.reload(src.utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_format_fun, format_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_fun = get_format_fun(\"few_shot_instruction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = format_prompt(results_input[\"text\"][:-12], format_fun, task_instruction = task_instruction, system_prompt = system_prompt, examples = examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = [zero_shot_instruction(text) for text in results_input[\"text\"][:-12]]\n",
    "test_loader = DataLoader(inputs, batch_size = 8, shuffle = False)\n",
    "\n",
    "sampler = samplers.greedy()\n",
    "\n",
    "generator = outlines.generate.choice(model, [\"primär progrediente Multiple Sklerose (PPMS)\",\n",
    "                                             \"sekundär progrediente Multiple Sklerose (SPMS)\",\n",
    "                                            \"schubförmig remittierende Multiple Sklerose (RRMS)\",\n",
    "                                            \"other\"], sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for batch in tqdm(test_loader):\n",
    "    answer = generator(batch)\n",
    "    results.extend(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mappings = dict(zip([\"primär progrediente Multiple Sklerose PPMS\",\n",
    "                                             \"sekundär progrediente Multiple Sklerose SPMS\",\n",
    "                                            \"schubförmig remittierende Multiple Sklerose RRMS\",\n",
    "                                            \"other\"], [0,2,1,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mapped = [label_mappings[pred] for pred in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {\"labels\": results_input[\"labels\"][:-12], \"preds\":results_mapped, \"text\": results_input[\"text\"][:-12]}\n",
    "# torch.save(outputs, paths.RESULTS_PATH/\"ms-diag\"/\"ms-diag_Llama2-MedTuned-13b_4bit_all_instruction_outlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = torch.load(paths.RESULTS_PATH/\"ms-diag\"/\"ms-diag_Llama2-MedTuned-13b_4bit_all_instruction_outlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true = outputs[\"labels\"], y_pred = outputs[\"preds\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "viridis_cmap = plt.get_cmap('viridis_r')\n",
    "cmap = sns.color_palette(\"ch:s=.25,rot=-.25\", as_cmap=True)\n",
    "\n",
    "# Extract a subset of colors from the \"viridis\" colormap\n",
    "start_index = 120  # Start index of colors to include\n",
    "end_index = 200 # End index of colors to include\n",
    "subset_colors = viridis_cmap(np.linspace(start_index / 255, end_index / 255, end_index - start_index + 1))\n",
    "\n",
    "# Create a custom colormap using the subset of colors\n",
    "custom_cmap = mcolors.ListedColormap(subset_colors)\n",
    "custom_cmap = sns.color_palette(\"light:#5A9\", as_cmap=True)\n",
    "\n",
    "def pretty_confusion_matrix(y_true, y_pred, labels):\n",
    "    # Create the confusion matrix\n",
    "    cm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "    # Plotting the confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.set_theme(font_scale=1.2)  # Adjust font size for labels\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=custom_cmap, cbar=False,\n",
    "                yticklabels=labels, xticklabels=labels, alpha=0.9, linewidths=0.5, linecolor='lightgrey')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_confusion_matrix(y_true = outputs[\"labels\"], y_pred = outputs[\"preds\"], labels = [\"PPMS\", \"RRMS\", \"SPMS\", \"other\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, pred, text in zip(*outputs.values()):\n",
    "    if label != pred:\n",
    "        print(\"Label :\", label)\n",
    "        print(\"Pred :\", pred)\n",
    "        print(\"Text: \\n\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ms_data_diag = load_ms_data(\"line\")[\"test\"].filter(lambda e: e[\"labels\"] != 3)\n",
    "# ms_data_no_diag = load_ms_data(\"line\")[\"test\"].filter(lambda e: e[\"labels\"] == 3).select(range(15))\n",
    "# ms_data = concatenate_datasets([ms_data_diag, ms_data_no_diag])\n",
    "ms_data = load_ms_data(\"line\")[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [zero_shot_instruction(text) for text in ms_data[\"text\"]]\n",
    "test_loader = DataLoader(inputs, batch_size = 4, shuffle = False)\n",
    "\n",
    "sampler = samplers.greedy()\n",
    "\n",
    "generator = outlines.generate.choice(model, [\"primär progrediente Multiple Sklerose (PPMS)\",\n",
    "                                             \"sekundär progrediente Multiple Sklerose (SPMS)\",\n",
    "                                            \"schubförmig remittierende Multiple Sklerose (RRMS)\",\n",
    "                                            \"not enough info\"], sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for batch in tqdm(test_loader):\n",
    "    answer = generator(batch)\n",
    "    results.extend(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mappings = dict(zip([\"primär progrediente Multiple Sklerose PPMS\",\n",
    "                                             \"sekundär progrediente Multiple Sklerose SPMS\",\n",
    "                                            \"schubförmig remittierende Multiple Sklerose RRMS\",\n",
    "                                            \"not enough info\"], [0,2,1,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mapped = [label_mappings[pred] for pred in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {\"labels\": ms_data[\"labels\"], \"preds\":results_mapped, \"text\": ms_data[\"text\"]}\n",
    "torch.save(outputs, paths.RESULTS_PATH/\"ms-diag\"/\"ms-diag_Llama2-MedTuned-13b_4bit_line_instruction_outlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true = outputs[\"labels\"], y_pred = outputs[\"preds\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_confusion_matrix(y_true = outputs[\"labels\"], y_pred = outputs[\"preds\"], labels = [\"PPMS\", \"RRMS\", \"SPMS\", \"other\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, pred, text in zip(*outputs.values()):\n",
    "    if label != pred:\n",
    "        print(\"Label :\", label)\n",
    "        print(\"Pred :\", pred)\n",
    "        print(\"Text: \\n\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
