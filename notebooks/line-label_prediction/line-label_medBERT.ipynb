{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "from src import paths\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import numpy as np\n",
    "from huggingface_hub import notebook_login\n",
    "import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Login to Hugging Face Hub as model is gated\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load Data\n",
    "df = pl.read_csv(os.path.join(paths.DATA_PATH_PREPROCESSED, \"line_labelling_clean.csv\"))\n",
    "num_labels = len(df['class_agg'].unique())\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = \"GerMedBERT/medbert-512\"\n",
    "\n",
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Load model for embedding\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**2} MB\")\n",
    "print(f\"GPU Memory Cached: {torch.cuda.memory_reserved() / 1024**2} MB\")\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to rerun if embeddings saved\n",
    "# Embed data while logging progress\n",
    "embeddings = []\n",
    "embeddings_base = []\n",
    "batch_size = 16\n",
    "\n",
    "for i in tqdm.tqdm(range(0, len(df), batch_size)):\n",
    "    tokens = tokenizer(df['text'][i:i+batch_size].to_list(), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    attention_mask = tokens[\"attention_mask\"][i:i+batch_size]\n",
    "    with torch.no_grad():\n",
    "        embeddings.append(model(**tokens, output_hidden_states=True).hidden_states[-1].cpu())\n",
    "        embeddings_base.append(model.base_model(**tokens).pooler_output.cpu())\n",
    "\n",
    "# Save embeddings\n",
    "torch.save(embeddings, os.path.join(paths.DATA_PATH_PREPROCESSED, \"line-label_pred/embeddings.pt\"))\n",
    "torch.save(embeddings_base, os.path.join(paths.DATA_PATH_PREPROCESSED, \"line-label_pred/embeddings_base.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings are stored in shape batch_size X sequence_length x embedding_size. Multiple ways to construct embeddings now:\n",
    "- Mean over sequence\n",
    "- CLS [:,0,:] token\n",
    "- Pooling of CLS tokens (in embeddings_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "embeddings = torch.load(os.path.join(paths.DATA_PATH_PREPROCESSED, \"embeddings.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean over sequence\n",
    "embeddings_mean = [torch.mean(embedding, dim=1) for embedding in embeddings]\n",
    "embeddings_mean = torch.cat(embeddings_mean, dim=0)\n",
    "\n",
    "# Plot Mean Embeddings\n",
    "plot_embeddings(embeddings_mean, df[\"class_agg\"], title=\"Mean Embeddings\", method=\"pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CLS embeddings\n",
    "embeddings_cls = [embedding[:,0,:] for embedding in embeddings]\n",
    "embeddings_cls = torch.cat(embeddings_cls, dim=0)\n",
    "plot_embeddings(embeddings_cls, df[\"class_agg\"], title=\"CLS Embeddings\", method=\"pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pool Embeddings\n",
    "embeddings_base = torch.load(os.path.join(paths.DATA_PATH_PREPROCESSED, \"embeddings_base.pt\"))\n",
    "embeddings_base = torch.cat(embeddings_base, dim=0)\n",
    "plot_embeddings(embeddings_base, df[\"class_agg\"], title=\"Pooler Output\", method=\"pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hidden State [-1] model is equal Hidden State [-1] base_model:\", torch.equal(output.hidden_states[-1], output_base.hidden_states[-1]))\n",
    "print(\"Hidden State [-1] model is equal base_model last_hidden_state:\", torch.equal(output.hidden_states[-1], output_base.last_hidden_state))\n",
    "print(\"Hidden State [-1] model is equal base_model pooler_output:\", torch.equal(output.hidden_states[-1], output_base.pooler_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My understanding:\n",
    "- AutoModelForSequenceClassification will add a pooling layer (weights and bias with tanh) and a classification head (weights and bias). In the model config there is no pooling layer specified. \n",
    "- The Model for Masked Sequence did not have a pooling layer, but when calling AutoModel this pooling layer config is set to true automatically even though it is not listed in the config, it is the default I think.\n",
    "- I think I will fine-tune the model normally then use the base model to get the pooled embeddings and visualize that. I need to figure out if the model actually makes use of the pooled embeddings or if it directly uses the last hidden layer.\n",
    "\n",
    "Question:\n",
    "- Will the pooling layer be trained as well?\n",
    "- Is the prediction based on the pooling layer?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf-extr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
