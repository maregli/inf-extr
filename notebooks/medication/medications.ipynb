{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "from src import paths\n",
    "\n",
    "import torch\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum, auto\n",
    "from src.utils import (load_model_and_tokenizer, \n",
    "                        get_sampler, \n",
    "                        get_format_fun, \n",
    "                        format_prompt, \n",
    "                        get_outlines_generator, \n",
    "                        get_pydantic_schema, \n",
    "                        outlines_medication_prompting,\n",
    "                        get_default_pydantic_model)\n",
    "\n",
    "from outlines import samplers\n",
    "from outlines.generate import SequenceGenerator\n",
    "from outlines.samplers import Sampler\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from typing import Callable, Union\n",
    "\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.utils\n",
    "importlib.reload(src.utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(\"Llama2-MedTuned-7b\",\n",
    "                                            task_type = \"outlines\",\n",
    "                                            quantization = \"4bit\",\n",
    "                                            attn_implementation = \"flash_attention_2\",\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = get_sampler(\"greedy\")\n",
    "schema = get_pydantic_schema(\"medication\")\n",
    "default_model = get_default_pydantic_model(\"medication\")\n",
    "generator = get_outlines_generator(model, sampler, task = \"json\", schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema(medications = [{\"name\": \"Cipralex\", \"unit\": \"IE/mmol\", \"amount\": 20, \"morning\": 0.5, \"noon\": 0, \"evening\": 0, \"night\": 0, \"extra\": \"fÃ¼r 4 Tage\"}]).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Dataset.load_from_disk(paths.DATA_PATH_PREPROCESSED/\"medication/kisim_medication_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at different medication formats:\n",
    "for idx, text in enumerate(df[\"text\"]):\n",
    "    print(5*\"---\")\n",
    "    print(idx)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(paths.DATA_PATH_PREPROCESSED/\"medication/task_instruction.txt\", \"r\") as f:\n",
    "    task_instruction = f.read()\n",
    "\n",
    "with open(paths.DATA_PATH_PREPROCESSED/\"medication/system_prompt.txt\", \"r\") as f:\n",
    "    system_prompt = f.read()\n",
    "\n",
    "with open(paths.DATA_PATH_PREPROCESSED/\"medication/examples.json\", \"r\") as file:\n",
    "    examples = json.load(file)                  \n",
    "format_fun = get_format_fun(\"few_shot_instruction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_input = format_prompt(df[\"text\"], format_fun, system_prompt = system_prompt, task_instruction=task_instruction, examples = examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results, successful = outlines_prompting_to(text = text_input, generator = generator, schema = schema, filename=\"medication-few-shot-instruction\", batch_size = 1, wait_time = 250) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[\"text\"][74]\n",
    "test_input = format_prompt([test], format_fun, system_prompt = system_prompt, task_instruction=task_instruction, examples = examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test)\n",
    "result, successful = outlines_medication_prompting(text= test_input, generator = generator, max_tokens = 1000, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in generator.stream(test_input, max_tokens = 200):\n",
    "    print(token, end = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(result, paths.RESULTS_PATH/\"medication/testing.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = torch.load(paths.RESULTS_PATH/\"medication/testing.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[res.json() for res in test_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medication_prompting_to(text: list[str], generator: SequenceGenerator, default_model:Type[BaseModel], batch_size: int = 1, wait_time:int = 120)-> list[Union[str, BaseModel]]:\n",
    "    \"\"\"\n",
    "    Generates a list of sequences using the given outlines generator and sampler. Function has built in time-out function, as the generation is prone\n",
    "    to hang with a complicated schema.\n",
    "\n",
    "    Args:\n",
    "        text (list[str]): list of strings to be used as prompts\n",
    "        generator (outlines.SequenceGenerator): outlines generator\n",
    "        default_model (Type[BaseModel]): default pydantic model to return if the generation times out\n",
    "        batch_size (int, optional): batch size. Defaults to 1.\n",
    "        wait_time (int, optional): wait time. Defaults to 120.\n",
    "\n",
    "    Returns:\n",
    "        list[Union[str, pydantic.BaseModel]]: list of generated sequences\n",
    "    \"\"\"\n",
    "\n",
    "    dataloader = DataLoader(text, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "    results = []\n",
    "    successful = []\n",
    "\n",
    "    # Save intermediate results\n",
    "    filename = \"intermediate_results\" + str(time.time()) + \".pt\"\n",
    "\n",
    "    def timeout_handler(signum, frame):\n",
    "        raise TimeoutError(\"Timed out\")\n",
    "    \n",
    "    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "    bar_prompt = tqdm(dataloader, desc=\"Prompting\", leave=False)\n",
    "\n",
    "    for batch in bar_prompt:\n",
    "        try:\n",
    "            signal.alarm(wait_time)  # Set the timeout\n",
    "            result = generator(batch)\n",
    "            if batch_size == 1:\n",
    "                result = \n",
    "            results.extend(result)\n",
    "            successful.extend([True] * len(batch))\n",
    "        \n",
    "        except TimeoutError:\n",
    "            print(\"Timed out at observation number\", len(results))\n",
    "            successful.extend([False] * len(batch))\n",
    "            results.extend([default_model for _ in range(len(batch))])\n",
    "\n",
    "        # except TimeoutError:\n",
    "        #     print(\"Timed out, trying stream_input\")\n",
    "        #     bar_stream = tqdm(batch, desc=\"Stream input\", leave=False)\n",
    "        #     for text in bar_stream:\n",
    "        #         _res = stream_input(text, generator)\n",
    "        #         try: \n",
    "        #             _res = schema.model_validate_json(_res)\n",
    "        #             successful.append(True)\n",
    "        #         except:\n",
    "        #             successful.append(False)\n",
    "        #         results.append(_res)\n",
    "        finally:\n",
    "            signal.alarm(0)\n",
    "        print(results)\n",
    "        print(type(results))\n",
    "        print(len(results))\n",
    "        # if batch_size == 1:\n",
    "        #     # Using this because generator returns tuples with the first item being the keys of the highest pedantic schema, the second being the value\n",
    "        #     results_unpacked = [res[1] for res in results]\n",
    "        # results_json = [res.json() for res in results_unpacked]\n",
    "        # results_json += [{\"successful\": s} for s in successful]\n",
    "        # os.makedirs(paths.RESULTS_PATH/\"intermediate\", exist_ok=True)\n",
    "        # torch.save(results_json, paths.RESULTS_PATH/\"intermediate\"/filename)\n",
    "        # print(f\"Saved intermediate results to {paths.RESULTS_PATH/'intermediate'}/{filename}\")\n",
    "    return results, successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2, successful2 = outlines_prompting_to(text = test_input, generator = generator, default_model = default_model, batch_size = 2, wait_time = 180) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(results[0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medication\n",
    "\n",
    "MedicationList(medications = results[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_model.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(paths.RESULTS_PATH/\"intermediate/intermediate_results1710344797.0171728.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"medication_test\"\n",
    "results_dump = [res.json() for res in results]\n",
    "torch.save(results_dump, paths.RESULTS_PATH/\"medication\"/f\"{filename}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[\"text\"][1]\n",
    "test_few_shot = format_prompt([test], few_shot_instruction, system_prompt = system_prompt, task_instruction=task_instruction, examples = examples)\n",
    "print(test_few_shot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test)\n",
    "for token in generator.stream(test_few_shot):\n",
    "    print(token, end =\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instruct= format_prompt([test], zero_shot_instruction, system_prompt = system_prompt, task_instruction=task_instruction, examples = examples)\n",
    "print(test)\n",
    "for token in generator.stream(test_instruct):\n",
    "    print(token, end =\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = df[\"text\"][38]\n",
    "test1_few_shot = format_prompt([test1], few_shot_instruction, system_prompt = system_prompt, task_instruction=task_instruction, examples = examples)\n",
    "print(test1)\n",
    "for token in generator.stream(test1_few_shot):\n",
    "    print(token, end =\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_instruct = format_prompt([test1], zero_shot_instruction, system_prompt = system_prompt, task_instruction=task_instruction, examples = examples)\n",
    "print(test1)\n",
    "for token in generator.stream(test1_instruct):\n",
    "    print(token, end =\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = df[\"text\"][23]\n",
    "test3_few_shot = format_prompt([test3], few_shot_instruction, system_prompt = system_prompt, task_instruction=task_instruction, examples = examples)\n",
    "print(test3)\n",
    "for token in generator.stream(test3_few_shot):\n",
    "    print(token, end =\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3_instruct = format_prompt([test3], zero_shot_instruction, system_prompt = system_prompt, task_instruction=task_instruction, examples = examples)\n",
    "print(test3)\n",
    "for token in generator.stream(test3_instruct):\n",
    "    print(token, end =\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3 = generator(test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems:\n",
    "- Some inputs are structured with one line per medication e.g. df[\"text\"][0], others are medical recipes like df[\"text\"][38]. The model struggles a bit sometimes with inputs that are not as well structured (but still really good)\n",
    "- The problem above seems to be solvable by providing appropriate examples but I don't know if I get all the different input formats.\n",
    "- Sometimes medications are misspelled (like Propanolol) and model extracts it the way it was (which is the desired behaviour I think, because I don't have the medical expertise to correct it). Unsure what the best way to correct it is.\n",
    "- A lot of times the schema for intake changes, so after 2 weeks maybe it is less or more. Additionally extracting this in detail could be very hard and might negatively affect the performance of the other outputs (which seem more important to me, but I am no doctor). This is also only the case for a few of the examples as far as I can tell.\n",
    "- If the text just mentions \"once daily\" or similar I told the model to map it all in the morning (so once daily is 1-0-0) but not sure if that would be desired behaviour.\n",
    "- How would I evaluate the performance (spelling mistakes, forget medicine etc.). I could evaluate a test set myself (100 examples) but I can't guarantee that the criteria I set would be reasonable from a medical point of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = get_sampler(\"multinomial\")\n",
    "generator = get_outlines_generator(model = model, sampler = sampler, task = \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Dataset.load_from_disk(paths.DATA_PATH_PREPROCESSED/\"medication/kisim_medication_sample\")\n",
    "results = torch.load(paths.RESULTS_PATH/\"medication/medication_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_examples_10.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"model_answers\"][77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"][77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"\"\"[INST]Your task is to extract specific information from medication descriptions. \n",
    "The input for this task is a list of medication descriptions, a report or doctors recipe, and the output should be how each these medications have to be taken during the day.\n",
    "The intake follows for each medication can follow a specific schema, which has to be translated to a format like 0.5-1-2-0\n",
    "The output should consist of the following:\n",
    "- morning (float): The dose to be taken in the morning.\n",
    "- noon (float): The dose to be taken at noon.\n",
    "- evening (float): The dose to be taken in the evening.\n",
    "- night (float): The dose to be taken at night.\n",
    "\n",
    "The output format should be: \n",
    "\n",
    "float-float-float-float\n",
    "\n",
    "This corresponds to morningDose-noonDose-eveningDose-nightDose\n",
    "\n",
    "- The intake doses over the day can be given several ways:\n",
    "    - If the amount of doses is given in the form of float-float-float, it corresponds to MorningDose-NoonDose-EveningDose with NightDose being 0.\n",
    "    - If the amount of doses is given in the form float-float-float-float, it corresponds to MorningDose-NoonDose-EveningDose-NightDose.\n",
    "    - If keywords like \"Morgen\", \"Mittag\", \"Abend\", \"Nacht\" are used, the corresponding doses should be extracted and the others set set 0.\n",
    "    - If an intake schema like the ones above is not detected, MorningDose, NoonDose, EveningDose and NightDose should all be represented as -99.\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "Input:\n",
    "Fampyra 10 mg \\t\\t1-1-0\\nSifrol 0.125 mg \\t\\t2h vor Schlafegehen\n",
    "\n",
    "Output:\n",
    "Let's think step by step.\n",
    "\n",
    "Step 1: Split Input\n",
    "First, we need to split the input into separate medication descriptions.\n",
    "\n",
    "Fampyra 10 mg     1-1-0\n",
    "Sifrol 0.125 mg     2h vor Schlafegehen\n",
    "\n",
    "Step 2: Extract Intake Schema\n",
    "Now, we'll go through each medication description to extract the intake schema.\n",
    "\n",
    "For Fampyra 10 mg:\n",
    "The intake schema is given as 1-1-0, which corresponds to Morning-Noon-Evening with Night being 0.\n",
    "So the output for Fampyra would be: 1.0-1.0-0.0-0.0\n",
    "\n",
    "For Sifrol 0.125 mg:\n",
    "The description mentions \"2h vor Schlafegehen\", which means \"2 hours before going to sleep\". This corresponds to NightDose.\n",
    "So the output for Sifrol would be: 0.0-0.0-0.0-1.0\n",
    "\n",
    "Answer:\n",
    "1.0-1.0-0.0-0.0\n",
    "0.0-0.0-0.0-1.0\n",
    "Each line corresponds to one medication's intake schema in the format: morning-noon-evening-night. If a dose is not mentioned, it's represented as 0.0.\n",
    "\n",
    "###Input:\n",
    "Vitamine A AS zur Nacht\n",
    "\n",
    "[/INST]\n",
    "Let's think step by step:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in generator.stream(input):\n",
    "    print(token, end = \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
