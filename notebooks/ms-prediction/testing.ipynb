{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, DataCollatorWithPadding\n",
    "\n",
    "from datasets import DatasetDict, Features, Sequence, Value, load_dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import gc\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "from src import paths\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(paths.MODEL_PATH/'llama2', padding_side='left')\n",
    "data_files = {\"train\": \"ms-diag_clean_train.csv\", \"validation\": \"ms-diag_clean_val.csv\", \"test\": \"ms-diag_clean_test.csv\"}\n",
    "df = load_dataset(os.path.join(paths.DATA_PATH_PREPROCESSED,'ms-diag'), data_files = data_files)\n",
    "text = df[\"train\"][\"text\"] + df[\"validation\"][\"text\"] + df[\"test\"][\"text\"]\n",
    "tokens = [tokenizer(t) for t in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, DataCollatorWithPadding\n",
    "\n",
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "\n",
    "from src import paths\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import argparse\n",
    "from typing import Tuple\n",
    "\n",
    "MODEL_PATH = paths.MODEL_PATH/'llama2-chat'\n",
    "QUANTIZATION = \"4bit\"\n",
    "\n",
    "\n",
    "BASE_PROMPT = \"<s>[INST]\\n<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{user_prompt}[/INST]\\n\\n{answer_init}\"\n",
    "SYSTEM_PROMP = \"Is the MS diagnosis in the text of type \\\"Sekundär progrediente Multiple Sklerose (SPMS)\\\", \\\"primäre progrediente Multiple Sklerose (PPMS)\\\" or \\\"schubförmig remittierende Multiple Sklerose (RRMS)\\\"?\"\n",
    "ANSWER_INIT = \"Based on the information provided in the text, the most likely diagnosis for the patient is: \"\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Zero Shot Classification with Llama2-Chat\")\n",
    "    parser.add_argument(\"--job_id\", type=str, default=\"unknown\", help=\"Job ID\")\n",
    "    parser.add_argument(\"--model_path\", type=str, default=MODEL_PATH, help=\"Path to the model. Defaults to llama2-chat\")\n",
    "    parser.add_argument(\"--quantization\", type=str, default=QUANTIZATION, help=\"Quantization. Must be one of 4bit or bfloat16. Defaults to 4bit\")\n",
    "    parser.add_argument(\"--base_prompt\", type=str, default=BASE_PROMPT, help=\"Base Prompt, must contain {system_prompt}, {user_prompt} and {answer_init}\")\n",
    "    parser.add_argument(\"--system_prompt\", type=str, default=SYSTEM_PROMP, help=\"System Prompt\")\n",
    "    parser.add_argument(\"--answer_init\", type=str, default=ANSWER_INIT, help=\"Answer Initialization for model\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=BATCH_SIZE, help=\"Batch Size. Defaults to 4\")\n",
    "    parser.add_argument(\"--max_new_tokens\", type=int, default=MAX_NEW_TOKENS, help=\"Maximum number of new tokens to be generated. Defaults to 20\")\n",
    "\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Print or log the parsed arguments\n",
    "    print(\"Parsed Arguments:\")\n",
    "    for arg, value in vars(args).items():\n",
    "        print(f\"{arg}: {value}\")\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        for gpu_id in range(num_gpus):\n",
    "            free_mem, total_mem = torch.cuda.mem_get_info(gpu_id)\n",
    "            gpu_properties = torch.cuda.get_device_properties(gpu_id)\n",
    "            print(f\"GPU {gpu_id}: {gpu_properties.name}\")\n",
    "            print(f\"   Total Memory: {total_mem / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Free Memory: {free_mem / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Allocated Memory : {torch.cuda.memory_allocated(gpu_id) / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Reserved Memory : {torch.cuda.memory_reserved(gpu_id) / (1024 ** 3):.2f} GB\")\n",
    "    else:\n",
    "        print(\"No GPU available.\")\n",
    "\n",
    "\n",
    "# Load Model and tokenizer\n",
    "\n",
    "def load_model_and_tokenizer(model_path:os.PathLike, quantization:str = QUANTIZATION)->Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"Loads the model and tokenizer from the given path and returns the compiled model and tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        model_path (os.PathLike): Path to the model\n",
    "        quantization (str, optional): Quantization. Must be one of 4bit or bfloat16. Defaults to QUANTIZATION.\n",
    "\n",
    "        Returns:\n",
    "            tuple(AutoModelForCausalLM, AutoTokenizer): Returns the compiled model and tokenizer\n",
    "            \n",
    "    \"\"\"\n",
    "    # ### Model\n",
    "    if quantization == \"bfloat16\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "    elif quantization == \"4bit\":\n",
    "        bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                        bnb_4bit_use_double_quant=True,\n",
    "                                        bnb_4bit_quant_type=\"nf4\",\n",
    "                                        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", quantization_config=bnb_config)\n",
    "    else:\n",
    "        raise ValueError(\"Quantization must be one of 4bit or bfloat16\")\n",
    "    \n",
    "    ### Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"left\")\n",
    "\n",
    "    # Check if the pad token is already in the tokenizer vocabulary\n",
    "    if '<pad>' not in tokenizer.get_vocab():\n",
    "        # Add the pad token\n",
    "        tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "    \n",
    "\n",
    "    #Resize the embeddings\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    #Configure the pad token in the model\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Check if they are equal\n",
    "    assert model.config.pad_token_id == tokenizer.pad_token_id, \"The model's pad token ID does not match the tokenizer's pad token ID!\"\n",
    "\n",
    "    # Print the pad token ids\n",
    "    print('Tokenizer pad token ID:', tokenizer.pad_token_id)\n",
    "    print('Model pad token ID:', model.config.pad_token_id)\n",
    "    print('Model config pad token ID:', model.config.pad_token_id)\n",
    "    print(\"Vocabulary Size with Pad Token: \", len(tokenizer))\n",
    "\n",
    "    return torch.compile(model), tokenizer # Compile Model for faster inference. # To-Do https://pytorch.org/blog/pytorch-compile-to-speed-up-inference/\n",
    "\n",
    "\n",
    "def load_data()->DatasetDict:\n",
    "    \"\"\"Loads the data for MS-Diag task and returns the dataset dictionary\n",
    "    \n",
    "    Returns:\n",
    "        DatasetDict: Returns the dataset dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    data_files = {\"train\": \"ms-diag_clean_train.csv\", \"validation\": \"ms-diag_clean_val.csv\", \"test\": \"ms-diag_clean_test.csv\"}\n",
    "\n",
    "    df = load_dataset(os.path.join(paths.DATA_PATH_PREPROCESSED,'ms-diag'), data_files = data_files)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_data(df:DatasetDict, tokenizer:AutoTokenizer, split:str=\"all\", truncation_size:int = 300)->list[str]:\n",
    "    \"\"\"Returns a list of input texts for the classification task\n",
    "    \n",
    "    Args:\n",
    "        df (DatasetDict): Dataset dictionary\n",
    "        tokenizer (AutoTokenizer): Tokenizer\n",
    "        split (str, optional): Split. Must be one of train, validation, test or all. Defaults to \"all\".\n",
    "        truncation_size (int, optional): Truncation size. Defaults to 300.\n",
    "        \n",
    "    Returns:\n",
    "        list(str): Returns a list of input texts for the classification task\n",
    "    \"\"\"\n",
    "\n",
    "    def format_prompt(text:str)->str:\n",
    "        \"\"\"Truncates the text to the given truncation size and formats the prompt.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text\n",
    "        \n",
    "        Returns:\n",
    "            str: Returns the formatted prompt\n",
    "        \"\"\"\n",
    "        if len(text) > truncation_size:\n",
    "            text = text[:truncation_size]\n",
    "        else:\n",
    "            pass\n",
    "        input = BASE_PROMPT.format(system_prompt = SYSTEM_PROMP,\n",
    "                                user_prompt = text,\n",
    "                                answer_init = ANSWER_INIT)\n",
    "\n",
    "        return input\n",
    "\n",
    "    \n",
    "    # Tokenize the text\n",
    "    if split == \"all\":\n",
    "        text = df[\"train\"][\"text\"] + df[\"validation\"][\"text\"] + df[\"test\"][\"text\"]\n",
    "    else:\n",
    "        text = df[split][\"text\"]\n",
    "\n",
    "    tokens = [tokenizer(format_prompt(t)) for t in text]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def get_DataLoader(tokens:list[dict], tokenizer:AutoTokenizer, batch_size:int = BATCH_SIZE, padding:bool = True)->DataLoader:\n",
    "    \"\"\"Returns a DataLoader for the given dataset dictionary\n",
    "    \n",
    "    Args:\n",
    "        tokens (list[dict]): List of tokenized texts. One dictionary per text with keys input_ids and attention_mask.\n",
    "        tokenizer (AutoTokenizer): Tokenizer\n",
    "        batch_size (int, optional): Batch size. Defaults to global BATCH_SIZE.\n",
    "        padding (bool, optional): Padding. Defaults to True.\n",
    "        \n",
    "    Returns:\n",
    "        DataLoader: Returns a DataLoader for the given dataset dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    # Default collate function \n",
    "    collate_fn = DataCollatorWithPadding(tokenizer, padding=padding)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset=tokens, collate_fn=collate_fn, batch_size=batch_size, shuffle = False) \n",
    "\n",
    "    return dataloader\n",
    "\n",
    "def generate_outputs(model:AutoModelForCausalLM, tokenizer:AutoTokenizer, dataloader:DataLoader, generation_config:dict)->list[str]:\n",
    "    \"\"\"Generates outputs for the given model, tokenizer and dataloader\n",
    "    \n",
    "    Args:\n",
    "        model (AutoModelForCausalLM): Model\n",
    "        tokenizer (AutoTokenizer): Tokenizer\n",
    "        dataloader (DataLoader): DataLoader\n",
    "        generation_config (dict): Generation Config\n",
    "        \n",
    "    Returns:\n",
    "        list[str]: Returns a list of outputs\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "\n",
    "    for idx, batch in enumerate(tqdm.tqdm(dataloader)):\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "        with torch.inference_mode():\n",
    "            generated_ids = model.generate(input_ids = input_ids, \n",
    "                                            attention_mask = attention_mask,\n",
    "                                            **generation_config).to(\"cpu\")\n",
    "    \n",
    "        outputs.append(tokenizer.batch_decode(generated_ids, skip_special_tokens=True))\n",
    "        check_gpu_memory()\n",
    "        break\n",
    "\n",
    "    # Free Memory\n",
    "    del input_ids\n",
    "    del attention_mask\n",
    "    del generated_ids\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def get_generation_config(strategy:str=\"greedy\")->dict:\n",
    "    \"\"\"Returns the generation config for the given strategy\n",
    "\n",
    "    Args:\n",
    "        strategy (str, optional): Strategy. Must be one of greedy, contrastive, sampling or beam. Defaults to \"greedy\".\n",
    "\n",
    "    Returns:\n",
    "        dict: Returns the generation config for the given strategy\n",
    "    \"\"\"\n",
    "\n",
    "    # Greedy Search Configuration\n",
    "    greedy_search_config = {\n",
    "        \"do_sample\": False,\n",
    "        \"num_beams\": 1,\n",
    "        \"max_new_tokens\": 20,\n",
    "        \"temperature\": 1, \n",
    "        \"top_p\": 1, # 1 means no top_p sampling filter\n",
    "        \"top_k\": 0, # 0 means no top_k sampling filter\n",
    "        \"penalty_alpha\": 0.0\n",
    "    }\n",
    "\n",
    "    # Contrastive Search Configuration\n",
    "    contrastive_search_config = {\n",
    "        \"do_sample\": False,\n",
    "        \"num_beams\": 1,\n",
    "        \"max_new_tokens\": 20,\n",
    "        \"temperature\": 1, \n",
    "        \"top_p\": 1, # 1 means no top_p sampling filter\n",
    "        \"top_k\": 4, # 0 means no top_k sampling filter\n",
    "        \"penalty_alpha\": 0.6\n",
    "    }\n",
    "\n",
    "    # Sampling Configuration\n",
    "    sampling_config = {\n",
    "        \"do_sample\": True,\n",
    "        \"num_beams\": 1,\n",
    "        \"max_new_tokens\": 20,\n",
    "        \"temperature\": 0.7, \n",
    "        \"top_p\": 0.6, # 1 means no top_p sampling filter\n",
    "        \"top_k\": 50, # 0 means no top_k sampling filter\n",
    "        \"penalty_alpha\": 0.0\n",
    "    }\n",
    "\n",
    "    # Beam Search Configuration\n",
    "    beam_search_config = {\n",
    "        \"do_sample\": False,\n",
    "        \"num_beams\": 4,\n",
    "        \"max_new_tokens\": 20,\n",
    "        \"temperature\": 1, \n",
    "        \"top_p\": 1, # 1 means no top_p sampling filter\n",
    "        \"top_k\": 0, # 0 means no top_k sampling filter\n",
    "        \"penalty_alpha\": 0.0\n",
    "    }\n",
    "\n",
    "    if strategy == \"greedy\":\n",
    "        return greedy_search_config\n",
    "    elif strategy == \"contrastive\":\n",
    "        return contrastive_search_config\n",
    "    elif strategy == \"sampling\":\n",
    "        return sampling_config\n",
    "    elif strategy == \"beam\":\n",
    "        return beam_search_config\n",
    "    else:\n",
    "        raise ValueError(\"Strategy must be one of greedy, contrastive, sampling or beam\")\n",
    "\n",
    "def main():\n",
    "\n",
    "    # # Parse Arguments\n",
    "    # args = parse_args()\n",
    "\n",
    "    # # Set Arguments\n",
    "    # JOB_ID = args.job_id\n",
    "    # MODEL_PATH = args.model_path\n",
    "    # QUANTIZATION = args.quantization\n",
    "    # BASE_PROMPT = args.base_prompt\n",
    "    # SYSTEM_PROMP = args.system_prompt\n",
    "    # ANSWER_INIT = args.answer_init\n",
    "    # BATCH_SIZE = args.batch_size\n",
    "    # MAX_NEW_TOKENS = args.max_new_tokens\n",
    "    \n",
    "    # Iterate over different truncation sizes and strategies\n",
    "    strategies = [\"greedy\", \"contrastive\", \"sampling\", \"beam\"]\n",
    "    truncation_sizes = [300]\n",
    "\n",
    "    # Load Data, Model and Tokenizer\n",
    "    df = load_data()\n",
    "\n",
    "    print(\"GPU Memory before Model is loaded:\\n\")\n",
    "    check_gpu_memory()\n",
    "    model, tokenizer = load_model_and_tokenizer(MODEL_PATH, quantization=QUANTIZATION)\n",
    "    \n",
    "    print(\"GPU Memory after Model is loaded:\\n\")\n",
    "    check_gpu_memory()\n",
    "\n",
    "    results = None\n",
    "\n",
    "    for input_size in truncation_sizes:\n",
    "        print(\"Starting with input size: {}\".format(input_size))\n",
    "\n",
    "        for strat in strategies:\n",
    "            print(\"Starting with strategy: {}\".format(strat))\n",
    "\n",
    "            # Prepare Data\n",
    "            tokens = prepare_data(df, tokenizer, split=\"all\", truncation_size=input_size)\n",
    "\n",
    "            # Get DataLoader\n",
    "            dataloader = get_DataLoader(tokens, tokenizer, batch_size=BATCH_SIZE, padding=True)\n",
    "\n",
    "            # Get Generation Config\n",
    "            generation_config = get_generation_config(strategy=strat)\n",
    "\n",
    "            # Generate Outputs\n",
    "            outputs = generate_outputs(model, tokenizer, dataloader, generation_config=generation_config)\n",
    "\n",
    "            # Extract the generated answers\n",
    "            outputs = list(chain.from_iterable(outputs))\n",
    "            outputs = [out.split(ANSWER_INIT)[1] for out in outputs]\n",
    "\n",
    "            col_name = f\"truncate_{input_size}_strategy_{strat}\"\n",
    "            \n",
    "            if results is None:\n",
    "                results = pd.DataFrame({col_name: outputs})\n",
    "            else:\n",
    "                results[col_name] = outputs\n",
    "            \n",
    "            break\n",
    "        break\n",
    "\n",
    "    file_name = f\"ms_diag-llama2-chat_zero-shot_generation-strats_{JOB_ID}.csv\"\n",
    "\n",
    "    results.to_csv(os.path.join(paths.DATA_PATH, file_name), index=False)\n",
    "\n",
    "    return\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before Model is loaded:\n",
      "\n",
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 10.20 GB\n",
      "   Allocated Memory : 0.00 GB\n",
      "   Reserved Memory : 0.00 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0fefc8e9c144d44bf6fd054473056a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer pad token ID: 32000\n",
      "Model pad token ID: 32000\n",
      "Model config pad token ID: 32000\n",
      "Vocabulary Size with Pad Token:  32001\n"
     ]
    }
   ],
   "source": [
    "# Iterate over different truncation sizes and strategies\n",
    "strategies = [\"greedy\", \"sampling\", \"beam\"]\n",
    "truncation_sizes = [300]\n",
    "\n",
    "# Load Data, Model and Tokenizer\n",
    "df = load_data()\n",
    "\n",
    "print(\"GPU Memory before Model is loaded:\\n\")\n",
    "check_gpu_memory()\n",
    "model, tokenizer = load_model_and_tokenizer(MODEL_PATH, quantization=QUANTIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory after Model is loaded:\n",
      "\n",
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 5.82 GB\n",
      "   Allocated Memory : 4.18 GB\n",
      "   Reserved Memory : 4.38 GB\n",
      "Starting with input size: 300\n",
      "Starting with strategy: greedy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/49 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  0%|          | 0/49 [00:10<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 4.86 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 5.14 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with strategy: sampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/49 [00:07<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 4.80 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 5.19 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with input size: 600\n",
      "Starting with strategy: greedy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/49 [00:08<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 3.93 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 6.07 GB\n",
      "Starting with strategy: sampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/49 [00:08<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 3.93 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 6.07 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        truncate_300_strategy_greedy  \\\n",
      "0  \\n\\n\"Sekundär progrediente Multiple Sklerose (...   \n",
      "1  \\n\\n\"primäre progrediente Multiple Sklerose (P...   \n",
      "2  \\n\\n\"schubförmig remittierende Multiple Sklero...   \n",
      "3  \\n\\n\"schubförmig remittierende Multiple Sklero...   \n",
      "\n",
      "                      truncate_300_strategy_sampling  \\\n",
      "0  \\n\\n* Sekundär progrediente Multiple Sklerose ...   \n",
      "1  \\n\\n\"primäre progrediente Multiple Sklerose (P...   \n",
      "2  \\n\\n\"schubförmig remittierende Multiple Sklero...   \n",
      "3  \\n\\n\"schubförmig remittierende Multiple Sklero...   \n",
      "\n",
      "                        truncate_600_strategy_greedy  \\\n",
      "0  \\n\\nSekundär progrediente Multiple Sklerose (S...   \n",
      "1  \\n\\n\"primäre progrediente Multiple Sklerose (P...   \n",
      "2  \\n\\n\"schubförmig remittierende Multiple Sklero...   \n",
      "3  \\n\\nSekundär progrediente Multiple Sklerose (S...   \n",
      "\n",
      "                      truncate_600_strategy_sampling  \n",
      "0  \\n\\nSekundär progrediente Multiple Sklerose (S...  \n",
      "1  \\n\\n\"primäre progrediente Multiple Sklerose (P...  \n",
      "2  \\n\\n\"schubförmig remittierende Multiple Sklero...  \n",
      "3  \\n\\n\"schubförmig remittierende Multiple Sklero...  \n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache() \n",
    "print(\"GPU Memory after Model is loaded:\\n\")\n",
    "check_gpu_memory()\n",
    "\n",
    "# Iterate over different truncation sizes and strategies\n",
    "strategies = [\"greedy\", \"sampling\"]\n",
    "truncation_sizes = [300, 600]\n",
    "results = None\n",
    "\n",
    "for input_size in truncation_sizes:\n",
    "    print(\"Starting with input size: {}\".format(input_size))\n",
    "\n",
    "    for strat in strategies:\n",
    "        print(\"Starting with strategy: {}\".format(strat))\n",
    "\n",
    "        # Prepare Data\n",
    "        tokens = prepare_data(df, tokenizer, split=\"all\", truncation_size=input_size)\n",
    "\n",
    "        # Get DataLoader\n",
    "        dataloader = get_DataLoader(tokens, tokenizer, batch_size=BATCH_SIZE, padding=True)\n",
    "\n",
    "        # Get Generation Config\n",
    "        generation_config = get_generation_config(strategy=strat)\n",
    "\n",
    "        # Generate Outputs\n",
    "        outputs = generate_outputs(model, tokenizer, dataloader, generation_config=generation_config)\n",
    "\n",
    "        # Extract the generated answers\n",
    "        outputs = list(chain.from_iterable(outputs))\n",
    "        outputs = [out.split(ANSWER_INIT)[1] for out in outputs]\n",
    "\n",
    "        col_name = f\"truncate_{input_size}_strategy_{strat}\"\n",
    "        \n",
    "        if results is None:\n",
    "            results = pd.DataFrame({col_name: outputs})\n",
    "        else:\n",
    "            results[col_name] = outputs\n",
    "\n",
    "file_name = f\"ms_diag-llama2-chat_zero-shot_generation-strats_testing.csv\"\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(os.path.join(paths.RESULTS_PATH, \"ms-diag\", file_name), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
