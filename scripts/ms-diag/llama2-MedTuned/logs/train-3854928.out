=====================================================================
                   Welcome to Leonhard Med v.2.0.1                    

 * Documentation:  https://unlimited.ethz.ch/display/LEOMED2
 * Support:        leomed-support@id.ethz.ch
 * Software stack: type "enable_modules" to use Modules

Tenant name: BIOMED
OS: Ubuntu 20.04.6 LTS x86_64 
Uptime: 11 days, 9 mins 
CPU: Intel (Broadwell, IBRS) (18) @ 2.199GHz 
CPU Usage: 4% 
GPU: NVIDIA GeForce GTX 1080 Ti 
GPU: NVIDIA GeForce GTX 1080 Ti 
GPU: NVIDIA GeForce GTX 1080 Ti 
GPU: NVIDIA GeForce GTX 1080 Ti 
GPU: NVIDIA GeForce GTX 1080 Ti 
GPU: NVIDIA GeForce GTX 1080 Ti 
GPU: NVIDIA GeForce GTX 1080 Ti 
GPU: NVIDIA GeForce GTX 1080 Ti 
Memory: 1676MiB / 243821MiB (0%) 

=====================================================================
Starting job with ID 3854928...
Parsed Arguments:
job_id: unknown
model_name: medbert-512
quantization: 4bit
batch_size: 16
lr: 0.0002
num_epochs: 8
gradient_accumulation_steps: 1
task_type: class
data: all
data_augmentation: oversample
num_labels: 4
peft_config: {"peft_type":"LORA","lora_alpha":16,"lora_dropout":0.1, "r":64, "bias":"none","task_type":"SEQ_CLS"}
attn_implementation: None
GPU 0: NVIDIA GeForce GTX 1080 Ti
   Total Memory: 10.91 GB
   Free Memory: 10.45 GB
   Allocated Memory : 0.00 GB
   Reserved Memory : 0.00 GB
Tokenizer pad token ID: 0
Tokenizer special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}
Model pad token ID: 0
Loaded Model and Tokenizer
Loaded Data
Starting Training
{'loss': 1.3854, 'learning_rate': 0.000175, 'epoch': 0.95}
{'eval_loss': 1.2705774307250977, 'eval_accuracy': 0.6666666666666666, 'eval_f1': 0.5555555555555555, 'eval_precision': 0.5, 'eval_recall': 0.6666666666666666, 'eval_runtime': 0.1298, 'eval_samples_per_second': 23.114, 'eval_steps_per_second': 7.705, 'epoch': 1.0}
{'loss': 1.3073, 'learning_rate': 0.00015000000000000001, 'epoch': 1.9}
{'eval_loss': 1.1594449281692505, 'eval_accuracy': 0.6666666666666666, 'eval_f1': 0.5, 'eval_precision': 0.5, 'eval_recall': 0.5, 'eval_runtime': 0.1245, 'eval_samples_per_second': 24.089, 'eval_steps_per_second': 8.03, 'epoch': 2.0}
{'loss': 1.1748, 'learning_rate': 0.000125, 'epoch': 2.86}
{'eval_loss': 0.9736418128013611, 'eval_accuracy': 0.3333333333333333, 'eval_f1': 0.25, 'eval_precision': 0.25, 'eval_recall': 0.25, 'eval_runtime': 0.1256, 'eval_samples_per_second': 23.876, 'eval_steps_per_second': 7.959, 'epoch': 3.0}
{'loss': 1.0358, 'learning_rate': 0.0001, 'epoch': 3.81}
{'eval_loss': 0.8251559138298035, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_runtime': 0.098, 'eval_samples_per_second': 30.616, 'eval_steps_per_second': 10.205, 'epoch': 4.0}
{'loss': 0.9718, 'learning_rate': 7.500000000000001e-05, 'epoch': 4.76}
{'eval_loss': 0.7950771450996399, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_runtime': 0.0992, 'eval_samples_per_second': 30.248, 'eval_steps_per_second': 10.083, 'epoch': 5.0}
{'loss': 0.8312, 'learning_rate': 5e-05, 'epoch': 5.71}
{'eval_loss': 0.7211287021636963, 'eval_accuracy': 0.6666666666666666, 'eval_f1': 0.5555555555555555, 'eval_precision': 0.5, 'eval_recall': 0.6666666666666666, 'eval_runtime': 0.1136, 'eval_samples_per_second': 26.415, 'eval_steps_per_second': 8.805, 'epoch': 6.0}
{'loss': 0.7766, 'learning_rate': 2.5e-05, 'epoch': 6.67}
{'eval_loss': 0.7095447182655334, 'eval_accuracy': 0.6666666666666666, 'eval_f1': 0.5555555555555555, 'eval_precision': 0.5, 'eval_recall': 0.6666666666666666, 'eval_runtime': 0.1285, 'eval_samples_per_second': 23.351, 'eval_steps_per_second': 7.784, 'epoch': 7.0}
{'loss': 0.6827, 'learning_rate': 0.0, 'epoch': 7.62}
{'eval_loss': 0.6961181163787842, 'eval_accuracy': 0.6666666666666666, 'eval_f1': 0.5555555555555555, 'eval_precision': 0.5, 'eval_recall': 0.6666666666666666, 'eval_runtime': 0.1124, 'eval_samples_per_second': 26.69, 'eval_steps_per_second': 8.897, 'epoch': 8.0}
{'train_runtime': 191.0606, 'train_samples_per_second': 13.734, 'train_steps_per_second': 0.879, 'train_loss': 1.006884753704071, 'epoch': 8.0}
Finished Training
Saving Model at: /cluster/dataset/midatams/inf-extr/resources/models/ms-diag_medbert-512_4bit_LORA_class_all_oversample
Saving Tokenizer at: /cluster/dataset/midatams/inf-extr/resources/models/ms-diag_medbert-512_4bit_LORA_class_all_oversample
Job finished
