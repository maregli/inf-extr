{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, DataCollatorWithPadding, get_scheduler\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset, ClassLabel\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "\n",
    "from src import paths\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import evaluate \n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_artifical_data_for_label(label:str):\n",
    "    label_dict = {\n",
    "        \"rrms\": \"relapsing_remitting_multiple_sclerosis\",\n",
    "        \"ppms\": \"primary_progressive_multiple_sclerosis\",\n",
    "        \"spms\": \"secondary_progressive_multiple_sclerosis\"\n",
    "    }\n",
    "    generated_data = pd.read_csv(paths.DATA_PATH_PREPROCESSED/f'ms-diag/artificial_{label}.csv')\n",
    "    generated_data[\"labels\"] = label_dict[label]\n",
    "    generated_data = generated_data[[\"0\", \"labels\"]].rename(columns = {\"0\":\"text\"})\n",
    "\n",
    "    return generated_data\n",
    "\n",
    "def get_artifical_data_all():\n",
    "    artifical_data = []\n",
    "    for label in [\"rrms\", \"ppms\", \"spms\"]:\n",
    "        try: \n",
    "            artifical_data.append(get_artifical_data_for_label(label))\n",
    "        except:\n",
    "            print(f\"Could not find data for {label}\")\n",
    "    artifical_data = pd.concat(artifical_data)\n",
    "    artifical_data = Dataset.from_pandas(artifical_data).remove_columns('__index_level_0__')\n",
    "    return artifical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data\n",
    "data_files = {\"train\": \"ms-diag_clean_train.csv\", \"validation\": \"ms-diag_clean_val.csv\", \"test\": \"ms-diag_clean_test.csv\", \"augmented\": \"ms-diag_augmented.csv\"}\n",
    "df = load_dataset(os.path.join(paths.DATA_PATH_PREPROCESSED,'ms-diag'), data_files = data_files)\n",
    "df_augmented = df[\"augmented\"]\n",
    "# df[\"train\"] = concatenate_datasets([df[\"augmented\"], df[\"train\"]])\n",
    "# Load corrected data\n",
    "df = load_dataset(\"csv\", data_files = os.path.join(paths.DATA_PATH_PREPROCESSED, \"ms-diag/ms-diag_content_annotated.csv\"))\n",
    "df = concatenate_datasets([df[\"train\"], df_augmented])\n",
    "new_features = df.features.copy()\n",
    "new_features[\"labels\"] = ClassLabel(names=[\"primary_progressive_multiple_sclerosis\", \"relapsing_remitting_multiple_sclerosis\", \"secondary_progressive_multiple_sclerosis\"])\n",
    "df = df.cast(new_features)\n",
    "df= df.filter(lambda x: x[\"contains_dm\"] == True)\n",
    "\n",
    "\n",
    "df = df.train_test_split(test_size=0.1, seed=42, stratify_by_column=\"labels\"\n",
    "                         )\n",
    "\n",
    "# Number of labels\n",
    "num_labels = len(set(df['train']['labels']))\n",
    "\n",
    "# Label to id\n",
    "label2id = {'primary_progressive_multiple_sclerosis': 0,\n",
    "            'relapsing_remitting_multiple_sclerosis': 1,\n",
    "            'secondary_progressive_multiple_sclerosis': 2}\n",
    "id2label = {v:k for k,v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run this cell if you want to download and fine-tune the model\n",
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# # Login to Hugging Face Hub as model is gated\n",
    "# notebook_login()\n",
    "\n",
    "# # Checkpoint\n",
    "# checkpoint = \"GerMedBERT/medbert-512\"\n",
    "\n",
    "# # Load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# # Save tokenizer\n",
    "# tokenizer.save_pretrained(paths.MODEL_PATH/'medbert')\n",
    "\n",
    "# # Load model for embedding\n",
    "# model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "# # Save model\n",
    "# model.save_pretrained(paths.MODEL_PATH/'medbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /mnt/c/Users/marc_/OneDrive/ETH/MSC_Thesis/inf-extr/resources/models/medbert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(paths.MODEL_PATH/'medbert')\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(paths.MODEL_PATH/'medbert', num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189f3d91d9cd474e9fb69127f0dd3d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d33b35a0d74d568e0ddba2478a5ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_data(data):\n",
    "    \n",
    "    # Label\n",
    "    # data['labels'] = [label2id[label] for label in data['labels']]\n",
    "\n",
    "    # Tokenize\n",
    "    # data[\"text\"] = [text[:256] for text in data[\"text\"]]\n",
    "    data = tokenizer(data['text'][:256], padding=True, truncation=True, return_tensors='pt', max_length = 512)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Tokenize dataset\n",
    "dataset = df.map(prepare_data, batched=True, batch_size=512, remove_columns=list(df['train'].features.keys())[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "EPOCHS = 12\n",
    "BATCH_SIZE = 2\n",
    "TRAIN_STEPS = EPOCHS * len(dataset['train']) // BATCH_SIZE\n",
    "NUM_GRADIENT_ACCUMULATION_STEPS = 4\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "# Collator\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer, padding=True, return_tensors='pt')\n",
    "\n",
    "# Dataloader\n",
    "train_loader = DataLoader(dataset['train'], batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "# val_loader = DataLoader(dataset['validation'], batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "test_loader = DataLoader(dataset['test'], batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "# Accelerator\n",
    "accelerator = Accelerator(mixed_precision='fp16')\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optim = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optim,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=TRAIN_STEPS\n",
    ")\n",
    "\n",
    "# Prepare with accelerator #I Removed val loader\n",
    "model, optim, train_loader, test_loader = accelerator.prepare(\n",
    "    model, optim, train_loader, test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/276 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  9%|▊         | 24/276 [00:04<00:53,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: F1 score: 0.3571428571428572 Loss: 1.0400390625 Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 47/276 [00:07<00:48,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: F1 score: 0.2222222222222222 Loss: 1.0548503398895264 Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 70/276 [00:11<00:43,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: F1 score: 0.2222222222222222 Loss: 1.0928955078125 Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 93/276 [00:15<00:38,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: F1 score: 0.2222222222222222 Loss: 1.1230876445770264 Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 116/276 [00:18<00:34,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: F1 score: 0.2222222222222222 Loss: 1.121337890625 Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 139/276 [00:22<00:29,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: F1 score: 0.2222222222222222 Loss: 1.0657145977020264 Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 162/276 [00:26<00:24,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: F1 score: 0.2222222222222222 Loss: 1.030517578125 Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 185/276 [00:29<00:19,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: F1 score: 0.2222222222222222 Loss: 1.0249837636947632 Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 208/276 [00:33<00:14,  4.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: F1 score: 0.2222222222222222 Loss: 1.0288900136947632 Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 231/276 [00:37<00:09,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: F1 score: 0.2222222222222222 Loss: 1.0257161855697632 Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 254/276 [00:41<00:04,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: F1 score: 0.2222222222222222 Loss: 1.0183919668197632 Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 276/276 [00:44<00:00,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: F1 score: 0.2222222222222222 Loss: 1.0264079570770264 Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "progress_bar = tqdm.tqdm(range(TRAIN_STEPS))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        optim.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss/NUM_GRADIENT_ACCUMULATION_STEPS\n",
    "        accelerator.backward(loss)\n",
    "        if step % NUM_GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            optim.step()\n",
    "            lr_scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = []\n",
    "        labels = []\n",
    "        val_loss = 0\n",
    "        acc = 0\n",
    "        for batch in test_loader:\n",
    "            outputs = model(**batch)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            \n",
    "            val_loss += outputs.loss\n",
    "            acc += (predictions == batch['labels']).sum().item()\n",
    "            \n",
    "            preds.extend(predictions.tolist())\n",
    "            labels.extend(batch['labels'].tolist())\n",
    "\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    # if epoch == 0:\n",
    "    #         min_val_loss = val_loss\n",
    "    #         largest_f1 = f1\n",
    "    # elif (val_loss < min_val_loss):\n",
    "    #     min_val_loss = val_loss\n",
    "    #     model.save_pretrained(paths.MODEL_PATH/'ms_diag_medbert_valloss')\n",
    "    # elif (largest_f1 < f1):\n",
    "    #     largest_f1 = f1\n",
    "    #     model.save_pretrained(paths.MODEL_PATH/'ms_diag_medbert_f1')\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: F1 score: {f1} Loss: {val_loss/len(test_loader)} Accuracy: {acc/len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
