{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaModel, LlamaTokenizer\n",
    "from datasets import DatasetDict, Features, Sequence, Value\n",
    "import datasets\n",
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "from src import paths\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download model\n",
    "# checkpoint = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# # Save model\n",
    "# model.save_pretrained(paths.MODEL_PATH/'llama2')\n",
    "\n",
    "# # Save tokenizer\n",
    "# tokenizer.save_pretrained(paths.MODEL_PATH/'llama2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(paths.MODEL_PATH/'llama2', padding_side='left')\n",
    "model = LlamaModel.from_pretrained(paths.MODEL_PATH/'llama2', device_map=\"auto\", load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device of model.embed_tokens.weight:  cuda:0\n",
      "Device of model.layers.0.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.0.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.0.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.0.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.0.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.0.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.0.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.0.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.0.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.1.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.1.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.1.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.1.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.1.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.1.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.1.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.1.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.1.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.2.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.2.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.2.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.2.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.2.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.2.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.2.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.2.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.2.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.3.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.3.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.3.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.3.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.3.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.3.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.3.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.3.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.3.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.4.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.4.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.4.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.4.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.4.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.4.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.4.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.4.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.4.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.5.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.5.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.5.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.5.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.5.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.5.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.5.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.5.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.5.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.6.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.6.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.6.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.6.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.6.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.6.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.6.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.6.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.6.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.7.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.7.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.7.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.7.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.7.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.7.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.7.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.7.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.7.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.8.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.8.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.8.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.8.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.8.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.8.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.8.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.8.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.8.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.9.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.9.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.9.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.9.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.9.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.9.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.9.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.9.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.9.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.10.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.10.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.10.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.10.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.10.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.10.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.10.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.10.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.10.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.11.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.11.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.11.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.11.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.11.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.11.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.11.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.11.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.11.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.12.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.12.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.12.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.12.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.12.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.12.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.12.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.12.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.12.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.13.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.13.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.13.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.13.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.13.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.13.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.13.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.13.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.13.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.14.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.14.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.14.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.14.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.14.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.14.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.14.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.14.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.14.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.15.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.15.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.15.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.15.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.15.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.15.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.15.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.15.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.15.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.16.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.16.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.16.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.16.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.16.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.16.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.16.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.16.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.16.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.17.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.17.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.17.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.17.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.17.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.17.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.17.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.17.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.17.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.18.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.18.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.18.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.18.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.18.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.18.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.18.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.18.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.18.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.19.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.19.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.19.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.19.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.19.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.19.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.19.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.19.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.19.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.20.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.20.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.20.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.20.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.20.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.20.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.20.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.20.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.20.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.21.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.21.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.21.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.21.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.21.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.21.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.21.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.21.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.21.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.22.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.22.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.22.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.22.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.22.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.22.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.22.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.22.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.22.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.23.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.23.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.23.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.23.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.23.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.23.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.23.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.23.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.23.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.24.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.24.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.24.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.24.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.24.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.24.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.24.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.24.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.24.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.25.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.25.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.25.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.25.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.25.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.25.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.25.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.25.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.25.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.26.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.26.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.26.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.26.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.26.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.26.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.26.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.26.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.26.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.27.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.27.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.27.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.27.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.27.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.27.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.27.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.27.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.27.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.28.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.28.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.28.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.28.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.28.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.28.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.28.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.28.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.28.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.29.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.29.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.29.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.29.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.29.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.29.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.29.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.29.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.29.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.30.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.30.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.30.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.30.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.30.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.30.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.30.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.30.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.30.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.31.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.31.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.31.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.31.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.31.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.31.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.31.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.31.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.31.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.norm.weight:  cuda:0\n",
      "Device of lm_head.weight:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check device allocation\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Device of {name}: \", param.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset = DatasetDict.load_from_disk(paths.DATA_PATH_PREPROCESSED/'line_labelling/line_labelling_clean_dataset')\n",
    "\n",
    "# Tokenize\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=256, return_tensors='pt')\n",
    "\n",
    "# # Set format of labels to FloatTensor\n",
    "features = Features({'labels': Sequence(Value(dtype='float32')),\n",
    "                     'input_ids': Sequence(Value(dtype='int32')),\n",
    "                     'attention_mask': Sequence(Value(dtype='int32')),\n",
    "                     'token_type_ids': Sequence(Value(dtype='int32')),\n",
    "                     'class_agg': Value(dtype='string'),\n",
    "                     'rid': Value(dtype='string'),\n",
    "                     'text': Value(dtype='string'),\n",
    "                     'class': Value(dtype='string')\n",
    "                     })\n",
    "\n",
    "# Tokenize dataset\n",
    "dataset = dataset.map(tokenize, batched=True, features=features)\n",
    "\n",
    "# Train/Val/Test \n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['val']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train embeddings, use mean over sequence of last layer\n",
    "def embed_dataset(\n",
    "        dataset: datasets.Dataset = None,\n",
    "        model: transformers.PreTrainedModel = None, \n",
    "        batch_size: int = 4\n",
    "        ) -> dict:\n",
    "    \"\"\" \n",
    "    Embeds a dataset using a model.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        dataset (datasets.Dataset): Dataset to embed.\n",
    "        model (transformers.PreTrainedModel): Model to use for embedding.\n",
    "        batch_size (int): Batch size to use for embedding.\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "            torch.Tensor: Embeddings of the dataset. Size (len(dataset), model.config.hidden_size, ).\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch = dataset[i:i+batch_size]\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "            embeddings.append(embeddings)\n",
    "        \n",
    "\n",
    "    return {'embeddings': torch.cat(embeddings, dim=0), 'labels': dataset['labels']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed train dataset\n",
    "BATCH_SIZE = 4\n",
    "train_embeddings = embed_dataset(train_dataset, model, BATCH_SIZE)\n",
    "val_embeddings = embed_dataset(val_dataset, model, BATCH_SIZE)\n",
    "test_embeddings = embed_dataset(test_dataset, model, BATCH_SIZE)\n",
    "\n",
    "# Save embeddings\n",
    "torch.save(train_embeddings, paths.DATA_PATH_PREPROCESSED/'line_label_pred/llama2-train_embeddings.pt')\n",
    "torch.save(val_embeddings, paths.DATA_PATH_PREPROCESSED/'line_label_pred/llama2-val_embeddings.pt')\n",
    "torch.save(test_embeddings, paths.DATA_PATH_PREPROCESSED/'line_label_pred/llama2-test_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Head (Linear)\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    \"\"\"Dataset for classification.\"\"\"\n",
    "    def __init__(self, embeddings: torch.Tensor, labels: torch.Tensor):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"Classification Head for n classes.\"\"\"\n",
    "    def __init__(self, input_dim: int = None, output_dim: int = 3):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.linear1 = nn.Linear(self.input_dim, self.output_dim)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10\n",
    "HIDDEN_DIM = train_embeddings['embeddings'].shape[1]\n",
    "OUTPUT_DIM = train_embeddings['labels'].shape[1]\n",
    "\n",
    "# Initialize datasets\n",
    "train_dataset = ClassificationDataset(train_embeddings['embeddings'], train_embeddings['labels'])\n",
    "val_dataset = ClassificationDataset(val_embeddings['embeddings'], val_embeddings['labels'])\n",
    "test_dataset = ClassificationDataset(test_embeddings['embeddings'], test_embeddings['labels'])\n",
    "\n",
    "# Initialize dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "model = ClassificationHead(input_dim=HIDDEN_DIM, output_dim=OUTPUT_DIM)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Initialize loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    bar = tqdm(train_dataloader)\n",
    "\n",
    "    for batch in bar:\n",
    "        optimizer.zero_grad()\n",
    "        embeddings, labels = batch\n",
    "        logits = model(embeddings)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        bar.set_description(f\"Epoch {epoch} loss: {loss.item():.5f}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    bar = tqdm(val_dataloader)\n",
    "    val_loss = 0\n",
    "    for batch in bar:\n",
    "        embeddings, labels = batch\n",
    "        with torch.no_grad():\n",
    "            logits = model(embeddings)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_dataloader)\n",
    "    print(f\"Validation loss: {val_loss:.5f}\")\n",
    "\n",
    "    # Save model if validation loss is lower than previous validation loss\n",
    "    if epoch == 0:\n",
    "        best_val_loss = val_loss\n",
    "    elif val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), paths.MODEL_PATH/'line_label_pred/llama2-classification-head.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "model.load_state_dict(torch.load(paths.MODEL_PATH/'line_label_pred/llama2-classification-head.pt'))\n",
    "model.eval()\n",
    "bar = tqdm(test_dataloader)\n",
    "logits = []\n",
    "labels = []\n",
    "embeddings = []\n",
    "\n",
    "for batch in bar:\n",
    "    embeddings, labels = batch\n",
    "    with torch.no_grad():\n",
    "        logits = model(embeddings)\n",
    "        logits.append(logits)\n",
    "        labels.append(labels)\n",
    "        embeddings.append(embeddings)\n",
    "\n",
    "results = {'logits': torch.cat(logits, dim=0), 'labels': torch.cat(labels, dim=0), 'embeddings': torch.cat(embeddings, dim=0)}\n",
    "torch.save(results, paths.RESULTS_PATH/'line_labelling-LLAMA2-classification-test_output.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(['A list of colors: red, blue'], return_tensors=\"pt\").to(\"cuda\")\n",
    "model_inputs = {k: v.to(torch.int32).to(\"cuda\") for k, v in model_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int32"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs['input_ids'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(**model_inputs, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A list of colors: red, blue, and yellow. профн.\\n1. The list of colors: red, blue, and'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
      " Unterscheidung: Die Schlacht im Hürtgenwald. München: List, 2003.\n",
      "# 3\n",
      "# 1945\n",
      "Ich warf den Kopf auf die Seite und starrte auf den Boden. Die Erde war von einer schmutzigen, dicken Asche bedeckt. Das Blut von den Wunden, die ich auf der Stelle bekommen hatte, war von der Erde verblasst.\n",
      "Ich schlug die Augen wieder auf. Auf dem Boden vor mir lag ein Mann mit einem weißen Hemd, das mit Blut und Schmutz übersät war. Seine Hände lagen auf der Brust, die Augen waren geschlossen, und die Mundwinkel waren gesenkt. Ich scha\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[535]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess as sp\n",
    "import os\n",
    "\n",
    "def get_gpu_memory():\n",
    "    command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    return memory_free_values\n",
    "\n",
    "get_gpu_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
