{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95b2955a-cfd9-48ec-8a22-a10c2f3546f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "\n",
    "from src import paths\n",
    "\n",
    "from src import paths\n",
    "from src.utils import load_ms_data, prepare_ms_data, get_DataLoader, load_model_and_tokenizer, perform_inference, check_gpu_memory\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb2716-64d9-4319-b018-778081297fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "PEFT_MODEL_NAMES = [\"ms-diag_llama2_4bit_lora_augmented_256\", \"ms-diag_llama2_4bit_lora_augmented_512\"]\n",
    "\n",
    "# Check GPU Memory\n",
    "check_gpu_memory()\n",
    "\n",
    "# Load Model and Tokenizer\n",
    "model, tokenizer = load_model_and_tokenizer(model_name=\"llama2\", quantization=\"4bit\", num_labels=3)\n",
    "\n",
    "print(\"Loaded Model and Tokenizer\")\n",
    "\n",
    "# Inference\n",
    "for peft_model_name in PEFT_MODEL_NAMES:\n",
    "\n",
    "    print(f\"Starting Inference for PEFT Model: {peft_model_name}\")\n",
    "\n",
    "    # Load PEFT Model\n",
    "    peft_model = PeftModel.from_pretrained(model, paths.MODEL_PATH/peft_model_name).to(device)\n",
    "\n",
    "    # Load Data in format matching the PEFT Model configuration\n",
    "    df = load_ms_data(data=\"original\")\n",
    "\n",
    "    # Prepare Data\n",
    "    truncation_size = int(peft_model_name.split(\"_\")[-1])\n",
    "    peft_type = peft_model_name.split(\"_\")[-3]\n",
    "\n",
    "    if peft_type in [\"prompt\", \"ptune\", \"prefix\"]:\n",
    "        is_prompt_tuning = True\n",
    "        num_virtual_tokens = peft_model.peft_config[\"default\"].num_virtual_tokens\n",
    "    else:\n",
    "        is_prompt_tuning = False\n",
    "        num_virtual_tokens = 0\n",
    "    \n",
    "    encoded_dataset = prepare_ms_data(df, tokenizer, is_prompt_tuning = is_prompt_tuning, num_virtual_tokens = num_virtual_tokens, truncation_size=truncation_size)\n",
    "\n",
    "    # Get DataLoaders\n",
    "    dataloader = get_DataLoader(encoded_dataset[\"test\"], tokenizer, batch_size=2, shuffle=False)\n",
    "\n",
    "    # Perform Inference\n",
    "    inference_results = perform_inference(peft_model, dataloader, device)\n",
    "\n",
    "    # Save Inference Results\n",
    "    torch.save(inference_results, paths.RESULTS_PATH/\"ms-diag\"/peft_model_name)\n",
    "\n",
    "return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
