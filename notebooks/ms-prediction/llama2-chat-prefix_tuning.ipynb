{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, BitsAndBytesConfig, TrainingArguments, Trainer, get_linear_schedule_with_warmup, pipeline\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "\n",
    "from peft import get_peft_config, get_peft_model, prepare_model_for_kbit_training, PeftConfig, PromptEncoderConfig, LoraConfig, PeftModel, PromptTuningConfig, PromptTuningInit\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "\n",
    "from src import paths\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 10.20 GB\n",
      "   Allocated Memory : 0.00 GB\n",
      "   Reserved Memory : 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        for gpu_id in range(num_gpus):\n",
    "            free_mem, total_mem = torch.cuda.mem_get_info(gpu_id)\n",
    "            gpu_properties = torch.cuda.get_device_properties(gpu_id)\n",
    "            print(f\"GPU {gpu_id}: {gpu_properties.name}\")\n",
    "            print(f\"   Total Memory: {total_mem / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Free Memory: {free_mem / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Allocated Memory : {torch.cuda.memory_allocated(gpu_id) / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Reserved Memory : {torch.cuda.memory_reserved(gpu_id) / (1024 ** 3):.2f} GB\")\n",
    "    else:\n",
    "        print(\"No GPU available.\")\n",
    "\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "def get_artifical_data_for_label(label:str):\n",
    "    label_dict = {\n",
    "        \"rrms\": \"relapsing_remitting_multiple_sclerosis\",\n",
    "        \"ppms\": \"primary_progressive_multiple_sclerosis\",\n",
    "        \"spms\": \"secondary_progressive_multiple_sclerosis\"\n",
    "    }\n",
    "    generated_data = pd.read_csv(paths.DATA_PATH_PREPROCESSED/f'ms-diag/artificial_{label}.csv')\n",
    "    generated_data[\"labels\"] = label_dict[label]\n",
    "    generated_data = generated_data[[\"0\", \"labels\"]].rename(columns = {\"0\":\"text\"})\n",
    "\n",
    "    return generated_data\n",
    "\n",
    "def get_artifical_data_all():\n",
    "    artifical_data = []\n",
    "    for label in [\"rrms\", \"ppms\", \"spms\"]:\n",
    "        try: \n",
    "            artifical_data.append(get_artifical_data_for_label(label))\n",
    "        except:\n",
    "            print(f\"Could not find data for {label}\")\n",
    "    artifical_data = pd.concat(artifical_data)\n",
    "    artifical_data = Dataset.from_pandas(artifical_data).remove_columns('__index_level_0__')\n",
    "    return artifical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_files = {\"train\": \"ms-diag_clean_train.csv\", \"validation\": \"ms-diag_clean_val.csv\", \"test\": \"ms-diag_clean_test.csv\", \"augmented\": \"ms-diag_augmented.csv\"}\n",
    "df = load_dataset(os.path.join(paths.DATA_PATH_PREPROCESSED,'ms-diag'), data_files = data_files)\n",
    "\n",
    "# Samples Generated from medBERT masked\n",
    "df[\"train\"] = concatenate_datasets([df[\"augmented\"], df[\"train\"]])\n",
    "\n",
    "# Samples Generated from Zero Shot\n",
    "# df[\"train\"] = concatenate_datasets([get_artifical_data_all(), df[\"train\"]])\n",
    "\n",
    "# # Oversampling\n",
    "# minority_classes = df['train'].filter(lambda example: example['labels'] != 'relapsing_remitting_multiple_sclerosis')\n",
    "# df[\"train\"] = concatenate_datasets(9*[minority_classes] + [df['train']])\n",
    "\n",
    "\n",
    "# Number of labels\n",
    "num_labels = len(set(df['train']['labels']))\n",
    "\n",
    "# Label to id\n",
    "label2id = {'primary_progressive_multiple_sclerosis': 0,\n",
    "            'relapsing_remitting_multiple_sclerosis': 1,\n",
    "            'secondary_progressive_multiple_sclerosis': 2}\n",
    "id2label = {v:k for k,v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before Model is loaded:\n",
      "\n",
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 10.20 GB\n",
      "   Allocated Memory : 0.00 GB\n",
      "   Reserved Memory : 0.00 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383f052467054430a8a91e716dc4890c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /cluster/dataset/midatams/inf-extr/resources/models/llama2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory after Model is loaded:\n",
      "\n",
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 6.55 GB\n",
      "   Allocated Memory : 3.45 GB\n",
      "   Reserved Memory : 3.64 GB\n"
     ]
    }
   ],
   "source": [
    "# Low precision config\n",
    "print(\"Memory before Model is loaded:\\n\")\n",
    "check_gpu_memory()\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(paths.MODEL_PATH/'llama2', \n",
    "                                                           device_map=\"auto\", \n",
    "                                                           quantization_config=bnb_config,\n",
    "                                                          num_labels = num_labels)\n",
    "print(\"Memory after Model is loaded:\\n\")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size without Pad Token:  32000\n",
      "Tokenizer pad token ID: 32000\n",
      "Model pad token ID: 32000\n",
      "Model config pad token ID: 32000\n",
      "Vocabulary Size with Pad Token:  32001\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(paths.MODEL_PATH/'llama2', padding_side='left')\n",
    "print(\"Vocabulary Size without Pad Token: \", len(tokenizer))\n",
    "\n",
    "# Check if the pad token is already in the tokenizer vocabulary\n",
    "if '<pad>' not in tokenizer.get_vocab():\n",
    "    # Add the pad token\n",
    "    tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "\n",
    "#Resize the embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#Configure the pad token in the model\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Check if they are equal\n",
    "assert model.config.pad_token_id == tokenizer.pad_token_id, \"The model's pad token ID does not match the tokenizer's pad token ID!\"\n",
    "\n",
    "# Print the pad token ids\n",
    "print('Tokenizer pad token ID:', tokenizer.pad_token_id)\n",
    "print('Model pad token ID:', model.config.pad_token_id)\n",
    "print('Model config pad token ID:', model.config.pad_token_id)\n",
    "print(\"Vocabulary Size with Pad Token: \", len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft_config = PromptEncoderConfig(task_type=\"SEQ_CLS\", num_virtual_tokens=20, encoder_hidden_size=128, encoder_dropout=0.1)\n",
    "# model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# print(peft_config)\n",
    "# peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# peft_config = PromptTuningConfig(\n",
    "#     task_type=\"SEQ_CLS\",\n",
    "#     prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "#     num_virtual_tokens=20,\n",
    "#     prompt_tuning_init_text=\"Klassifiziere als primär, sekundär oder schubförmige MS\",\n",
    "#     tokenizer_name_or_path=paths.MODEL_PATH/'llama2-chat',\n",
    "# )\n",
    "# peft_model = get_peft_model(model, peft_config)\n",
    "# check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 6.05 GB\n",
      "   Allocated Memory : 3.95 GB\n",
      "   Reserved Memory : 4.14 GB\n"
     ]
    }
   ],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.to(device)\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For quantized training need to prepare model\n",
    "# model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# config = {\n",
    "#     \"peft_type\": \"PREFIX_TUNING\",\n",
    "#     \"task_type\": \"SEQ_CLS\",\n",
    "#     \"inference_mode\": False,\n",
    "#     \"num_virtual_tokens\": 0,\n",
    "#     \"token_dim\": model.config.hidden_size,\n",
    "#     \"num_transformer_submodules\": 1,\n",
    "#     \"num_attention_heads\": model.config.num_attention_heads,\n",
    "#     \"num_layers\": model.config.num_hidden_layers,\n",
    "#     \"encoder_hidden_size\": 128,\n",
    "#     \"prefix_projection\": True,\n",
    "# }\n",
    "# peft_config = get_peft_config(config)\n",
    "# print(peft_config)\n",
    "# peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,206,592 || all params: 6,611,566,592 || trainable%: 0.06362473918193577\n",
      "model.layers.0.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.0.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.0.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.0.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.1.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.1.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.1.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.1.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.2.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.2.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.2.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.2.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.3.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.3.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.3.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.3.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.4.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.4.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.4.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.4.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.5.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.5.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.5.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.5.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.6.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.6.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.6.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.6.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.7.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.7.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.7.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.7.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.8.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.8.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.8.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.8.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.9.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.9.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.9.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.9.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.10.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.10.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.10.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.10.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.11.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.11.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.11.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.11.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.12.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.12.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.12.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.12.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.13.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.13.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.13.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.13.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.14.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.14.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.14.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.14.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.15.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.15.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.15.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.15.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.16.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.16.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.16.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.16.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.17.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.17.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.17.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.17.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.18.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.18.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.18.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.18.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.19.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.19.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.19.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.19.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.20.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.20.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.20.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.20.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.21.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.21.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.21.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.21.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.22.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.22.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.22.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.22.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.23.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.23.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.23.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.23.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.24.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.24.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.24.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.24.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.25.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.25.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.25.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.25.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.26.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.26.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.26.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.26.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.27.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.27.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.27.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.27.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.28.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.28.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.28.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.28.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.29.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.29.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.29.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.29.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.30.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.30.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.30.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.30.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.31.self_attn.q_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.31.self_attn.q_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "model.layers.31.self_attn.v_proj.lora_A.default.weight Shape: torch.Size([8, 4096])\n",
      "model.layers.31.self_attn.v_proj.lora_B.default.weight Shape: torch.Size([4096, 8])\n",
      "score.modules_to_save.default.weight Shape: torch.Size([3, 4096])\n"
     ]
    }
   ],
   "source": [
    "# Trainable parameters\n",
    "peft_model.print_trainable_parameters()\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, \"Shape:\",param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6b51b9870542fa9ab18854d1378511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/59 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Prepare dataset\n",
    "# if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "#     tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # We need space for the prefixes, so if the sequence is longer/equal than the max model length we need to truncate to tokenizer.model_max_length - peft_config.num_virtual_tokens\n",
    "    outputs = tokenizer(examples[\"text\"], truncation=True, max_length=256)\n",
    "    outputs[\"labels\"] = [label2id[label] for label in examples[\"labels\"]]\n",
    "    return outputs\n",
    "\n",
    "encoded_dataset = df.map(tokenize_function, batched=True, remove_columns=[\"text\", \"rid\", \"date\"])\n",
    "# # Small train dataset with balanced classes\n",
    "# rmms_id = label2id['relapsing_remitting_multiple_sclerosis']\n",
    "# small_train_dataset = concatenate_datasets([encoded_dataset['train'].filter(lambda example: example['labels'] != rmms_id), encoded_dataset['train'].filter(lambda example: example['labels'] == rmms_id).shuffle(seed=42).select(range(10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "train_batch_size = 8\n",
    "eval_batch_size = 8\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 4\n",
    "gradient_accumulation_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Small train dataset with balanced classes\n",
    "# rmms_id = label2id['relapsing_remitting_multiple_sclerosis']\n",
    "# small_train_dataset = concatenate_datasets([encoded_dataset['train'].filter(lambda example: example['labels'] != rmms_id), encoded_dataset['train'].filter(lambda example: example['labels'] == rmms_id).shuffle(seed=42).select(range(10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding=\"longest\")\n",
    "\n",
    "# DataLoaders creation\n",
    "train_dataloader = DataLoader(\n",
    "    encoded_dataset[\"train\"], shuffle=True, collate_fn=data_collator, batch_size=train_batch_size,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    encoded_dataset[\"validation\"], shuffle=False, collate_fn=data_collator, batch_size=eval_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(peft_model.parameters(), lr=learning_rate)\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")\n",
    "\n",
    "# # Loss\n",
    "# # Class weights\n",
    "# power = 1\n",
    "# class_weights = [1/np.log(len(encoded_dataset['train'].filter(lambda example: example['labels'] == label))) for label in set(encoded_dataset['train']['labels'])]\n",
    "# class_weights = [weight**power for weight in class_weights]\n",
    "# class_weights = torch.tensor(class_weights, dtype=model.dtype).detach().to(device)\n",
    "\n",
    "\n",
    "# loss_fun = torch.nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/43 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Loss: 0.2727: 100%|██████████| 43/43 [05:18<00:00,  7.41s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: train_epoch_loss=tensor(1.2179, device='cuda:0') eval_epoch_loss=tensor(0.2788, device='cuda:0') f1=0.30769230769230765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/43 [00:00<?, ?it/s]/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Loss: 0.0006: 100%|██████████| 43/43 [05:12<00:00,  7.26s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n",
      "epoch=1: train_epoch_loss=tensor(0.0401, device='cuda:0') eval_epoch_loss=tensor(0.1872, device='cuda:0') f1=0.6533333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/43 [00:00<?, ?it/s]/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Loss: 0.0029: 100%|██████████| 43/43 [05:12<00:00,  7.26s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2: train_epoch_loss=tensor(0.0174, device='cuda:0') eval_epoch_loss=tensor(0.1650, device='cuda:0') f1=0.6533333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/43 [00:00<?, ?it/s]/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Loss: 0.0002: 100%|██████████| 43/43 [05:12<00:00,  7.26s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3: train_epoch_loss=tensor(0.0016, device='cuda:0') eval_epoch_loss=tensor(0.0908, device='cuda:0') f1=0.6533333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/43 [00:00<?, ?it/s]/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Loss: 0.0001: 100%|██████████| 43/43 [05:12<00:00,  7.26s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4: train_epoch_loss=tensor(6.8905e-05, device='cuda:0') eval_epoch_loss=tensor(0.0940, device='cuda:0') f1=0.6533333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/43 [00:00<?, ?it/s]/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Loss: 0.0000:  14%|█▍        | 6/43 [00:50<05:09,  8.37s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m gradient_accumulation_steps\n\u001b[0;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m gradient_accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     23\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Seed\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Training\n",
    "peft_model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    peft_model.train()\n",
    "    total_loss = 0\n",
    "    bar = tqdm(train_dataloader)\n",
    "    for step, batch in enumerate(bar):\n",
    "        optimizer.zero_grad()\n",
    "        batch.to(device)\n",
    "        outputs = peft_model(**batch)\n",
    "        logits = outputs.logits\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "        bar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    peft_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        eval_loss = 0\n",
    "        eval_preds = []\n",
    "        labels = []\n",
    "        for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "            batch.to(device)\n",
    "            outputs = peft_model(**batch)\n",
    "                \n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            eval_preds.extend(predictions.tolist())\n",
    "            labels.extend(batch['labels'].tolist())\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.detach().float()\n",
    "            \n",
    "    f1 = f1_score(labels, eval_preds, average='macro')\n",
    "\n",
    "    if epoch == 0:\n",
    "        max_f1 = 0\n",
    "        peft_model.save_pretrained(paths.MODEL_PATH/'peft-lora_llama2_msdiag')\n",
    "    elif f1 > max_f1:\n",
    "        max_f1 = f1\n",
    "        print(\"Saved\")\n",
    "        peft_model.save_pretrained(paths.MODEL_PATH/'peft-lora_llama2_msdiag')\n",
    "\n",
    "    eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"{epoch=}: {train_epoch_loss=} {eval_epoch_loss=} {f1=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.save_pretrained(paths.MODEL_PATH/'msdiag_llama2_peft_lora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:23<00:00,  2.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# Inference on test set\n",
    "peft_model.eval()\n",
    "test_dataloader = DataLoader(\n",
    "    encoded_dataset[\"test\"], shuffle=False, collate_fn=data_collator, batch_size=1\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "        test_preds = []\n",
    "        labels = []\n",
    "        for step, batch in enumerate(tqdm(test_dataloader)):\n",
    "            batch.to(device)\n",
    "            outputs = peft_model(**batch)\n",
    "                \n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            test_preds.extend(predictions.tolist())\n",
    "            \n",
    "            labels.extend(batch['labels'].tolist())            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9491525423728814\n",
      "Precision: 0.9531819635433323\n",
      "Recall: 0.9491525423728814\n",
      "F1: 0.9418617164379876\n"
     ]
    }
   ],
   "source": [
    "# Sklearn F1, precision, recall, accuracy\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, ConfusionMatrixDisplay\n",
    "print(f\"Accuracy: {accuracy_score(encoded_dataset['test']['labels'], test_preds)}\")\n",
    "print(f\"Precision: {precision_score(encoded_dataset['test']['labels'], test_preds, average='weighted')}\")\n",
    "print(f\"Recall: {recall_score(encoded_dataset['test']['labels'], test_preds, average='weighted')}\")\n",
    "print(f\"F1: {f1_score(encoded_dataset['test']['labels'], test_preds, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1472fc317d90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwsAAALECAYAAABHb6xvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACs00lEQVR4nOzdd1gUV9sG8HvovSogiBRBEcWCHaNi7BpLNBoVu8ZeEIXECjYwJCqWqFGjoomaxBZjwS72iij2hogKwY4KIuzu9wcf+7qyKLgLw8L9u6654s7Mzjyzu8Z55jznHEEmk8lARERERET0AS2xAyAiIiIiouKJyQIRERERESnFZIGIiIiIiJRiskBEREREREoxWSAiIiIiIqWYLBARERERkVJMFoiIiIiISCkdsQMgIiqupFIpHj16BFNTUwiCIHY4RERUQDKZDK9evYK9vT20tArnGfnbt2/x7t07tRxLT08PBgYGajmWujBZICLKw6NHj+Do6Ch2GEREpKLExESUL19e7cd9+/YtXJxMkJwiUcvx7OzsEB8fX6wSBiYLRER5MDU1BQD4lhsIHS09kaOhwpb1MEnsEIhIzbKQiWPYJf//ubq9e/cOySkSJJx3hpmpai0Xqa+kcKp9D+/evWOyQESkCXJKj3S09KCjpS9yNFToBF2xIyAidZNl/6ewS0lNTAWYmKp2DimKZ7krkwUiIiIiIhVIZFJIZKofozjiaEhERERERBomJCQEgiAoLHZ2dvLtMpkMISEhsLe3h6GhIXx9fXHlypUCn4fJAhERERGRCqSQqWUpqKpVqyIpKUm+xMXFybeFh4dj3rx5WLx4Mc6ePQs7Ozu0bNkSr169KtA5WIZERERERKQCKaRQtYjoc46go6Oj0JqQQyaTISIiApMnT0aXLl0AAJGRkbC1tcX69esxdOjQfJ+DLQtERERERMVEamqqwpKRkZHnvrdu3YK9vT1cXFzQo0cP3L17FwAQHx+P5ORktGrVSr6vvr4+mjZtihMnThQoHiYLREREREQqkMhkalkAwNHREebm5vIlLCxM6Tnr16+PtWvXYs+ePVixYgWSk5Ph4+ODp0+fIjk5GQBga2ur8B5bW1v5tvxiGRIRERERkQo+t8/Bh8cAsieQMzMzk6/X11c+dHfbtm3lf/by8kLDhg1RsWJFREZGokGDBgByDxkrk8kKPIwsWxaIiIiIiIoJMzMzhSWvZOFDxsbG8PLywq1bt+T9GD5sRUhJScnV2vApTBaIiIiIiFQghQwSFRdVWyYyMjJw7do1lCtXDi4uLrCzs8O+ffvk29+9e4fo6Gj4+PgU6LgsQyIiIiIiUoE6y5Dya8KECejQoQMqVKiAlJQUzJo1C6mpqejXrx8EQYC/vz9CQ0Ph7u4Od3d3hIaGwsjICL169SrQeZgsEBERERGp4P0OyqocoyAePHiAnj174smTJyhbtiwaNGiAU6dOwcnJCQAQFBSE9PR0jBgxAs+fP0f9+vWxd+9emJqaFug8TBaIiIiIiDTMxo0bP7pdEASEhIQgJCREpfMwWSAiIiIiUoH0/xdVj1EcMVkgIiIiIlJBTidlVY9RHHE0JCIiIiIiUootC0REREREKpDIshdVj1EcMVkgIiIiIlJBSe6zwDIkIiIiIiJSii0LREREREQqkEKABILKxyiOmCwQEREREalAKsteVD1GccQyJCIiIiIiUootC0REREREKpCooQxJ1fcXFiYLREREREQqYLJARERERERKSWUCpDIVOzir+P7Cwj4LRERERESkFFsWiIiIiIhUwDIkIiIiIiJSSgItSFQs2JGoKRZ1YxkSEREREREpxZYFIiIiIiIVyNTQwVlWTDs4M1kgIiIiIlJBSe6zwDIkIiIiIiJSii0LREREREQqkMi0IJGp2MFZpqZg1IzJAhERERGRCqQQIFWxYEeK4pktsAyJiIiIiIiUYssCEREREZEKSnIHZyYLREREREQqUE+fheJZhsRkgYiIiIhIBdl9FlRrGVD1/YWFfRaIiIiIiEgptiwQEREREalACi1ISuhoSEwWiIiIiIhUUJL7LLAMiYiIiIiIlGLLAhERERGRCqTQKrGTsjFZICIiIiJSgUQmQCJTcZ4FFd9fWFiGRERERERESrFlgYiIiIhIBRI1jIYkYRkSEREREVHJI5VpQariaEhSjoZERERERESahC0LREREREQqYBkSEREREREpJYXqoxlJ1ROK2jFZICIiIiJSgXrmWSievQOKZ1RERERERCQ6tiwQEREREalAItOCRMXRkFR9f2FhskBEREREpAIpBEihap8FzuBMREREREQapFQmC/fu3YMgCIiNjRU7FPqEkvJdHT58GIIg4MWLFx/dz9nZGREREfk+bnH6fARBwLZt20SNISQkBDVr1hQ1hpKiW7/b2HlmF74bd1XsUKiQfNXvCSJPXcO/dy9hcdRNVKv3WuyQqJDwuy58OWVIqi7FUfGMqpA5OjoiKSkJ1apVEzsU+oSS+l2tWbMGFhYWudafPXsWQ4YMKfqASogJEybgwIEDYoeh8dyrvECbrxNx95ap2KFQIWna8TmGTX+EDQttMKJVJVw+bYxZf8SjrMM7sUMjNeN3XTRy5llQdSmOimdUhejdu3fQ1taGnZ0ddHSKvstGZmZmkZ9TJpMhKyurUI4tkUgglRbeyMBifldiKFu2LIyMjMQOQxTv3qn+D5eJiQmsra3VEE3pZWCYhcCZsVg02wuvU3XFDocKSZchT7BngxWi1lsj8bYBlgU74PEjXXzV96nYoZGa8bsmVWl8suDr64tRo0Zh1KhRsLCwgLW1NaZMmQKZLHsWPGdnZ8yaNQv9+/eHubk5vvvuu1ylGzklInv27EGtWrVgaGiIL7/8EikpKdi9ezeqVKkCMzMz9OzZE2lpafJzR0VF4YsvvpCf96uvvsKdO3fk23PO89dff8HX1xcGBgZYvnw5zMzMsGnTJoXr+Pfff2FsbIxXr1599Hpzjrlx40b4+PjAwMAAVatWxeHDh+X7vH89derUgb6+Po4ePYqMjAyMGTMGNjY2MDAwwBdffIGzZ88qHH/79u1wd3eHoaEhmjVrhsjISIXymZwn4jt27ICnpyf09fWRkJCAd+/eISgoCA4ODjA2Nkb9+vUVYkpISECHDh1gaWkJY2NjVK1aFbt27QIAPH/+HH5+fihbtiwMDQ3h7u6O1atXK1xvbGwspFIpypcvj2XLlinEHBMTA0EQcPfuXQDAy5cvMWTIENjY2MDMzAxffvklLl68+NHPNUdOGcuqVatQoUIFmJiYYPjw4ZBIJAgPD4ednR1sbGwwe/bsXN/J+6VAL168gCAICp/B+9/PgAED8PLlSwiCAEEQEBISAiB3GZIgCFi6dCnatm0LQ0NDuLi44O+///7oNVy9ehXt2rWDiYkJbG1t0adPHzx58iRf179p0yZ4eXnB0NAQ1tbWaNGiBd68eSPfvmrVKlStWhX6+vooV64cRo0aleexHj58iG+//RaWlpawtrZGp06dcO/ePfn2/v37o3PnzggLC4O9vT0qVaoEAIiLi8OXX34pj2HIkCF4/fp/TeaHDx9GvXr1YGxsDAsLCzRq1AgJCQkAcpchfWxfUm540BWcPW6D2LNlxA6FComOrhTu1dNwPlqx5eh8tCk867zJ412kifhdFx2pTFDLUhxpfLIAAJGRkdDR0cHp06excOFCzJ8/HytXrpRv/+mnn1CtWjWcP38eU6dOzfM4ISEhWLx4MU6cOIHExER0794dERERWL9+PXbu3Il9+/Zh0aJF8v3fvHmDgIAAnD17FgcOHICWlha+/vrrXE/av//+e4wZMwbXrl3D119/jR49eshvhnOsXr0a33zzDUxN89fsHxgYiPHjx+PChQvw8fFBx44d8fSp4lOCoKAghIWF4dq1a6hevTqCgoKwefNmREZGIiYmBm5ubmjdujWePXsGIPum95tvvkHnzp0RGxuLoUOHYvLkybnOnZaWhrCwMKxcuRJXrlyBjY0NBgwYgOPHj2Pjxo24dOkSunXrhjZt2uDWrVsAgJEjRyIjIwNHjhxBXFwcfvzxR5iYmAAApk6diqtXr2L37t24du0ali5dijJlct+oaGlpoUePHvjjjz8U1q9fvx4NGzaEq6srZDIZ2rdvj+TkZOzatQvnz5+Ht7c3mjdvLr/OT7lz5w52796NqKgobNiwAatWrUL79u3x4MEDREdH48cff8SUKVNw6tSpfB3vQz4+PoiIiICZmRmSkpKQlJSECRMm5Ln/1KlT0bVrV1y8eBG9e/dGz549ce3aNaX7JiUloWnTpqhZsybOnTuHqKgo/Pfff+jevfsn40pKSkLPnj0xcOBAXLt2DYcPH0aXLl3kiffSpUsxcuRIDBkyBHFxcdi+fTvc3NyUHistLQ3NmjWDiYkJjhw5gmPHjsHExARt2rRRaEE4cOAArl27hn379mHHjh1IS0tDmzZtYGlpibNnz+Lvv//G/v375UlJVlYWOnfujKZNm+LSpUs4efIkhgwZAkHI/T/YguybIyMjA6mpqQpLadKk5SO4VX6JNb9UFjsUKkRmVhJo6wAvnii22L54rANLm8JphSZx8LsuOlI1lCAV10nZSkRth6OjI+bPnw9BEFC5cmXExcVh/vz5+O677wAAX375pcLN2PtPN983a9YsNGrUCAAwaNAgTJw4EXfu3IGrqysA4JtvvsGhQ4fw/fffAwC6du2q8P7ffvsNNjY2uHr1qkKNvb+/P7p06SJ/PXjwYPj4+ODRo0ewt7fHkydPsGPHDuzbty/f1zxq1Cj5+ZcuXYqoqCj89ttvCAoKku8zY8YMtGzZEkB2YrN06VKsWbMGbdu2BQCsWLEC+/btw2+//YbAwEAsW7YMlStXxk8//QQAqFy5Mi5fvqzwFB3ILqVasmQJatSoASD75nrDhg148OAB7O3tAWTXjkdFRWH16tUIDQ3F/fv30bVrV3h5eQGA/DMFgPv376NWrVqoU6cOgOyn63nx8/PDvHnzkJCQACcnJ0ilUmzcuBGTJk0CABw6dAhxcXFISUmBvr4+AODnn3/Gtm3bsGnTpnz1B5BKpVi1ahVMTU3h6emJZs2a4caNG9i1axe0tLRQuXJl/Pjjjzh8+DAaNGjwyeN9SE9PD+bm5hAEAXZ2dp/cv1u3bhg8eDAAYObMmfKkdcmSJbn2Xbp0Kby9vREaGipft2rVKjg6OuLmzZvyp/fKJCUlISsrC126dIGTkxMAyL8vIPvvx/jx4zF27Fj5urp16yo91saNG6GlpYWVK1fKb85Xr14NCwsLHD58GK1atQIAGBsbY+XKldDT0wOQ/ZtMT0/H2rVrYWxsDABYvHgxOnTogB9//BG6urp4+fIlvvrqK1SsWBEAUKVKFaUxpKam5nvfHGFhYZg+ffpH9ympytikY0jAVUwdUw+Z77TFDoeKwP8/B5ATBAAypbuShuN3TaoonilMATVo0EDhaWHDhg1x69YtSCQSAJDfhH5K9erV5X+2tbWFkZGRwk2tra0tUlJS5K/v3LmDXr16wdXVFWZmZnBxcQGQffP7vg/PX69ePVStWhVr164FAKxbtw4VKlRAkyZN8hVnzjXm0NHRQZ06dXI9bX7/vHfu3EFmZqY8GQIAXV1d1KtXT/6+Gzdu5Lr5q1evXq5z6+npKXxWMTExkMlkqFSpEkxMTORLdHS0vCxrzJgx8mQsODgYly5dkr9/+PDh2LhxI2rWrImgoCCcOHEiz+uuVasWPDw8sGHDBgBAdHQ0UlJS5E/Oz58/j9evX8Pa2lohlvj4eIUSsY9xdnZWaOGxtbWFp6cntLS0FNa9/1soTO9/1zmv82pZOH/+PA4dOqRw7R4eHgDwyeuvUaMGmjdvDi8vL3Tr1g0rVqzA8+fPAQApKSl49OgRmjdvnq+Yz58/j9u3b8PU1FQeh5WVFd6+fasQh5eXlzxRAIBr166hRo0a8kQBABo1agSpVIobN27AysoK/fv3R+vWrdGhQwcsWLAASUlJSmMoyL45Jk6ciJcvX8qXxMTEfF1vSeBW5SUsrd9hQeRxbD+xG9tP7Eb12s/Q8dt72H5iN7S0eGdRUqQ+04YkC7Asq/hk2bxMFp4/LhHPEOn/8bsuOlKZllqW4qh4RqVm7994fIyu7v868wmCoPA6Z937JUYdOnTA06dPsWLFCpw+fRqnT58GkLujprLzDx48WF6KtHr1agwYMOCj5RH58eH73z9vTinJh/vIZDL5uvf//OH73mdoaKiwn1Qqhba2Ns6fP4/Y2Fj5cu3aNSxYsABA9vXevXsXffr0QVxcHOrUqSMv6Wrbti0SEhLg7+8vvyH9WFmOn58f1q9fDyC7BKl169bysiWpVIpy5copxBEbG4sbN24gMDDwI5/e/yj73j/2W8hJIt7/rAq7I3tevxWpVIoOHTrkuv5bt259MhnV1tbGvn37sHv3bnh6emLRokWoXLky4uPjYWhoWKD4pFIpateunSuOmzdvolevXvL9Pvy7oew3+OE1r169GidPnoSPjw/+/PNPVKpUKc+SsILsCwD6+vowMzNTWEqLi2fLYESPxhjd+wv5cvOqOQ5H2WN07y8glRbPWloquKxMLdy6ZATvJop95LybvMLVc/n795I0A7/roiOBoJalOCoRycKH//ifOnUK7u7u0NYuvKb0p0+f4tq1a5gyZQqaN2+OKlWqyJ/C5kfv3r1x//59LFy4EFeuXEG/fv0KdP73rzkrKwvnz5+XP0FWxs3NDXp6ejh27Jh8XWZmJs6dOycvzfDw8MjV4fncuXOfjKVWrVqQSCRISUmBm5ubwvJ+mY2joyOGDRuGLVu2YPz48VixYoV8W9myZdG/f3/8/vvviIiIwPLly/M8X69evRAXF4fz589j06ZN8PPzk2/z9vZGcnIydHR0csWirB+EOpQtWxYAFJ5af2reAz09PXnL16co+33n9V17e3vjypUrcHZ2znX9+UmaBUFAo0aNMH36dFy4cAF6enrYunUrTE1N4ezsnO9hSb29vXHr1i3Y2NjkisPc3DzP93l6eiI2NlahU/Xx48ehpaWlUEJVq1YtTJw4ESdOnEC1atXkyaMyBdm3NEtP00HCXVOF5W26NlJf6iHhLodQLWm2LC+DNr2eoVWPp3B0e4uhIQ9h45CJnWs5mlhJw++6aLBloZhLTExEQEAAbty4gQ0bNmDRokUKddWFIWeEl+XLl+P27ds4ePAgAgICCvT+Ll26IDAwEK1atUL58uULdP5ffvkFW7duxfXr1zFy5Eg8f/4cAwcOzHN/Y2NjDB8+HIGBgYiKisLVq1fx3XffIS0tDYMGDQIADB06FNevX8f333+Pmzdv4q+//sKaNWsA5P0kGwAqVaoEPz8/9O3bF1u2bEF8fDzOnj2LH3/8UT7ikb+/P/bs2YP4+HjExMTg4MGD8iRl2rRp+Oeff3D79m1cuXIFO3bs+GhtuYuLC3x8fDBo0CBkZWWhU6dO8m0tWrRAw4YN0blzZ+zZswf37t3DiRMnMGXKlHwlPp/D0NAQDRo0wJw5c3D16lUcOXIEU6ZM+eh7nJ2d8fr1axw4cABPnjxRGGXrQ3///TdWrVqFmzdvIjg4GGfOnMlzFKKRI0fi2bNn6NmzJ86cOYO7d+9i7969GDhw4CeTk9OnTyM0NBTnzp3D/fv3sWXLFjx+/Fj+XYSEhGDu3LlYuHAhbt26hZiYGIUO/+/z8/NDmTJl0KlTJxw9ehTx8fGIjo7G2LFj8eDBgzxj8PPzg4GBAfr164fLly/j0KFDGD16NPr06QNbW1vEx8dj4sSJOHnyJBISErB3717cvHlT6e+lIPsSlTbR2y2xLNgefuP+w5J9N+HV4A2m9HZBykO9T7+ZNAq/a1JViShY69u3L9LT01GvXj1oa2tj9OjRhT6xlZaWFjZu3IgxY8agWrVqqFy5MhYuXAhfX998H2PQoEFYv379R2/y8zJnzhz8+OOPuHDhAipWrIh//vnnk0/O58yZA6lUij59+uDVq1eoU6cO9uzZA0tLSwDZN+GbNm3C+PHjsWDBAjRs2BCTJ0/G8OHD5Z2F87J69Wp5B9iHDx/C2toaDRs2RLt27QBkz8cwcuRIPHjwAGZmZmjTpg3mz58PIPsp+8SJE3Hv3j0YGhqicePG2Lhx40fP5+fnh5EjR6Jv374KJTKCIGDXrl2YPHkyBg4ciMePH8POzg5NmjSBra3tJz/Xz7Vq1SoMHDgQderUQeXKlREeHi7vxKuMj48Phg0bhm+//RZPnz5FcHCwfPjUD02fPh0bN27EiBEjYGdnhz/++AOenp5K97W3t8fx48fx/fffo3Xr1sjIyICTkxPatGmj0OdCGTMzMxw5cgQRERFITU2Fk5MT5s6dK+8Q369fP7x9+xbz58/HhAkTUKZMGXzzzTdKj2VkZIQjR47g+++/R5cuXfDq1Ss4ODigefPmHy3tMTIywp49ezB27FjUrVsXRkZG6Nq1K+bNmyfffv36dURGRuLp06fy4VuHDh2q9Fj53ZeUmzi84B34SXPsiCyDHZEcIrc04Hdd+CSAymVE+as3KHqCTFlRugbx9fVFzZo1Fcam1xR//PEHxo4di0ePHil08vyYe/fuwcXFBRcuXFAYT76wzJ49G8uWLStVHT2LE0EQsHXrVnTu3FnsUEql1NRUmJubo4XDMOhofTxhJs2X9eCh2CEQkZplyTJxGP/g5cuXhdIPLeffiSmnWsHARLWJLN++zsSsBnsLLdbPVSJaFjRNWloa4uPjERYWhqFDh+Y7USgKS5YsQd26dWFtbY3jx4/jp59++ujEW0RERERUcpWIPguaJjw8HDVr1oStrS0mTpyosC00NFRh2Mv3l5xykMJ069YtdOrUCZ6enpg5cybGjx+fZ3mMJqpatWqen++Hk72VNPfv38/z2k1MTHIN+UtERET5I5FpqWUpjjS+DKmkefbsWZ4zDRsaGsLBwaGIIypZEhIS8hzW1NbWNt8zaGuirKysPCckBLI7XevosLHxfSxDKl1YhkRU8hRVGdIPJ9tCX8UypIzXmZjTcDfLkOjjrKysYGVlJXYYJVbOzMSlUc5wskRERET5xWSBiIiIiEgF6igjKq5lSEwWiIiIiIhUIJUJkMpUGzpV1fcXluKZwhARERERkeiYLBARERERqUACLbUsnyssLAyCIMDf31++TiaTISQkBPb29jA0NISvry+uXLlS4GMzWSAiIiIiUkFOGZKqy+c4e/Ysli9fjurVqyusDw8Px7x587B48WKcPXsWdnZ2aNmyJV69elWg4zNZICIiIiJSgRRaalkK6vXr1/Dz88OKFStgaWkpXy+TyRAREYHJkyejS5cuqFatGiIjI5GWlob169cX6BxMFoiIiIiIionU1FSFJSMjI899R44cifbt26NFixYK6+Pj45GcnIxWrVrJ1+nr66Np06Y4ceJEgeJhskBEREREpAKJTFDLAgCOjo4wNzeXL2FhYUrPuXHjRsTExCjdnpycDCB7wtn32drayrflF4dOJSIiIiJSgTqHTk1MTFSYwVlfXz/XvomJiRg7diz27t0LAwODPI8pCIoxyWSyXOs+hckCEREREVExYWZmppAsKHP+/HmkpKSgdu3a8nUSiQRHjhzB4sWLcePGDQDZLQzlypWT75OSkpKrteFTWIZERERERKQCmUwLUhUXWQFmcG7evDni4uIQGxsrX+rUqQM/Pz/ExsbC1dUVdnZ22Ldvn/w97969Q3R0NHx8fAp0bWxZICIiIiJSgQQCJFCtDKkg7zc1NUW1atUU1hkbG8Pa2lq+3t/fH6GhoXB3d4e7uztCQ0NhZGSEXr16FSguJgtERERERCVMUFAQ0tPTMWLECDx//hz169fH3r17YWpqWqDjMFkgIiIiIlKBVAY1dHBWLYbDhw8rvBYEASEhIQgJCVHpuEwWiIiIiIhUkNPvQNVjFEfFMyoiIiIiIhIdWxaIiIiIiFQghQCpih2cVX1/YWGyQERERESkgvdnYFblGMURkwUiIiIiIhWwzwIREREREZU6bFkgIiIiIlKBFILqQ6eyzwIRERERUckjU0MHZ1kxTRZYhkREREREREqxZYGIiIiISAVSmRrKkDgaEhERERFRycPRkIiIiIiIqNRhywIRERERkQpYhkREREREREpJ1TAaUnEdOpVlSEREREREpBRbFoiIiIiIVMAyJCIiIiIiUorJAhERERERKVWSkwX2WSAiIiIiIqXYskBEREREpIKS3LLAZIGIiIiISAUyqD70qUw9oagdy5CIiIiIiEgptiwQEREREamAZUhERERERKRUSU4WWIZERERERERKsWWBiIiIiEgFJbllgckCEREREZEKSnKywDIkIiIiIiJSii0LREREREQqkMkEyFRsGVD1/YWFyQIRERERkQqkEFSelE3V9xcWJgtERERERCpgnwUiIiIiIip12LJARERERKQC9lkgIiIiIiKlWIZERERERESlDlsWiIiIiIhUwDIkIqJSTPLkGQRBV+wwqJDteRQrdghUhFrb1xQ7BCpBZGooQyquyQLLkIiIiIiISCm2LBARERERqUAGQCZT/RjFEZMFIiIiIiIVSCFAKKEzOLMMiYiIiIiIlGLLAhERERGRCjgaEhERERERKSWVCRBK6KRsTBaIiIiIiFQgk6mhg3Mx7eHMPgtERERERKQUWxaIiIiIiFTAPgtERERERKRUSU4WWIZERERERERKsWWBiIiIiEgFHA2JiIiIiIiU4mhIRERERERU6rBlgYiIiIhIBdktC6p2cFZTMGrGZIGIiIiISAUcDYmIiIiIiEodtiwQEREREalA9v+LqscojpgsEBERERGpoCSXITFZICIiIiJSRQluWmCfBSIiIiIiUootC0REREREqlBDGRJYhkREREREVPJwBmciIiIiIip12LJARERERKSCkjwaElsWiIiIiIhUIRPUsxTA0qVLUb16dZiZmcHMzAwNGzbE7t27/xeSTIaQkBDY29vD0NAQvr6+uHLlSoEvjckCEREREZGGKV++PObMmYNz587h3Llz+PLLL9GpUyd5QhAeHo558+Zh8eLFOHv2LOzs7NCyZUu8evWqQOdhskBEREREpIKcDs6qLgXRoUMHtGvXDpUqVUKlSpUwe/ZsmJiY4NSpU5DJZIiIiMDkyZPRpUsXVKtWDZGRkUhLS8P69esLdB4mC0REREREqpCpaQGQmpqqsGRkZHzy9BKJBBs3bsSbN2/QsGFDxMfHIzk5Ga1atZLvo6+vj6ZNm+LEiRMFujQmC0RERERExYSjoyPMzc3lS1hYWJ77xsXFwcTEBPr6+hg2bBi2bt0KT09PJCcnAwBsbW0V9re1tZVvy698jYa0cOHCfB9wzJgxBQqAiIiIiEiTqXM0pMTERJiZmcnX6+vr5/meypUrIzY2Fi9evMDmzZvRr18/REdHy7cLgmJMMpks17pPyVeyMH/+/HwdTBAEJgtEREREVPqoaVK1nNGN8kNPTw9ubm4AgDp16uDs2bNYsGABvv/+ewBAcnIyypUrJ98/JSUlV2vDp+QrWYiPjy/QQYmIiIiISoviMs+CTCZDRkYGXFxcYGdnh3379qFWrVoAgHfv3iE6Oho//vhjgY752ZOyvXv3DvHx8ahYsSJ0dDi3GxERERFRUZk0aRLatm0LR0dHvHr1Chs3bsThw4cRFRUFQRDg7++P0NBQuLu7w93dHaGhoTAyMkKvXr0KdJ4C3+WnpaVh9OjRiIyMBADcvHkTrq6uGDNmDOzt7fHDDz8U9JBERERERJrrvdGMVDpGAfz333/o06cPkpKSYG5ujurVqyMqKgotW7YEAAQFBSE9PR0jRozA8+fPUb9+fezduxempqYFOk+BR0OaOHEiLl68iMOHD8PAwEC+vkWLFvjzzz8LejgiIiIiIg0nqGnJv99++w337t1DRkYGUlJSsH//fnmiAGT3JQ4JCUFSUhLevn2L6OhoVKtWrcBXVuCWhW3btuHPP/9EgwYNFHpTe3p64s6dOwUOgIiIiIiIiqcCJwuPHz+GjY1NrvVv3rwp8FBMREREREQaT4QypKJS4DKkunXrYufOnfLXOQnCihUr0LBhQ/VFRkRERESkCdQ4g3NxU+CWhbCwMLRp0wZXr15FVlYWFixYgCtXruDkyZMKk0AQEREREZFmK3DLgo+PD44fP460tDRUrFgRe/fuha2tLU6ePInatWsXRoxERERERMWXTFDPUgx91gQJXl5e8qFTiYiIiIhKM5kse1H1GMXRZyULEokEW7duxbVr1yAIAqpUqYJOnTpxcjYiIiIiohKkwHf3ly9fRqdOnZCcnIzKlSsDyJ6YrWzZsti+fTu8vLzUHiQRERERUbHF0ZD+Z/DgwahatSoePHiAmJgYxMTEIDExEdWrV8eQIUMKI0YiIiIiouKLfRb+5+LFizh37hwsLS3l6ywtLTF79mzUrVtXrcERERERERV3gix7UfUYxVGBWxYqV66M//77L9f6lJQUuLm5qSUoIiIiIiISX75aFlJTU+V/Dg0NxZgxYxASEoIGDRoAAE6dOoUZM2bgxx9/LJwoiYiIiIiKqxLcZyFfyYKFhYV8pmYAkMlk6N69u3yd7P/HeurQoQMkEkkhhElEREREVEypo8+BJvdZOHToUGHHQURERERExUy+koWmTZsWdhxERERERJqptJchKZOWlob79+/j3bt3CuurV6+uclBERERERBqDycL/PH78GAMGDMDu3buVbmefBSIiIiKikqHAQ6f6+/vj+fPnOHXqFAwNDREVFYXIyEi4u7tj+/bthREjEREREVHxJVPTUgwVuGXh4MGD+Oeff1C3bl1oaWnByckJLVu2hJmZGcLCwtC+ffvCiJOIiIiIqHgqwaMhFbhl4c2bN7CxsQEAWFlZ4fHjxwAALy8vxMTEqDc6IiIiIiISzWfN4Hzjxg0AQM2aNfHrr7/i4cOHWLZsGcqVK6f2AMXg6+sLf3//Ijvf4cOHIQgCXrx4UWTnLEqqXN+aNWtgYWGh9pjE0L9/f3Tu3Pmj+3zOZxUSEoKaNWuqFJs6FJfvytnZGREREWKHoZGq1UtFyMqb+OPUBUTFn0HDls/FDonUYN3PdmhtX1Nh6VGjqnz7sV3mmNTTFd2qVkNr+5q4c9lQxGipMHzV7wkiT13Dv3cvYXHUTVSr91rskEocQaaepTj6rD4LSUlJAIDg4GBERUWhQoUKWLhwIUJDQ9UeYGng4+ODpKQkmJubix1Kofjw+vK6qVR2k/ftt9/i5s2bRRBl0VOWlJb030JROHv2LIYMGSJ2GBrJwFCK+GtGWBLsJHYopGZOldOxIfayfFl28Lp829s0LXjWfYOBkx6JGCEVlqYdn2PY9EfYsNAGI1pVwuXTxpj1RzzKOrz79Jsp/9hn4X/8/Pzkf65Vqxbu3buH69evo0KFCihTpoxagysM7969g56enthhKNDT04OdnZ2oMRTm56LK9RkaGsLQsPQ85SoOvwUxZWZmQldXV6VjlC1bVk3RlD7noi1wLtpC7DCoEGhrA1Y2WUq3tfgmuwUpObF4/dtI6tFlyBPs2WCFqPXWAIBlwQ6o7fsKX/V9itVhJaMihApXgVsWPmRkZARvb+9imyj4+vpi1KhRCAgIQJkyZdCyZUtcvXoV7dq1g4mJCWxtbdGnTx88efIkz2P8/vvvqFOnDkxNTWFnZ4devXohJSVFvj2ndGTnzp2oUaMGDAwMUL9+fcTFxcn3SUhIQIcOHWBpaQljY2NUrVoVu3btUnh/TulJzpP3PXv2oEqVKjAxMUGbNm3kLToAkJWVhTFjxsDCwgLW1tb4/vvv0a9fv0+WuXzscwHwyc/G19cXo0ePhr+/PywtLWFra4vly5fjzZs3GDBgAExNTVGxYkWFoXXfv77Dhw9jwIABePnyJQRBgCAICAkJga+vLxISEjBu3Dj5+vc/ixw5JTfr1q2Ds7MzzM3N0aNHD7x69Uq+z6tXr+Dn5wdjY2OUK1cO8+fPL1BpmbOzM2bNmoW+ffvCxMQETk5O+Oeff/D48WN06tQJJiYm8PLywrlz53LF9b6IiAg4OzsrPUf//v0RHR2NBQsWyK/33r17ef4Wtm3bhkqVKsHAwAAtW7ZEYmLiR69h9erVqFKlCgwMDODh4YElS5bk69rfvXuHUaNGoVy5cjAwMICzszPCwsLk21+8eIEhQ4bA1tYWBgYGqFatGnbs2JHn8f7991/Url0bBgYGcHV1xfTp05GV9b8bFkEQsGzZMnTq1AnGxsaYNWsWAGDp0qWoWLEi9PT0ULlyZaxbt07huCEhIahQoQL09fVhb2+PMWPGyLd92EL1sX2JSouH8XroWasq+tavgtBhTkhKYGJQGujoSuFePQ3no00V1p+PNoVnnTciRUWaJl/JQkBAQL6X4igyMhI6Ojo4fvw45syZg6ZNm6JmzZo4d+4coqKi8N9//6F79+55vv/du3eYOXMmLl68iG3btiE+Ph79+/fPtV9gYCB+/vlnnD17FjY2NujYsSMyMzMBACNHjkRGRgaOHDmCuLg4/PjjjzAxMcnznGlpafj555+xbt06HDlyBPfv38eECRPk23/88Uf88ccfWL16NY4fP47U1FRs27btsz+XX3/9FUlJSfn6bCIjI1GmTBmcOXMGo0ePxvDhw9GtWzf4+PggJiYGrVu3Rp8+fZCWlpbrnD4+PoiIiICZmRmSkpKQlJSECRMmYMuWLShfvjxmzJghX5+XO3fuYNu2bdixYwd27NiB6OhozJkzR749ICAAx48fx/bt27Fv3z4cPXq0wJ3v58+fj0aNGuHChQto3749+vTpg759+6J3796IiYmBm5sb+vbtC5ns89oMFyxYgIYNG+K7776TX6+jo6PSfdPS0jB79mxERkbKv+sePXrkeewVK1Zg8uTJmD17Nq5du4bQ0FBMnToVkZGRn4xr4cKF2L59O/766y/cuHEDv//+uzzhkUqlaNu2LU6cOIHff/8dV69exZw5c6Ctra30WHv27EHv3r0xZswYXL16Fb/++ivWrFmD2bNnK+wXHByMTp06IS4uDgMHDsTWrVsxduxYjB8/HpcvX8bQoUMxYMAAHDp0CACwadMmzJ8/H7/++itu3bqFbdu2wcvLS2kMBdkXADIyMpCamqqwEGk6D+83CFx4H6Hr78D/p0Q8f6yLcR3dkfpM+d9dKjnMrCTQ1gFePFEsJHnxWAeWebQ00ecRoIY+C2JfRB7yVYZ04cKFfB0s52lwcePm5obw8HAAwLRp0+Dt7a3Qv2LVqlVwdHTEzZs3UalSpVzvHzhwoPzPrq6uWLhwIerVq4fXr18r3PAHBwfLn9BHRkaifPny2Lp1K7p374779++ja9eu8hsVV1fXj8acmZmJZcuWoWLFigCAUaNGYcaMGfLtixYtwsSJE/H1118DABYvXixvqficzwXI/2dTo0YNTJkyBQAwceJEzJkzB2XKlMF3330nP87SpUtx6dIlNGjQQOGcenp6MDc3hyAIucpttLW15a03HyOVSrFmzRqYmmY/KenTpw8OHDiA2bNn49WrV4iMjMT69evRvHlzANlP2e3t7Qv02bRr1w5Dhw5VuJ66deuiW7duAIDvv/8eDRs2xH///fdZZUPm5ubQ09ODkZHRJ9+fmZmJxYsXo379+gCyf1tVqlTBmTNnUK9evVz7z5w5E3PnzkWXLl0AAC4uLvKb9X79+n30XPfv34e7uzu++OILCIIAJ6f/1a7v378fZ86cwbVr1+S/hY/9jmfPno0ffvhBfk5XV1fMnDkTQUFBCA4Olu/Xq1cvhb9jvXr1Qv/+/TFixAgA2cnfqVOn8PPPP6NZs2a4f/8+7Ozs0KJFC+jq6qJChQpKP4ec68nvvgAQFhaG6dOnf/QzItI0db/8X8urSxXAs85d9G9YBfv+tkLXoY9FjIyKyofPtQQBxbY+XmOV4KFT85Us5DzR01R16tSR//n8+fM4dOiQ0qf6d+7cUZosXLhwASEhIYiNjcWzZ88glUoBZN+IeHp6yvdr2LCh/M9WVlaoXLkyrl27BgAYM2YMhg8fjr1796JFixbo2rUrqlevnmfMRkZG8kQBAMqVKycvfXr58iX+++8/hZsebW1t1K5dWx5bfrz/uQD5/2zej1tbWxvW1tYKT2ttbW0BQKFUS52cnZ3liQKg+NncvXsXmZmZCp+Nubk5KleuXKBzvH+NOdeT1zUWdh8DHR0dhe/Kw8MDFhYWuHbtWq4b38ePHyMxMRGDBg2SJ29AdtlafjpN9+/fHy1btkTlypXRpk0bfPXVV2jVqhUAIDY2FuXLl1f6d0SZ8+fP4+zZswotCRKJBG/fvkVaWhqMjIwA5P4dXrt2LVcH5UaNGmHBggUAgG7duiEiIgKurq5o06YN2rVrhw4dOkBHJ/f/zgqyL5Cd/L7fQpqamppniw+RpjIwksLZ4y0exuuLHQoVstRn2pBkAZZlFVsRzMtk4fnjAndbpVJK5T4LmsDY2Fj+Z6lUig4dOiA2NlZhuXXrFpo0aZLrvW/evEGrVq1gYmKC33//HWfPnsXWrVsBZJcnfUpOa8vgwYNx9+5d9OnTB3FxcahTpw4WLVqU5/s+7OQpCEKukpcPW3IKWhLz/ucC5P+zURbb++ty4ipI4lIQys6fc66cz0DVz0bZ9XzsGrW0tHKdI6cETR2UtdopW5cTz4oVKxS+w8uXL+PUqVOfPI+3tzfi4+Mxc+ZMpKeno3v37vjmm28AoMAdzaVSKaZPn64QR1xcHG7dugUDAwP5fh/+DpVdm0wmk69zdHTEjRs38Msvv8DQ0BAjRoxAkyZNlH7eBdkXAPT19WFmZqawEJU07zIEJN7Wh5WN+v4fRcVTVqYWbl0ygneTVwrrvZu8wtVzuf/fSyoowaMhlYpk4X3e3t64cuUKnJ2d4ebmprAou2m5fv06njx5gjlz5qBx48bw8PDI84n5+zdjz58/x82bN+Hh4SFf5+joiGHDhmHLli0YP348VqxY8VnXYG5uDltbW5w5c0a+TiKR5LtcLC8F/Ww+l56eHiQSSb7XF0TFihWhq6ur8Nmkpqbi1q1bKh33U8qWLYvk5GSFhCE2Nvaj78nv9WZlZSl0pr5x4wZevHih8NvKYWtrCwcHB9y9ezfXd+ji4pKvazEzM8O3336LFStW4M8//8TmzZvx7NkzVK9eHQ8ePMj3ULbe3t64ceNGrjjc3NygpZX3/3qqVKmCY8eOKaw7ceIEqlSpIn9taGiIjh07YuHChTh8+DBOnjypMKDA+wqyb2lnYCSBa5U3cK2S3fHRzjEDrlXeoKx9hsiRkSqWT7fHpZPGSL6vh+sxRpj1nTPSXmmjZfdnAIDU59q4c9kQ929mtzQk3tHHncuGeJbCJ88lwZblZdCm1zO06vEUjm5vMTTkIWwcMrFzrbXYoZUsJThZKHX/Jxg5ciRWrFiBnj17IjAwEGXKlMHt27exceNGrFixIldnzQoVKkBPTw+LFi3CsGHDcPnyZcycOVPpsWfMmAFra2vY2tpi8uTJKFOmjHx0In9/f7Rt2xaVKlXC8+fPcfDgQYWbn4IaPXo0wsLC4ObmBg8PDyxatAjPnz9Xqd9IQT+bz+Xs7IzXr1/jwIEDqFGjBoyMjGBkZARnZ2ccOXIEPXr0gL6+/meNsGVqaop+/fohMDAQVlZWsLGxQXBwMLS0tAq1T42vry8eP36M8PBwfPPNN4iKisLu3bs/+mTa2dkZp0+fxr1792BiYgIrKyul++nq6mL06NFYuHAhdHV1MWrUKDRo0CDP2vuQkBCMGTMGZmZmaNu2LTIyMnDu3Dk8f/78k4MQzJ8/H+XKlUPNmjWhpaWFv//+G3Z2drCwsEDTpk3RpEkTdO3aFfPmzYObmxuuX78OQRDQpk2bXMeaNm0avvrqKzg6OqJbt27Q0tLCpUuXEBcXJx/1SJnAwEB0794d3t7eaN68Of79919s2bIF+/fvB5A9QpREIkH9+vVhZGSEdevWwdDQUKF/RY6C7EtAJa83CN/4v/H3h069DwDYt6kM5gZ+vJ8VFV9PknQRNsIZqc+0YW6dBQ/vNETsuAnb8tktC6f2mmPuuAry/cOGOwMAegcko8+EZDFCJjWK3m4JU0sJ/Mb9ByubLCTcMMCU3i5IecgRsSh/Sl2yYG9vj+PHj+P7779H69atkZGRAScnJ7Rp00bp086yZctizZo1mDRpEhYuXAhvb2/8/PPP6NixY65958yZg7Fjx+LWrVuoUaMGtm/fLp+7QCKRYOTIkXjw4AHMzMzQpk0bzJ8//7Ov4/vvv0dycjL69u0LbW1tDBkyBK1bt1bphr6gn83n8vHxwbBhw/Dtt9/i6dOnCA4ORkhICGbMmIGhQ4eiYsWKyMjI+OyRhubNm4dhw4bhq6++gpmZGYKCgpCYmKhQ+qJuVapUwZIlSxAaGoqZM2eia9eumDBhApYvX57neyZMmIB+/frB09MT6enpiI+PV7qfkZERvv/+e/Tq1QsPHjzAF198gVWrVuV53MGDB8PIyAg//fQTgoKCYGxsDC8vr3wNHWtiYoIff/wRt27dgra2NurWrYtdu3bJv//NmzdjwoQJ6NmzJ968eQM3NzeFkaje17p1a+zYsQMzZsxAeHg4dHV14eHhgcGDB380hs6dO2PBggX46aefMGbMGLi4uGD16tXw9fUFAFhYWGDOnDkICAiARCKBl5cX/v33X1hb535KVpB9Cbh02gxtXPLuAE6aadKyhI9ub/XtM7T69lkRRUNi2BFZBjsii+cQ9yWFOmZgLq4zOAuyz70jI7nDhw+jWbNmeP78udKZiYuCVCpFlSpV0L179zxbPkqrN2/ewMHBAXPnzsWgQYPEDqdA1qxZA39/f/m8C1S0UlNTYW5ujmb63aEjqDZZHBV/UfGnxQ6BilBr+5pih0BFIEuWicP4By9fviyUfmg5/044z5oNLRUfSkrfvsW9KZMLLdbP9VmPi9etW4dGjRrB3t4eCQnZTywiIiLwzz//qDU4yltCQgJWrFiBmzdvIi4uDsOHD0d8fDx69eoldmiiu3DhAjZs2IA7d+4gJiZGPut4p06dRI6MiIiISLMUOFlYunQpAgIC0K5dO7x48ULeQdPCwkJh1lQqXFpaWlizZg3q1q2LRo0aIS4uDvv370eVKlVw//59mJiY5Lncv39f7PAL3c8//4waNWqgRYsWePPmDY4ePYoyZcrg6NGjH/1sSrrQ0NA8r71t27Zih0dERKSZSnAH5wKXIXl6eiI0NBSdO3eGqakpLl68CFdXV1y+fBm+vr548uRJYcVK+ZSVlYV79+7lud3Z2TnPceZLuvT0dDx8+DDP7W5ubkUYTdF79uwZnj1TXptsaGgIBweHIo6oeGMZUunCMqTShWVIpUNRlSG5zFBPGVL8tOJXhlTgO8b4+HjUqlUr13p9fX28efNGLUGRanR0dEr8Te/nMjQ0LNWfjZWVVZ6jLhEREdFnKsEzOBe4DMnFxUXp+PG7d+9WmM2YiIiIiIg0W4FbFgIDAzFy5Ei8ffsWMpkMZ86cwYYNGxAWFoaVK1cWRoxERERERMWXOvocFNM+CwVOFgYMGICsrCwEBQUhLS0NvXr1goODAxYsWIAePXoURoxERERERMVWSZ5n4bN6uX733Xf47rvv8OTJE0ilUtjY2Kg7LiIiIiIiEplKQ+KUKcPZAImIiIiolGMZ0v+4uLhAEPLurX337l2VAiIiIiIi0ihqKEMqMcmCv7+/wuvMzExcuHABUVFRCAwMVFdcREREREQksgInC2PHjlW6/pdffsG5c+dUDoiIiIiISKOU4DKkAs+zkJe2bdti8+bN6jocEREREZFmkKlpKYbUlixs2rSJM8MSEREREZUgBS5DqlWrlkIHZ5lMhuTkZDx+/BhLlixRa3BERERERMUd51l4T+fOnRVea2lpoWzZsvD19YWHh4e64iIiIiIiIpEVKFnIysqCs7MzWrduDTs7u8KKiYiIiIhIc7CDczYdHR0MHz4cGRkZhRUPEREREREVEwXu4Fy/fn1cuHChMGIhIiIiItI4OX0WVF2KowL3WRgxYgTGjx+PBw8eoHbt2jA2NlbYXr16dbUFR0RERESkEYrpzb6q8p0sDBw4EBEREfj2228BAGPGjJFvEwQBMpkMgiBAIpGoP0oiIiIiIipy+U4WIiMjMWfOHMTHxxdmPEREREREmqUEd3DOd7Igk2VfgZOTU6EFQ0RERESkaUryPAsF6uD8/mRsRERERERUshWog3OlSpU+mTA8e/ZMpYCIiIiIiDQKy5CyTZ8+Hebm5oUVCxERERGRxinJZUgFShZ69OgBGxubwoqFiIiIiIiKkXwnC+yvQERERESkBMuQ/jcaEhERERERvacEJwv5Hg1JKpWyBImIiIiI6AM5fRZUXQoiLCwMdevWhampKWxsbNC5c2fcuHFDYR+ZTIaQkBDY29vD0NAQvr6+uHLlSoHOU6ChU4mIiIiISHzR0dEYOXIkTp06hX379iErKwutWrXCmzdv5PuEh4dj3rx5WLx4Mc6ePQs7Ozu0bNkSr169yvd5CtTBmYiIiIiIPiBCGVJUVJTC69WrV8PGxgbnz59HkyZNIJPJEBERgcmTJ6NLly4AgMjISNja2mL9+vUYOnRovs7DlgUiIiIiIlXI1LQASE1NVVgyMjLyFcLLly8BAFZWVgCA+Ph4JCcno1WrVvJ99PX10bRpU5w4cSLfl8ZkgYiIiIiomHB0dIS5ubl8CQsL++R7ZDIZAgIC8MUXX6BatWoAgOTkZACAra2twr62trbybfnBMiQiIiIiIhWoc1K2xMREmJmZydfr6+t/8r2jRo3CpUuXcOzYsdzH/WD6A5lMVqApEZgsEBERERGpQo19FszMzBSShU8ZPXo0tm/fjiNHjqB8+fLy9XZ2dgCyWxjKlSsnX5+SkpKrteFjWIZERERERKRhZDIZRo0ahS1btuDgwYNwcXFR2O7i4gI7Ozvs27dPvu7du3eIjo6Gj49Pvs/DlgUiIiIiIhWoswwpv0aOHIn169fjn3/+gampqbwfgrm5OQwNDSEIAvz9/REaGgp3d3e4u7sjNDQURkZG6NWrV77Pw2SBiIiIiEgVIgydunTpUgCAr6+vwvrVq1ejf//+AICgoCCkp6djxIgReP78OerXr4+9e/fC1NQ03+dhskBEREREpAoRkgWZ7NNvEAQBISEhCAkJ+byYwD4LRERERESUB7YsEBERERGpQPj/RdVjFEdMFoiIiIiIVCFCGVJRYRkSEREREREpxZYFIiIiIiIViDF0alFhskBEREREpAqWIRERERERUWnDlgUiIiIiIlUV05YBVTFZICIiIiJSQUnus8AyJCIiIiIiUootC0REREREqijBHZyZLBARERERqaAklyExWSAiIiIiUkUJbllgnwUiIiIiIlKKLQtERJ8gy8iATJCKHQYVsnYeTcQOgYqQoJ8hdghUBASZFlAEXzXLkIiIiIiISDmWIRERERERUWnDlgUiIiIiIlWU4JYFJgtERERERCooyX0WWIZERERERERKsWWBiIiIiEgVLEMiIiIiIiJlBJkMgky1u31V319YWIZERERERERKsWWBiIiIiEgVLEMiIiIiIiJlSvJoSEwWiIiIiIhUUYJbFthngYiIiIiIlGLLAhERERGRCliGREREREREyrEMiYiIiIiIShu2LBARERERqYBlSEREREREpBzLkIiIiIiIqLRhywIRERERkYqKaxmRqpgsEBERERGpQibLXlQ9RjHEMiQiIiIiIlKKLQtERERERCrgaEhERERERKRcCR4NickCEREREZEKBGn2ouoxiiP2WSAiIiIiIqXYskBEREREpAqWIRERERERkTIluYMzy5CIiIiIiEgptiwQEREREamiBE/KxmSBiIiIiEgFLEMiIiIiIqJShy0LRERERESq4GhIRERERESkDMuQiIiIiIio1GHLAhERERGRKjgaEhERERERKVOSy5CYLBARERERqaIEd3BmnwUiIiIiIlKKLQtERERERCpgGRIRERERESknlWUvqh6jGGIZEhERERERKcWWBSIiIiIiVZTgDs5MFoiIiIiIVCBADX0W1BKJ+rEMiYiIiIiIlGLLAhERERGRKjiDMxERERERKVOSh05lGRIRERERESnFZIGIiIiISBUyNS0FcOTIEXTo0AH29vYQBAHbtm1TDEkmQ0hICOzt7WFoaAhfX19cuXKlwJfGZIGIiIiISAWCTKaWpSDevHmDGjVqYPHixUq3h4eHY968eVi8eDHOnj0LOzs7tGzZEq9evSrQedhngYiIiIhIFdL/X1Q9RgG0bdsWbdu2VbpNJpMhIiICkydPRpcuXQAAkZGRsLW1xfr16zF06NB8n4ctC0RERERExURqaqrCkpGRUeBjxMfHIzk5Ga1atZKv09fXR9OmTXHixIkCHYvJAhERERGRCtRZhuTo6Ahzc3P5EhYWVuB4kpOTAQC2trYK621tbeXb8otlSEREREREqviMDspKjwEgMTERZmZm8tX6+vqffUhBUJwXWiaT5Vr3KUwWiIiIiIiKCTMzM4Vk4XPY2dkByG5hKFeunHx9SkpKrtaGT2EZEhERERGRKnJmcFZ1URMXFxfY2dlh37598nXv3r1DdHQ0fHx8CnQstiwQEREREalAjBmcX79+jdu3b8tfx8fHIzY2FlZWVqhQoQL8/f0RGhoKd3d3uLu7IzQ0FEZGRujVq1eBzsNkgYiI5L7q9wTdhj+GlU0mEm4aYNk0e1w+YyJ2WKRG3YckwqflE5R3Tce7t1q4dsEMq+Y642G8kdihUSGoVi8V3wxJhnu1N7C2zcT0Ie44uc9S7LBIDc6dO4dmzZrJXwcEBAAA+vXrhzVr1iAoKAjp6ekYMWIEnj9/jvr162Pv3r0wNTUt0HlYhlQAhw8fhiAIePHihdihlCq+vr7w9/cXOwyVOTs7IyIi4qP7hISEoGbNmgU6bnH5fPr374/OnTuLGsO9e/cgCAJiY2NFjUNTNe34HMOmP8KGhTYY0aoSLp82xqw/4lHW4Z3YoZEaVav7EjvW2yPg2xqYPLAatHVkmL3yMvQNJWKHRoXAwFCK+GtGWBLsJHYoJZsIZUi+vr6QyWS5ljVr1gDI7twcEhKCpKQkvH37FtHR0ahWrVqBL40tC1TsbdmyBbq6umKHoXaCIGDr1q0KN9gTJkzA6NGjxQtKwzk6OiIpKQllypQROxSN1GXIE+zZYIWo9dYAgGXBDqjt+wpf9X2K1WHlPvFu0hTTvlO8WZg30R0bT56Ge9XXuHzOXKSoqLCci7bAuWgLscMo8QRp9qLqMYojtiwUsXfvxHlCV5jnzczMLLRjA4CVlVWBm8w0lYmJCaytrcUOQxQymQxZWVkqHUNbWxt2dnbQ0eFzkILS0ZXCvXoazkcr/l07H20KzzpvRIqKioKxaXaLwquX/HtDRLmJmixs2rQJXl5eMDQ0hLW1NVq0aIE3b7L/UVq9ejWqVKkCAwMDeHh4YMmSJQrvffDgAXr06AErKysYGxujTp06OH36tHz70qVLUbFiRejp6aFy5cpYt26dwvsFQcDKlSvx9ddfw8jICO7u7ti+fbvCPrt27UKlSpVgaGiIZs2a4d69ewrbnz59ip49e6J8+fIwMjKCl5cXNmzYoLCPr68vRo0ahYCAAJQpUwYtW7bEwIED8dVXXynsl5WVBTs7O6xateqTn1vOMUeNGgULCwtYW1tjypQpkL3XfOXs7IxZs2ahf//+MDc3x3fffQcA2Lx5M6pWrQp9fX04Oztj7ty5CsdOSkpC+/btYWhoCBcXF6xfvz5X+YwgCFi2bBk6deoEY2NjzJo1CwDw77//onbt2jAwMICrqyumT5+ucPMXEhKCChUqQF9fH/b29hgzZox825IlS+Du7g4DAwPY2trim2++UbjenDKbiRMnokGDBrk+k+rVqyM4OFj++lO/n7zklLH89ddfaNy4MQwNDVG3bl3cvHkTZ8+eRZ06dWBiYoI2bdrg8ePHSmPM0blzZ/Tv31/peZydnQEAX3/9NQRBkL/+sAwpp7Rn+vTpsLGxgZmZGYYOHfrR5O/du3cICgqCg4MDjI2NUb9+fRw+fDhf15+QkIAOHTrA0tISxsbGqFq1Knbt2iXffuXKFbRv3x5mZmYwNTVF48aNcefOHaXHkslkCA8Ph6urKwwNDVGjRg1s2rRJvj2nrG/Pnj2oU6cO9PX1cfToUWRkZGDMmDGwsbGBgYEBvvjiC5w9e1b+vufPn8PPzw9ly5aFoaEh3N3dsXr1agC5y5A+tq8yGRkZuWbOLC3MrCTQ1gFePFG8YXzxWAeWNqolcVScyfDdD3dx+ZwZEm4Zix0MkeYqZqMhqZNojxGSkpLQs2dPhIeH4+uvv8arV69w9OhRyGQyrFixAsHBwVi8eDFq1aqFCxcu4LvvvoOxsTH69euH169fo2nTpnBwcMD27dthZ2eHmJgYSKXZ7Tdbt27F2LFjERERgRYtWmDHjh0YMGAAypcvr9ARZPr06QgPD8dPP/2ERYsWwc/PDwkJCbCyskJiYiK6dOmCYcOGYfjw4Th37hzGjx+vcA1v375F7dq18f3338PMzAw7d+5Enz594Orqivr168v3i4yMxPDhw3H8+HHIZDI8e/YMTZo0QVJSknzs2127duH169fo3r17vj6/yMhIDBo0CKdPn8a5c+cwZMgQODk5yZMCAPjpp58wdepUTJkyBQBw/vx5dO/eHSEhIfj2229x4sQJjBgxAtbW1vKb2r59++LJkyc4fPgwdHV1ERAQgJSUlFznDw4ORlhYGObPnw9tbW3s2bMHvXv3xsKFC+U3kEOGDJHvu2nTJsyfPx8bN25E1apVkZycjIsXLwLI7qAzZswYrFu3Dj4+Pnj27BmOHj2q9Lr9/PwwZ84c3LlzBxUrVgSQfQMbFxcnvxH91O8nP4KDgxEREYEKFSpg4MCB6NmzJ8zMzLBgwQIYGRmhe/fumDZtGpYuXZqv433o7NmzsLGxwerVq9GmTRtoa2vnue+BAwdgYGCAQ4cO4d69exgwYADKlCmD2bNnK91/wIABuHfvHjZu3Ah7e3ts3boVbdq0QVxcHNzd3T8a18iRI/Hu3TscOXIExsbGuHr1KkxMsju3Pnz4EE2aNIGvry8OHjwIMzMzHD9+PM/WgClTpmDLli1YunQp3N3dceTIEfTu3Rtly5ZF06ZN5fsFBQXh559/hqurKywsLBAUFITNmzcjMjISTk5OCA8PR+vWrXH79m1YWVlh6tSpuHr1Knbv3o0yZcrg9u3bSE9PVxpDQfYFgLCwMEyfPv2jn1FJ9+G/VYIA1ScaomJrxNQ7cKn8BhN61RA7FCLNpsZJ2YobUZOFrKwsdOnSBU5O2Z1uvLy8AAAzZ87E3Llz0aVLFwDZY8VevXoVv/76K/r164f169fj8ePHOHv2LKysrAAAbm5u8mP//PPP6N+/P0aMGAEgu3f4qVOn8PPPPyskC/3790fPnj0BAKGhoVi0aBHOnDmDNm3aYOnSpXB1dcX8+fMhCAIqV66MuLg4/Pjjj/L3Ozg4YMKECfLXo0ePRlRUFP7++2+FZMHNzQ3h4eEK15/T2hEUFAQg+0l4t27d5Ddmn+Lo6Jgrtvnz5yskC19++aVCfH5+fmjevDmmTp0KAKhUqRKuXr2Kn376Cf3798f169exf/9++RN0AFi5cqXSG8xevXph4MCB8td9+vTBDz/8IL8Zd3V1xcyZMxEUFITg4GDcv38fdnZ2aNGiBXR1dVGhQgXUq1cPAHD//n0YGxvjq6++gqmpKZycnFCrVi2l112tWjVUr14d69evl1/HH3/8gbp166JSpUoAPv37yY8JEyagdevWAICxY8eiZ8+eOHDgABo1agQAGDRokLwD0ecoW7YsAMDCwkI+cUpe9PT0sGrVKhgZGaFq1aqYMWMGAgMDMXPmTGhpKTYO3rlzBxs2bMCDBw9gb28vv5aoqCisXr0aoaGhHz3X/fv30bVrV/nfRVdXV/m2X375Bebm5ti4caO8D0nOZ/6hN2/eYN68eTh48CAaNmwoP9axY8fw66+/KiQLM2bMQMuWLeXvW7p0KdasWYO2bdsCyE7+9u3bh99++w2BgYG4f/8+atWqJf+N5rTK5HU9+d0XyG65yhlNAgBSU1Ph6Oj40feUFKnPtCHJAizLKiZ/5mWy8Pwxy1NKomFTbqP+l08R1LsGnv73+TPEEhEgyGQQVGwZUPX9hUW0MqQaNWqgefPm8PLyQrdu3bBixQo8f/4cjx8/RmJiIgYNGgQTExP5MmvWLHm5Q2xsLGrVqiVPFD507do1+U1djkaNGuHatWsK66pXry7/s7GxMUxNTeVP0a9du4YGDRooTImdc9OTQyKRYPbs2ahevTqsra1hYmKCvXv34v79+wr75dyovG/w4MHycoiUlBTs3LlT4eb7U5TFduvWLUgk/xvN4sPz5vW55Lzvxo0b0NHRgbe3t3y7m5sbLC1zD7H24bHPnz+PGTNmKHxn3333HZKSkpCWloZu3bohPT0drq6u+O6777B161b5E+mWLVvCyckJrq6u6NOnD/744w+kpaXlee1+fn74448/AGSXumzYsAF+fn4AkK/fT368/9vImekw5wY6Z52yFpfCUKNGDRgZ/W9Iw4YNG+L169dITEzMtW9MTAxkMhkqVaqkcP3R0dH5uv4xY8Zg1qxZaNSoEYKDg3Hp0iX5ttjYWDRu3Dhfnc2vXr2Kt2/fomXLlgpxrF27Nlcc7/+W7ty5g8zMTIXfqa6uLurVqyf/+zt8+HBs3LgRNWvWRFBQEE6cOJFnHAXZFwD09fXlM2eqYwZNTZKVqYVbl4zg3eSVwnrvJq9w9RzLU0oWGYZPvQ2flk8xsX91/PfQQOyAiKgYE+1xkba2Nvbt24cTJ05g7969WLRoESZPnox///0XQPbTxPefzue8BwAMDQ0/efz3b6SB7JvKD9d9eNMjCIK8lEmWj+xu7ty5mD9/PiIiIuDl5QVjY2P4+/vnqic3Ns79D23fvn3xww8/4OTJkzh58iScnZ3RuHHjT56zID48r7LP4P3rzOuala3/8NhSqRTTp0+XP81/n4GBARwdHXHjxg3s27cP+/fvx4gRI/DTTz8hOjoapqamiImJweHDh7F3715MmzYNISEhOHv2LCwsLHIdr1evXvjhhx8QExOD9PR0JCYmokePHvI4gI//fvLj/d9Gzmf24bqccwGAlpZWrs+psDt+f/hdAtnXr62tjfPnz+e63vy0Wg0ePBitW7fGzp07sXfvXoSFhWHu3LkYPXp0vv7evR8HAOzcuRMODg4K2/T1FZ9gvv9byvkMP/b3t23btkhISMDOnTuxf/9+NG/eHCNHjsTPP/+cK46C7EvAluVlELgwETcvGeLaOWO06/0UNg6Z2Lm2dHa6L6lGTLsD369SMGOkJ9LfaMOyTPa/WW9eaeNdRv7/P0mawcBIAnunt/LXdo4ZcK3yBq9e6uDxI7YoqY06+hwU05YFUduWBUFAo0aN0KhRI0ybNg1OTk44fvw4HBwccPfuXfnT4g9Vr14dK1euxLNnz5S2LlSpUgXHjh1D37595etOnDiBKlWq5Ds2T09PbNu2TWHdqVOnFF4fPXoUnTp1Qu/evQFk3yDdunUrX+extrZG586dsXr1apw8eRIDBgzId2zKYjl16hTc3d0/ekPs6emJY8eOKaw7ceIEKlWqBG1tbXh4eCArKwsXLlxA7dq1AQC3b9/O17wS3t7euHHjhkI52IcMDQ3RsWNHdOzYESNHjoSHhwfi4uLg7e0NHR0dtGjRAi1atEBwcDAsLCxw8OBBpclH+fLl0aRJE/zxxx9IT09HixYt5E//bW1tP/n7KQxly5ZFUlKS/LVEIsHly5cVyt4+pKurq9ASlJeLFy8iPT1dfrN+6tQpmJiYoHz58rn2rVWrFiQSCVJSUj47+XR0dMSwYcMwbNgwTJw4EStWrMDo0aNRvXp1REZGIjMz85OtC56entDX18f9+/cVSo4+xc3NDXp6ejh27Jh8hsnMzEycO3dOoQN52bJl0b9/f/Tv3x+NGzdGYGBgnglAQfYt7aK3W8LUUgK/cf/ByiYLCTcMMKW3C1Ie6okdGqnRV72y/18Vvi5OYf28iZWwf6utGCFRIark9QbhG6/LXw+dml39sG9TGcwNdM3rbVRQMgCqDn1aPHMF8ZKF06dP48CBA2jVqhVsbGxw+vRpPH78GFWqVEFISAjGjBkDMzMztG3bFhkZGTh37hyeP3+OgIAA9OzZE6GhoejcuTPCwsJQrlw5XLhwAfb29mjYsCECAwPRvXt3eHt7o3nz5vj333+xZcsW7N+/P9/xDRs2DHPnzkVAQACGDh2K8+fP56pRd3Nzw+bNm3HixAlYWlpi3rx5SE5OzndSMnjwYHz11VeQSCT5rqXPkZiYKI8tJiYGixYtyjWy0YfGjx+PunXrYubMmfj2229x8uRJLF68WD5SkIeHB1q0aIEhQ4Zg6dKl0NXVxfjx42FoaKj0Kfb7pk2bhq+++gqOjo7o1q0btLS0cOnSJcTFxWHWrFlYs2YNJBIJ6tevDyMjI6xbtw6GhoZwcnLCjh07cPfuXTRp0gSWlpbYtWsXpFIpKleunOf5/Pz8EBISgnfv3mH+/PkK2z71+ykMX375JQICArBz505UrFgR8+fP/2SS5ezsLO8Hoa+vr7TcC8ge3WjQoEGYMmUKEhISEBwcjFGjRuXqrwBk9yHw8/ND3759MXfuXNSqVQtPnjzBwYMH4eXlhXbt2n00Jn9/f7Rt2xaVKlXC8+fPcfDgQfnvedSoUVi0aBF69OiBiRMnwtzcHKdOnUK9evVyfVempqaYMGECxo0bB6lUii+++AKpqak4ceIETExM8vy9GxsbY/jw4QgMDJRPVx8eHo60tDQMGjQIQPZvrXbt2qhatSoyMjKwY8eOPP/OFWRfyrYjsgx2RHKeipKsnYd6W7GpeLt02gxtXOqJHQZpMNGSBTMzMxw5cgQRERFITU2Fk5MT5s6dK+/UaGRkhJ9++glBQUEwNjaGl5eX/Mminp4e9u7di/Hjx6Ndu3bIysqCp6cnfvnlFwDZQ1YuWLAAP/30E8aMGQMXFxesXr0avr6++Y6vQoUK2Lx5M8aNG4clS5agXr16CA0NVehXMHXqVMTHx6N169YwMjLCkCFD0LlzZ7x8+TJf52jRogXKlSuHqlWryjuj5lffvn2Rnp6OevXqQVtbG6NHj5aPPpQXb29v/PXXX5g2bRpmzpyJcuXKYcaMGQrDe65duxaDBg1CkyZNYGdnh7CwMFy5cgUGBh+vaW3dujV27NiBGTNmIDw8HLq6uvDw8MDgwYMBZHfknTNnDgICAiCRSODl5YV///0X1tbWsLCwwJYtWxASEoK3b9/C3d0dGzZsQNWqVfM8X7du3TB69Ghoa2vnmjV48ODBH/39FIaBAwfi4sWL6Nu3L3R0dDBu3LiPtioAkCejK1asgIODQ66heXM0b94c7u7uaNKkCTIyMtCjRw+EhITkedzVq1dj1qxZGD9+PB4+fAhra2s0bNjwk4kCkN0iMnLkSDx48ABmZmZo06aNPBmztrbGwYMHERgYiKZNm0JbWxs1a9bM1Q8mx8yZM2FjY4OwsDDcvXsXFhYW8Pb2xqRJkz4aw5w5cyCVStGnTx+8evUKderUwZ49e+TJlJ6eHiZOnIh79+7B0NAQjRs3xsaNG5UeqyD7EhERfa6S3MFZkOWnOJ8KRVpaGuzt7bFq1Sql5TZ58fX1Rc2aNRXmPigsDx48gKOjo7zem4pW//798eLFi1wlcVQ0UlNTYW5uDl90go5Q8mYRJ0XapahDOwHSjAyxQ6AikCXLxKGMv/Dy5ctCGbQi59+JL2v+AB1t1fqAZEkycDB2TqHF+rk4Hp4IpFIpkpOTMXfuXJibm6Njx45ihyR38OBBvH79Gl5eXkhKSkJQUBCcnZ3RpEkTsUMjIiIioiIm6gzOpdX9+/fh4OCAv/76C6tWrYKOjo7CtveHmvxw+XBYVnXLzMzEpEmTULVqVXz99dcoW7asfIK2kiA0NDTPzzanBK4ka9u2bZ7X/6k5GIiIiCgPJXgGZ5YhFTNZWVl51q4D2Z1i308uqGCePXuGZ8+eKd1maGiYa5jPkubhw4d5zmBsZWWV59wlpRXLkEoXliGVLixDKh2KrAzJ63v1lCHF/cgyJPo4HR2djw4/Sqop7TfEJT0ZIiIiIvViskBEREREpIKSPBoSkwUiIiIiIlVwBmciIiIiIlKqBCcLHA2JiIiIiIiUYssCEREREZEqSnDLApMFIiIiIiJVSAEIajhGMcQyJCIiIiIiUootC0REREREKuDQqUREREREpFwJ7rPAMiQiIiIiIlKKLQtERERERKqQygBBxZYBafFsWWCyQERERESkCpYhERERERFRacOWBSIiIiIilaihZQHFs2WByQIRERERkSpKcBkSkwUiIiIiIlVIZVC5ZaCYdnBmnwUiIiIiIlKKLQtERERERKqQSbMXVY9RDDFZICIiIiJSRQnus8AyJCIiIiIiUootC0REREREqijBHZyZLBARERERqYJlSEREREREVNqwZYGIiIiISBUyqKFlQS2RqB2TBSIiIiIiVbAMiYiIiIiIShu2LBARERERqUIqBaDipGpSTspGRERERFTylOAyJCYLRERERESqKMHJAvssEBERERGRUmxZICIiIiJSBWdwJiIiIiIiZWQyKWQy1Tooq/r+wsIyJCIiIiIiUootC0REREREqpDJVC8jKqYdnJksEBERERGpQqaGPgvFNFlgGRIRERERESnFlgUiIiIiIlVIpYCgYgflYtrBmckCEREREZEqWIZERERERESlDVsWiIiIiIhUIJNKIVOxDKm4zrPAZIGIiIiISBUluAyJyQIRERERkSqkMkAomckC+ywQEREREZFSbFkgIiIiIlKFTAZA1aFTi2fLApMFIiIiIiIVyKQyyFQsQ5IV02SBZUhERERERKQUkwUiIiIiIlXIpOpZCmjJkiVwcXGBgYEBateujaNHj6r90pgsEBERERGpQCaVqWUpiD///BP+/v6YPHkyLly4gMaNG6Nt27a4f/++Wq+NyQIRERERkYaZN28eBg0ahMGDB6NKlSqIiIiAo6Mjli5dqtbzsIMzEVEecjqbZSFT5bl2qPiTyd6JHQIVIaksU+wQqAhk/f/3XNidh7NkGZ9VRqRwDGTHmpqaqrBeX18f+vr6CuvevXuH8+fP44cfflBY36pVK5w4cUKlOD7EZIGIKA+vXr0CABzDLpEjoSKR+uldiEgzvXr1Cubm5mo/rp6eHuzs7HAsWT3/TpiYmMDR0VFhXXBwMEJCQhTWPXnyBBKJBLa2tgrrbW1tkZycrJZYcjBZICLKg729PRITE2FqagpBEMQOp8ikpqbC0dERiYmJMDMzEzscKkT8rkuP0vpdy2QyvHr1Cvb29oVyfAMDA8THx+PdO/W0TMpkslz/3nzYqvC+D/dV9n5VMVkgIsqDlpYWypcvL3YYojEzMytVNxWlGb/r0qM0fteF0aLwPgMDAxgYGBTqOT5UpkwZaGtr52pFSElJydXaoCp2cCYiIiIi0iB6enqoXbs29u3bp7B+37598PHxUeu52LJARERERKRhAgIC0KdPH9SpUwcNGzbE8uXLcf/+fQwbNkyt52GyQERECvT19REcHPzROlkqGfhdlx78rkueb7/9Fk+fPsWMGTOQlJSEatWqYdeuXXByclLreQRZYY8lRUREREREGol9FoiIiIiISCkmC0REREREpBSTBSIiIiIiUorJAhERERERKcVkgYiIiIiIlGKyQERERERESjFZICIiKsUkEgliY2Px/PlzsUMhNYuMjMTOnTvlr4OCgmBhYQEfHx8kJCSIGBlpEiYLRESExMREPHjwQP76zJkz8Pf3x/Lly0WMigqDv78/fvvtNwDZiULTpk3h7e0NR0dHHD58WNzgSK1CQ0NhaGgIADh58iQWL16M8PBwlClTBuPGjRM5OtIUTBaIiAi9evXCoUOHAADJyclo2bIlzpw5g0mTJmHGjBkiR0fqtGnTJtSoUQMA8O+//yI+Ph7Xr1+Hv78/Jk+eLHJ0pE6JiYlwc3MDAGzbtg3ffPMNhgwZgrCwMBw9elTk6EhTMFkgIiJcvnwZ9erVAwD89ddfqFatGk6cOIH169djzZo14gZHavXkyRPY2dkBAHbt2oVu3bqhUqVKGDRoEOLi4kSOjtTJxMQET58+BQDs3bsXLVq0AAAYGBggPT1dzNBIgzBZICIiZGZmQl9fHwCwf/9+dOzYEQDg4eGBpKQkMUMjNbO1tcXVq1chkUgQFRUlv4FMS0uDtra2yNGROrVs2RKDBw/G4MGDcfPmTbRv3x4AcOXKFTg7O4sbHGkMJgtERISqVati2bJlOHr0KPbt24c2bdoAAB49egRra2uRoyN1GjBgALp3745q1apBEAS0bNkSAHD69Gl4eHiIHB2p0y+//IKGDRvi8ePH2Lx5s/zv8vnz59GzZ0+RoyNNIchkMpnYQRARkbgOHz6Mr7/+GqmpqejXrx9WrVoFAJg0aRKuX7+OLVu2iBwhqdOmTZuQmJiIbt26oXz58gCyR86xsLBAp06dRI6OiIoTJgtERAQge2Sc1NRUWFpaytfdu3cPRkZGsLGxETEyIsqvS5cuoVq1atDS0sKlS5c+um/16tWLKCrSZEwWiIiISriFCxdiyJAhMDAwwMKFCz+675gxY4ooKioMWlpaSE5Oho2NDbS0tCAIAt6/1ct5LQgCJBKJiJGSpmCyQERUSnl7e+PAgQOwtLRErVq1IAhCnvvGxMQUYWSkbi4uLjh37hysra3h4uKS536CIODu3btFGBmpW0JCAipUqABBED458ZqTk1MRRUWaTEfsAIiISBydOnWSj4DUuXNncYOhQhUfH6/0z1TyvJ8AMBkgdWDLAhERUSkmkUgQFxcHJycnhf4qpPkiIyNRpkwZ+ZCpQUFBWL58OTw9PbFhwwYmE5QvHDqViIiQmJiIBw8eyF+fOXMG/v7+WL58uYhRUWHw9/fHb7/9BiA7UWjSpAm8vb3h6OiIw4cPixscqVVoaCgMDQ0BACdPnsTixYsRHh6OMmXKYNy4cSJHR5qCyQIREaFXr144dOgQACA5ORktWrTAmTNnMGnSJMyYMUPk6EidNm3ahBo1agAA/v33X9y7dw/Xr1+Hv78/Jk+eLHJ0pE6JiYlwc3MDAGzbtg3ffPMNhgwZgrCwMBw9elTk6EhTMFkgIiJcvnwZ9erVAwD89ddf8PLywokTJ7B+/XqsWbNG3OBIrZ48eQI7OzsAwK5du9CtWzdUqlQJgwYNQlxcnMjRkTqZmJjg6dOnAIC9e/fKZ+s2MDBAenq6mKGRBmGyQEREyMzMlHd23r9/Pzp27AgA8PDwQFJSkpihkZrZ2tri6tWrkEgkiIqKkt9ApqWlQVtbW+ToSJ1atmyJwYMHY/Dgwbh586a878KVK1fg7OwsbnCkMZgsEBERqlatimXLluHo0aPYt28f2rRpAwB49OgRrK2tRY6O1GnAgAHo3r07qlWrBkEQ0LJlSwDA6dOn4eHhIXJ0pE6//PILGjZsiMePH2Pz5s3yv8vnz59Hz549RY6ONAVHQyIiIhw+fBhff/01UlNT0a9fP6xatQoAMGnSJFy/fh1btmwROUJSp02bNiExMRHdunVD+fLlAWSPnGNhYYFOnTqJHB0RFSdMFoiICED2yDipqakKw2feu3cPRkZGsLGxETEyIvpcL168wG+//YZr165BEARUqVIFgwYNgrm5udihkYZgskBERHKPHz/GjRs3IAgCKlWqhLJly4odEhWC6Oho/Pzzzwo3kIGBgWjcuLHYoZEanTt3Dq1bt4ahoSHq1asHmUyGc+fOIT09HXv37oW3t7fYIZIGYLJARER48+YNRo8ejbVr10IqlQIAtLW10bdvXyxatAhGRkYiR0jq8vvvv2PAgAHo0qULGjVqBJlMhhMnTmDr1q1Ys2YNevXqJXaIpCaNGzeGm5sbVqxYAR0dHQBAVlYWBg8ejLt37+LIkSMiR0iagMkCERFh6NCh2L9/PxYvXoxGjRoBAI4dO4YxY8agZcuWWLp0qcgRkrpUqVIFQ4YMyTUp17x587BixQpcu3ZNpMhI3QwNDXHhwoVcHdevXr2KOnXqIC0tTaTISJNwNCQiIsLmzZvx22+/oW3btjAzM4OZmRnatWuHFStWYNOmTWKHR2p09+5ddOjQIdf6jh07Ij4+XoSIqLCYmZnh/v37udYnJibC1NRUhIhIEzFZICIipKWlwdbWNtd6GxsbPn0sYRwdHXHgwIFc6w8cOABHR0cRIqLC8u2332LQoEH4888/kZiYiAcPHmDjxo0YPHgwh06lfNMROwAiIhJfw4YNERwcjLVr18LAwAAAkJ6ejunTp6Nhw4YiR0fqNH78eIwZMwaxsbHw8fGBIAg4duwY1qxZgwULFogdHqnRzz//DEEQ0LdvX2RlZQEAdHV1MXz4cMyZM0fk6EhTsM8CEREhLi4Obdu2xdu3b1GjRg0IgoDY2FgYGBhgz549qFq1qtghkhpt3boVc+fOlfdPyBkNiXMslBwSiQTHjh2Dl5cXDAwMcOfOHchkMri5uXHAAioQJgtERAQguyXh999/x/Xr1yGTyeDp6Qk/Pz8YGhqKHRqpSVZWFmbPno2BAwey5KgUMDAwwLVr1+Di4iJ2KKTBmCwQEZVymZmZqFy5Mnbs2AFPT0+xw6FCZmJigsuXL8PZ2VnsUKiQ1a1bF3PmzEHz5s3FDoU0GDs4ExGVcrq6usjIyIAgCGKHQkWgRYsWOHz4sNhhUBGYPXs2JkyYgB07diApKQmpqakKC1F+sGWBiIgwZ84cXL9+HStXrpRP3kQl06+//oqQkBD4+fmhdu3aMDY2VtjesWNHkSIjddPS+t8z4fcfBshkMgiCAIlEIkZYpGGYLBAREb7++mscOHAAJiYm8PLyynUDuWXLFpEiI3V7/wbyQ7yBLFmio6M/ur1p06ZFFAlpMj4+IiIiWFhYoGvXrmKHQUVAKpWKHQIVESYDpA5sWSAiIiql3r59K59Xg0qmo0eP4tdff8Xdu3fx999/w8HBAevWrYOLiwu++OILscMjDcAOzkREBCB7WM39+/fj119/xatXrwAAjx49wuvXr0WOjNRJIpFg5syZcHBwgImJCe7evQsAmDp1Kn777TeRoyN12rx5M1q3bg1DQ0PExMQgIyMDAPDq1SuEhoaKHB1pCiYLRESEhIQEeHl5oVOnThg5ciQeP34MAAgPD8eECRNEjo7Uafbs2VizZg3Cw8Ohp6cnX+/l5YWVK1eKGBmp26xZs7Bs2TKsWLECurq68vU+Pj6IiYkRMTLSJEwWiIgIY8eORZ06dfD8+XOFSdhyOj5TybF27VosX74cfn5+0NbWlq+vXr06rl+/LmJkpG43btxAkyZNcq03MzPDixcvij4g0kjs4ExERDh27BiOHz+u8KQZAJycnPDw4UORoqLC8PDhQ7i5ueVaL5VKkZmZKUJEVFjKlSuH27dv55qA79ixY3B1dRUnKNI4bFkgIiJIpVKlQ2Y+ePAApqamIkREhaVq1ao4evRorvV///03atWqJUJEVFiGDh2KsWPH4vTp0xAEAY8ePcIff/yBCRMmYMSIEWKHRxqCLQtERISWLVsiIiICy5cvB5A93v7r168RHByMdu3aiRwdqVNwcDD69OmDhw8fQiqVYsuWLbhx4wbWrl2LHTt2iB0eqVFQUBBevnyJZs2a4e3bt2jSpAn09fUxYcIEjBo1SuzwSENw6FQiIsKjR4/QrFkzaGtr49atW6hTpw5u3bqFMmXK4MiRI7CxsRE7RFKjPXv2IDQ0FOfPn4dUKoW3tzemTZuGVq1aiR0aFYK0tDRcvXoVUqkUnp6eMDExETsk0iBMFoiICACQnp6ODRs2ICYmRn4D6efnp9DhmYiIShcmC0REREQlRJcuXfK975YtWwoxEiop2GeBiKiU2r59e7737dixYyFGQoXN0tISgiDka99nz54VcjRUmMzNzcUOgUoYtiwQEZVSWlr5GxBPEASlIyWR5oiMjMz3vv369SvESIhI0zBZICIiIiqB4uPjkZWVBXd3d4X1t27dgq6ubq75F4iU4TwLREREpciuXbuwZ8+eXOv37t2L3bt3ixARFZb+/fvjxIkTudafPn0a/fv3L/qASCMxWSAiIowZMwYLFy7MtX7x4sXw9/cv+oCo0Pzwww9Ky8qkUil++OEHESKiwnLhwgU0atQo1/oGDRogNja26AMijcRkgYiIsHnzZqU3FT4+Pti0aZMIEVFhuXXrFjw9PXOt9/DwwO3bt0WIiAqLIAh49epVrvUvX75kPyTKNyYLRESEp0+fKh1FxczMDE+ePBEhIios5ubmuHv3bq71t2/fhrGxsQgRUWFp3LgxwsLCFBIDiUSCsLAwfPHFFyJGRpqEQ6cSERHc3NwQFRWFUaNGKazfvXs3XF1dRYqKCkPHjh3h7++PrVu3omLFigCyE4Xx48dziNwSJjw8HE2aNEHlypXRuHFjAMDRo0eRmpqKgwcPihwdaQqOhkRERFi1ahVGjRqFwMBAfPnllwCAAwcOYO7cuYiIiMB3330ncoSkLi9fvkSbNm1w7tw5lC9fHgDw4MEDNG7cGFu2bIGFhYW4AZJaPXr0CIsXL8bFixdhaGiI6tWrY9SoUbCyshI7NNIQTBaIiAgAsHTpUsyePRuPHj0CADg7OyMkJAR9+/YVOTJSN5lMhn379incQDZp0kTssIioGGKyQERECh4/fgxDQ0OYmJiIHQoVkRcvXrBFoQSKioqCiYmJvH/CL7/8ghUrVsDT0xO//PILLC0tRY6QNAE7OBMREdLT05GWlgYAKFu2LJ4+fYqIiAjs3btX5MhI3X788Uf8+eef8tfdu3eHtbU1HBwccPHiRREjI3ULDAxEamoqACAuLg4BAQFo164d7t69i4CAAJGjI03BZIGIiNCpUyesXbsWQPZT5nr16mHu3Lno1KkTli5dKnJ0pE6//vorHB0dAQD79u3Dvn37sHv3brRt2xaBgYEiR0fqFB8fLx8md/PmzejQoQNCQ0OxZMkSTsBH+cZkgYiIEBMTIx8tZdOmTbCzs0NCQgLWrl2rdLI20lxJSUnyZGHHjh3o3r07WrVqhaCgIJw9e1bk6Eid9PT05C2G+/fvR6tWrQAAVlZW8hYHok9hskBEREhLS4OpqSkAYO/evejSpQu0tLTQoEEDJCQkiBwdqZOlpSUSExMBZNe0t2jRAkB2p2dO1FWyfPHFFwgICMDMmTNx5swZtG/fHgBw8+ZN+UhYRJ/CZIGIiODm5oZt27YhMTERe/bskT+BTElJgZmZmcjRkTp16dIFvXr1QsuWLfH06VO0bdsWABAbGws3NzeRoyN1Wrx4MXR0dLBp0yYsXboUDg4OALLnT2nTpo3I0ZGm4GhIRESETZs2oVevXpBIJGjevLm8Y3NYWBiOHDnC+uYSJDMzEwsWLEBiYiL69++PWrVqAQAiIiJgYmKCwYMHixwhFbU5c+Zg2LBhHBGLlGKyQEREAIDk5GQkJSWhRo0a0NLKbng+c+YMzMzM4OHhASB78i57e3v5diq52rdvj5UrV6JcuXJih0KFzMzMDLGxsZytnZRiskBERPnGm4rSw9TUFBcvXuR3XQrwu6aP4aMhIiLKNz5fIiIqXZgsEBERERGRUkwWiIiIiIhIKSYLRERERESkFJMFIiLKN0EQxA6BiNSscePGMDQ0FDsMKqaYLBARUb6xg3PpMWnSJFhZWYkdBqnozp07mDJlCnr27ImUlBQA2TN3X7lyRb7Prl27OEQu5YnJAhERyd2+fRt79uxBeno6gNzJwdWrV+Hk5CRGaKRG69atQ6NGjWBvb4+EhAQA2ZOy/fPPP/J9Jk6cyEm6NFx0dDS8vLxw+vRpbNmyBa9fvwYAXLp0CcHBwSJHR5qCyQIREeHp06do0aIFKlWqhHbt2iEpKQkAMHjwYIwfP16+n6OjI7S1tcUKk9Rg6dKlCAgIQLt27fDixQtIJBIAgIWFBSIiIsQNjtTqhx9+wKxZs7Bv3z7o6enJ1zdr1gwnT54UMTLSJEwWiIgI48aNg46ODu7fvw8jIyP5+m+//RZRUVEiRkbqtmjRIqxYsQKTJ09WSPzq1KmDuLg4ESMjdYuLi8PXX3+da33ZsmXx9OlTESIiTaQjdgBERCS+vXv3Ys+ePShfvrzCend3d3mZCpUM8fHxqFWrVq71+vr6ePPmjQgRUWGxsLBAUlISXFxcFNZfuHABDg4OIkVFmoYtC0REhDdv3ii0KOR48uQJ9PX1RYiICouLiwtiY2Nzrd+9ezc8PT2LPiAqNL169cL333+P5ORkCIIAqVSK48ePY8KECejbt6/Y4ZGGYLJARERo0qQJ1q5dK3+dc2Px008/oVmzZiJGRuoWGBiIkSNH4s8//4RMJsOZM2cwe/ZsTJo0CYGBgWKHR2o0e/ZsVKhQAQ4ODnj9+jU8PT3RpEkT+Pj4YMqUKWKHRxpCkHEcPCKiUu/q1avw9fVF7dq1cfDgQXTs2BFXrlzBs2fPcPz4cVSsWFHsEEmNVqxYgVmzZiExMREA4ODggJCQEAwaNEjkyKgw3LlzBxcuXIBUKkWtWrXg7u4udkikQZgsEBERACA5ORlLly7F+fPnIZVK4e3tjZEjR3L89RLsyZMnkEqlsLGxETsUIiqmmCwQERERlRABAQH53nfevHmFGAmVFBwNiYiolLp06VK+961evXohRkKFrVatWhAEIV/7xsTEFHI0VJguXLiQr/3y+3sgYrJARFRK1axZE4Ig5Jql+UOCIMgn7iLN1LlzZ7FDoCJy6NAhsUOgEoZlSEREpVRB5k9wcnIqxEiIqLAlJiZCEIRcc6kQfQqTBSIiolLo3LlzuHbtGgRBQJUqVVC7dm2xQyI1y8rKwvTp07Fw4UK8fv0aAGBiYoLRo0cjODgYurq6IkdImoBlSEREBAC4ceMGFi1aJL+B9PDwwOjRo1G5cmWxQyM1evDgAXr27Injx4/DwsICAPDixQv4+Phgw4YNcHR0FDdAUptRo0Zh69atCA8PR8OGDQEAJ0+eREhICJ48eYJly5aJHCFpArYsEBERNm3ahJ49e6JOnTrym4pTp07h7NmzWL9+Pbp16yZyhKQurVq1QmpqKiIjI+WJ4I0bNzBw4EAYGxtj7969IkdI6mJubo6NGzeibdu2Cut3796NHj164OXLlyJFRpqEyQIREcHV1RW9e/fGjBkzFNYHBwdj3bp1uHv3rkiRkboZGhrixIkTqFWrlsL6mJgYNGrUCOnp6SJFRupma2uLw4cPo0qVKgrrr127hiZNmuDx48ciRUaaREvsAIiISHzJycno27dvrvW9e/dGcnKyCBFRYalQoQIyMzNzrc/KyoKDg4MIEVFhGTlyJGbOnImMjAz5uoyMDMyePRujRo0SMTLSJOyzQERE8PX1xdGjR+Hm5qaw/tixY2jcuLFIUVFhCA8Px+jRo/HLL7+gdu3aEAQB586dw9ixY/Hzzz+LHR6p0YULF3DgwAGUL18eNWrUAABcvHgR7969Q/PmzdGlSxf5vlu2bBErTCrmWIZERERYtmwZpk2bhu7du6NBgwYAsvss/P3335g+fTrs7e3l+3bs2FGsMEkNLC0tkZaWhqysLOjoZD8zzPmzsbGxwr7Pnj0TI0RSkwEDBuR739WrVxdiJKTJmCwQERG0tPJXlcoJ2jRfZGRkvvft169fIUZCRJqAyQIRERERESnFPgtEREQlXGpqKszMzOR//pic/UgzeXt748CBA7C0tEStWrUgCEKe+8bExBRhZKSpmCwQEZVSCxcuxJAhQ2BgYICFCxd+dN8xY8YUUVRUGCwtLZGUlAQbGxtYWFgovYGUyWQsMysBOnXqBH19ffmfP5YsEOUHy5CIiEopFxcXnDt3DtbW1nBxcclzP0EQOM+ChouOjkajRo2go6OD6Ojoj+7btGnTIoqKiDQBkwUiIqJS5P79+3B0dMz1xFkmkyExMREVKlQQKTJSN1dXV5w9exbW1tYK61+8eAFvb28+BKB84aRsRESEGTNmIC0tLdf69PT0XLM6k2ZzcXFROnPvs2fPPtrCRJrn3r17SsvKMjIy8ODBAxEiIk3ElgUiIoK2tra8pv19T58+hY2NDevYSxAtLS38999/KFu2rML6hIQEeHp64s2bNyJFRuqyfft2AEDnzp0RGRkJc3Nz+TaJRIIDBw5g3759uHHjhlghkgZhB2ciIpJ3bv3QxYsXYWVlJUJEpG4BAQEAsvugTJ06FUZGRvJtEokEp0+fRs2aNUWKjtSpc+fOALK/6w/nytDV1YWzszPmzp0rQmSkiZgsEBGVYpaWlhAEAYIgoFKlSgoJg0QiwevXrzFs2DARIyR1uXDhAoDsxDAuLg56enrybXp6eqhRowYmTJggVnikRlKpFEB2ydnZs2dRpkwZkSMiTcYyJCKiUiwyMhIymQwDBw5ERESEQrmCnp4enJ2d0bBhQxEjJHUbMGAAFixYwPkUiChfmCwQERGio6Ph4+MDXV1dsUMhIhV8as6U93H+FMoPJgtERKXUp2byfR+fQmu2Ll265HvfLVu2FGIkVNjyO6IV50+h/GKfBSKiUiqvmXzfx1l9S4b3y8uoZIuPjxc7BCph2LJARFRKfWom3/dxVl8iotKJyQIRERFRCTRw4MCPbl+1alURRUKajGVIRESEI0eOfHR7kyZNiigSKmwuLi4fLT9jHXvJ8fz5c4XXmZmZuHz5Ml68eIEvv/xSpKhI0zBZICIi+Pr65lr34ZwLVDL4+/srvM7MzMSFCxcQFRWFwMBAcYKiQrF169Zc66RSKUaMGAFXV1cRIiJNxDIkIiLCy5cvFV7n3EBOnToVs2fPRvPmzUWKjIrKL7/8gnPnzmH16tVih0KF7MaNG/D19UVSUpLYoZAGYLJARER5OnLkCMaNG4fz58+LHQoVsrt376JmzZoFGlKXNNOuXbvQr18/PH78WOxQSAOwDImIiPJUtmxZ3LhxQ+wwqAhs2rQJVlZWYodBahQQEKDwWiaTISkpCTt37kS/fv1Eioo0DZMFIiLCpUuXFF7n3FTMmTMHNWrUECkqKgy1atVS6I8ik8mQnJyMx48fY8mSJSJGRup24cIFhddaWlooW7Ys5s6d+8mRkohysAyJiIigpaUFQRDw4T8JDRo0wKpVq+Dh4SFSZKRu06dPV3idcwPp6+vL75mIcmGyQERESEhIUHidcwNpYGAgUkRERFQcMFkgIiIqhVJSUpCSkgKpVKqwvnr16iJFROr29OlTTJs2DYcOHVL6XT979kykyEiTsM8CEREBAM6cOYPDhw8rvamYN2+eSFGRup0/fx79+vXDtWvXcpWdCYLAOTVKkN69e+POnTsYNGgQbG1tPzoZH1FemCwQERFCQ0MxZcoUVK5cOddNBW8wSpYBAwagUqVK+O2333gDWcIdO3YMx44d4yAFpBImC0REhAULFmDVqlXo37+/2KFQIYuPj8eWLVvg5uYmdihUyDw8PJCeni52GKThtMQOgIiIxKelpYVGjRqJHQYVgebNm+PixYtih0FFYMmSJZg8eTKio6Px9OlTpKamKixE+cEOzkREhPDwcDx69AgRERFih0KF7MmTJ+jXrx/q1auHatWqQVdXV2F7x44dRYqM1O3WrVvo2bNnrvkWZDIZ+6dQvjFZICIiSKVStG/fHjdv3oSnp2euG8gtW7aIFBmp2/bt29GnTx+8evUq1zbeQJYs9erVg46ODsaOHau0f0rTpk1Fiow0CfssEBERRo8ejUOHDqFZs2awtrZmp9cSbMyYMejTpw+mTp0KW1tbscOhQnT58mVcuHABlStXFjsU0mBMFoiICGvXrsXmzZvRvn17sUOhQvb06VOMGzeOiUIpUKdOHSQmJjJZIJUwWSAiIlhZWaFixYpih0FFoEuXLjh06BC/71Jg9OjRGDt2LAIDA+Hl5ZWrvJAT8FF+sM8CERFh9erViIqKwurVq2FkZCR2OFSIZs+ejYiICLRv317pDeSYMWNEiozUTUsr96CXgiCwgzMVCJMFIiJCrVq1cOfOHchkMjg7O+e6gYyJiREpMlI3FxeXPLcJgoC7d+8WYTRUmBISEj663cnJqYgiIU3GMiQiIkLnzp3FDoGKSHx8vNghUBHJbzLQvn17rFy5EuXKlSvkiEgTsWWBiIjybcOGDejYsSOMjY3FDoUKmZmZGWJjY+Hq6ip2KFTITE1NcfHiRX7XpBRncCYionwbOnQo/vvvP7HDoCLAZ4lEBDBZICKiAuANJBFR6cJkgYiIiIiIlGKyQERERERESjFZICIiolwEQRA7BCIqBpgsEBERUS7sn1J6TJo0CVZWVmKHQcUUkwUiIso3JyenXBO2Ucm0e/duODg4iB0GqWjdunVo1KgR7O3t5ZO0RURE4J9//pHvM3HiRFhYWIgUIRV3nJSNiIgAAC9evMCmTZtw584dBAYGwsrKCjExMbC1tZXfNF6+fFnkKElVAQEBStcLggADAwO4ubmhU6dO+OKLL4o4MlK3pUuXYtq0afD398fs2bMhkUgAABYWFoiIiECnTp1EjpA0ASdlIyIiXLp0CS1atIC5uTnu3buHGzduwNXVFVOnTkVCQgLWrl0rdoikJs2aNUNMTAwkEgkqV64MmUyGW7duQVtbGx4eHrhx4wYEQcCxY8fg6ekpdrikAk9PT4SGhqJz584KE69dvnwZvr6+ePLkidghkgZgGRIRESEgIAD9+/fHrVu3YGBgIF/ftm1bHDlyRMTISN06deqEFi1a4NGjRzh//jxiYmLw8OFDtGzZEj179sTDhw/RpEkTjBs3TuxQSUXx8fGoVatWrvX6+vp48+aNCBGRJmKyQEREOHv2LIYOHZprvYODA5KTk0WIiArLTz/9hJkzZ8LMzEy+zszMDCEhIQgPD4eRkRGmTZuG8+fPixglqYOLiwtiY2Nzrd+9ezdbjSjf2GeBiIhgYGCA1NTUXOtv3LiBsmXLihARFZaXL18iJSUl183i48eP5b8BCwsLvHv3TozwSI0CAwMxcuRIvH37FjKZDGfOnMGGDRsQFhaGlStXih0eaQgmC0REhE6dOmHGjBn466+/AGR3dr1//z5++OEHdO3aVeToSJ06deqEgQMHYu7cuahbty4EQcCZM2cwYcIEdO7cGQBw5swZVKpUSdxASWUDBgxAVlYWgoKCkJaWhl69esHBwQELFixAjx49xA6PNAQ7OBMREVJTU9GuXTtcuXIFr169gr29PZKTk9GwYUPs2rULxsbGYodIavL69WuMGzcOa9euRVZWFgBAR0cH/fr1w/z582FsbCwvXalZs6Z4gZJaPXnyBFKpFDY2NmKHQhqGyQIREckdPHgQMTExkEql8Pb2RosWLcQOiQrJ69evcffuXchkMlSsWBEmJiZih0RqNn36dPTu3RsVK1YUOxTSYEwWiIgI9+7dg7Ozs9hhEJEaVa9eHVeuXEHdunXRu3dvfPvtt+yDRAXGZIGIiKClpQUfHx/06dMH3bp1g5WVldghUSF58+YN5syZgwMHDiAlJQVSqVRh+927d0WKjArDlStX8Mcff2Djxo148OABWrRogd69e6Nz584wMjISOzzSAEwWiIgIMTEx2LBhAzZu3IjHjx+jdevW6N27Nzp27Ah9fX2xwyM16tmzJ6Kjo9GnTx+UK1cOgiAobB87dqxIkVFhO378ONavX4+///4bb9++VToCGtGHmCwQEZGcTCbD4cOHsX79emzevBkSiQRdu3bFqlWrxA6N1MTCwgI7d+5Eo0aNxA6FilhsbCx+//13bNy4EU+fPkV6errYIZEG4KRsREQkJwgCmjVrhhUrVmD//v1wdXVFZGSk2GGRGllaWrLMrBSJj4/H7Nmz4enpiTp16iAmJgYhISGcbJHyjckCERHJJSYmIjw8HDVr1kTdunVhbGyMxYsXix0WqdHMmTMxbdo0pKWliR0KFbKGDRvCzc0Nf//9NwYMGICEhAQcPHgQgwcPhrm5udjhkYZgGRIREWH58uX4448/cPz4cVSuXBl+fn7o1asXR0gqgWrVqoU7d+5AJpPB2dkZurq6CttjYmJEiozUbdKkSfDz80PVqlXFDoU0GJMFIiKCo6MjevToAT8/P07EVcJNnz79o9uDg4OLKBIi0gRMFoiICDKZLNeoOESkeQICAjBz5kwYGxsjICDgo/vOmzeviKIiTaYjdgBERCSOS5cuoVq1atDS0kJcXNxH961evXoRRUVEqrhw4QIyMzPlf84LHw5QfrFlgYiolNLS0kJycjJsbGygpaUFQRDw/j8JOa8FQYBEIhExUlKVlZUVbt68iTJlysDS0vKjN4rPnj0rwsiIqLhjywIRUSkVHx+PsmXLyv9MJdf8+fNhamoq/zOfKpdOqampOHjwIDw8PODh4SF2OKQh2LJAREREVAJ1794dTZo0wahRo5Ceno4aNWrg3r17kMlk2LhxI7p27Sp2iKQBOM8CEREhMjISO3fulL8OCgqChYUFfHx8kJCQIGJkpG7a2tpISUnJtf7p06fQ1tYWISIqLEeOHEHjxo0BAFu3boVMJsOLFy+wcOFCzJo1S+ToSFMwWSAiIoSGhsLQ0BAAcPLkSSxevBjh4eEoU6YMxo0bJ3J0pE55FRRkZGRAT0+viKOhwvTy5Uv5bN1RUVHo2rUrjIyM0L59e9y6dUvk6EhTsM8CEREhMTERbm5uAIBt27bhm2++wZAhQ9CoUSP4+vqKGxypxcKFCwFkd1xfuXIlTExM5NskEgmOHDnCOvYSxtHRESdPnoSVlRWioqKwceNGAMDz589hYGAgcnT/1969x/dc938cf3xns4NtZk4NazYzh4zGHGYilZTDwnUh1MJWSVkMURLC0JU5pTleRnUVCaXkECJDyylmExk2ZeZ4uX7bZIfv7w8331u7tlyT73z6bs/77bbbre/78/Hds+2WPq/v+/ASW6FiQUREcHV15dKlS9x///1s3rzZMpvg5ORETk6OwenEGmbNmgXcnFlYsGBBoSVHFStWpG7duixYsMCoeFIKhg8fzoABA3B1dcXHx8dS+O/cuZPAwEBjw4nNULEgIiJ06tSJyMhIgoKCOH78OF27dgXg6NGj1K1b19hwYhW3Trzq2LEja9asoUqVKgYnktI2dOhQWrVqRXp6Op06dcLO7ubqcz8/P+1ZkBLTngUREWH+/PmEhIRw4cIFPvvsM6pWrQrA/v376devn8HpxJo6duyIo6NjkfGcnBzefvttAxJJaQoODqZnz564urqSn5/PoUOHaNu2LaGhoUZHExuho1NFRETKkQoVKnDu3Dlq1KhRaPzSpUvUqFFDDfjKkOHDhxMYGEhERAT5+fl06NCB3bt34+Liwpdffqn9SFIimlkQERE2btzIrl27LK/nz5/Pgw8+SP/+/bly5YqBycTabnXl/m8//vij5eQcKRtWr15Ns2bNAFi/fj2nTp3i2LFjDB8+nHHjxhmcTmyFigUREWH06NFcu3YNgCNHjjBy5Ei6dOlCamoq0dHRBqcTa6hSpQqenp6YTCYCAgLw9PS0fFWuXJlOnTrRp08fo2OKFV28eJH77rsPgA0bNtC7d28CAgKIiIjgyJEjBqcTW6ENziIiwqlTp2jcuDEAn332Gd26dSMmJoYDBw7QpUsXg9OJNcyePRuz2czgwYOZNGkSlStXtly7dRpSSEiIgQnF2mrWrElycjJeXl5s3LiR999/H4Ds7Gw14JMSU7EgIiJUrFiR7OxsAL755hvCw8MB8PT0tMw4iG177rnnAPD19aVt27Y4ODgYnEhK26BBg+jTpw9eXl6YTCY6deoEwPfff6+eGlJiKhZERIR27doRHR1NaGgoiYmJrFy5EoDjx49Tp04dg9PJ3bp27Rru7u4ABAUFkZOT84f9M27dJ7Zv4sSJNGnShPT0dHr37m05BatChQqMHTvW4HRiK3QakoiIkJaWxtChQ0lPTycqKoqIiAgARowYQX5+vqX7r9im35+AZGdnV+wG51sbn3UaUtl0/fp1dW2WP0XFgoiISBm3Y8cOQkNDsbe3Z8eOHbe9t0OHDvcolZS2/Px8YmJiWLBgAefPn+f48eP4+fkxfvx46tata/lQQOR2VCyIiAgAJ0+eZNmyZZw8eZI5c+ZQo0YNNm7ciLe3Nw888IDR8UTkDr399tssX76ct99+m+eff56kpCT8/PxYtWoVs2bNYs+ePUZHFBugYkFERNixYwdPPvkkoaGh7Ny5k5SUFPz8/HjnnXdITExk9erVRkcUK7p+/TqHDx8mMzOTgoKCQtfCwsIMSiXW5u/vz8KFC3n00Udxc3Pjxx9/xM/Pj2PHjhESEqIeKlIi2uAsIiKMHTuWKVOmEB0djZubm2W8Y8eOzJkzx8BkYm0bN24kPDycixcvFrmmPQtlyy+//IK/v3+R8YKCAnJzcw1IJLZITdlERIQjR47Qs2fPIuPVq1fn0qVLBiSS0vLKK6/Qu3dvzp07R0FBQaEvFQplywMPPMB3331XZPzTTz8lKCjIgERiizSzICIieHh4cO7cOXx9fQuNHzx4kNq1axuUSkpDZmYm0dHR1KxZ0+goUsomTJjAs88+yy+//EJBQQFr1qzhp59+YsWKFXz55ZdGxxMboZkFERGhf//+jBkzhoyMDEwmEwUFBSQkJDBq1ChLgzYpG/7+97/z7bffGh1D7oHu3buzcuVKNmzYgMlk4q233iIlJYX169dbGrSJ/C/a4CwiIuTm5jJw4EA++eQTzGYz9vb25Ofn079/f+Lj46lQoYLREcVKsrOz6d27N9WrVycwMLBIJ+eoqCiDkok15eXlMXXqVAYPHoy3t7fRccSGqVgQESnnzGYzaWlpVK9enYyMDA4cOEBBQQFBQUHUr1/f6HhiZUuWLGHIkCE4OztTtWrVQg3aTCYTqampBqYTa3J1dSUpKYm6desaHUVsmIoFEZFyrqCgACcnJ44eParioBy47777iIqKYuzYsdjZaTVyWdajRw969OjBwIEDjY4iNkwbnEVEyjk7Ozvq16/PpUuXVCyUAzdu3KBv374qFMqBJ598ktdff52kpCRatGhBpUqVCl1XTw0pCc0siIgIX331FdOnTycuLo4mTZoYHUdK0YgRI6hevTpvvPGG0VGklN2uIFRPDSkpFQsiIkKVKlXIzs4mLy+PihUr4uzsXOj65cuXDUom1hYVFcWKFSto1qwZTZs2LbLBOTY21qBkIvJXpGVIIiLC7NmzjY4g98iRI0csDbmSkpIKXfv9ZmcREdDMgoiIiEiZNHfu3GLHTSYTTk5O+Pv70759ex2NLLelYkFERLh27Vqx4yaTCUdHRypWrHiPE0lp+/nnnzl58iTt27fH2dkZs9msmYUyxtfXlwsXLpCdnU2VKlUwm81cvXoVFxcXXF1dyczMxM/Pj+3bt6sXg/whHYUgIiJ4eHhQpUqVIl8eHh44Ozvj4+PDhAkTKCgoMDqq3KVLly7x6KOPEhAQQJcuXTh37hwAkZGRjBw50uB0Yk0xMTG0bNmSEydOcOnSJS5fvszx48dp3bo1c+bMIS0tjfvuu48RI0YYHVX+wlQsiIgI8fHx1KpVizfeeIN169axdu1a3njjDWrXrk1cXBwvvPACc+fOZfr06UZHlbs0YsQIHBwcSEtLw8XFxTLet29fNm7caGAysbY333yTWbNmUa9ePcuYv78/7777Lq+//jp16tThnXfeISEhwcCU8lenDc4iIsLy5cuZOXMmffr0sYyFhYURGBjIwoUL2bp1K/fffz9Tp07VkZs2bvPmzWzatIk6deoUGq9fvz5nzpwxKJWUhnPnzpGXl1dkPC8vj4yMDABq1arFf/7zn3sdTWyIZhZERIQ9e/ZYTsj5vaCgIPbs2QNAu3btSEtLu9fRxMqysrIKzSjccvHiRRwdHQ1IJKWlY8eOvPjiixw8eNAydvDgQV566SUeeeQR4ObpWL6+vkZFFBugYkFERKhTpw5Lly4tMr506VLLxsdLly5RpUqVex1NrKx9+/asWLHC8tpkMlFQUMA//vEPOnbsaGAysbalS5fi6elJixYtcHR0xNHRkeDgYDw9PS3/vbu6ujJz5kyDk8pfmU5DEhERvvjiC3r37k3Dhg1p2bIlJpOJH374gWPHjrF69Wq6detGXFwcJ06cUNMuG5ecnMzDDz9MixYt2LZtG2FhYRw9epTLly+TkJBQaH27lA3Hjh3j+PHjmM1mGjZsSIMGDYyOJDZExYKIiABw+vRpFixYUOih4sUXX6Ru3bpGRxMry8jIIC4ujv3791NQUEDz5s15+eWX8fLyMjqalJJbj3s6HlfulIoFERGRciI3N5fHH3+chQsXEhAQYHQcuQdWrFjBP/7xD06cOAFAQEAAo0eP5tlnnzU4mdgKnYYkIiIAXL16laVLl5KSkoLJZKJx48YMHjyYypUrGx1NrMTBwYGkpCR9ulxOxMbGMn78eF555RVCQ0Mxm80kJCQwZMgQLl68qP4KUiKaWRAREfbt20fnzp1xdnamVatWmM1m9u3bR05ODps3b6Z58+ZGRxQrGTlyJA4ODuqZUQ74+voyadIkwsPDC40vX76ciRMncurUKYOSiS1RsSAiIjz00EP4+/uzePFi7O1vTjrn5eURGRlJamoqO3fuNDihWMuwYcNYsWIF/v7+BAcHU6lSpULXtYG97HByciIpKQl/f/9C4ydOnCAwMJDr168blExsiZYhiYgI+/btK1QoANjb2/Paa68RHBxsYDKxtqSkJMtM0fHjxwtd0/KkssXf359Vq1YVaaS4cuVK6tevb1AqsTUqFkREBHd3d9LS0mjYsGGh8fT0dNzc3AxKJaVh+/btJbrv7Nmz1KpVCzs7tWSyVZMmTaJv377s3LmT0NBQTCYTu3btYuvWraxatcroeGIj9DeAiIjQt29fIiIiWLlyJenp6Zw9e5ZPPvmEyMhI+vXrZ3Q8MUDjxo05ffq00THkLvztb38jMTGRatWqsW7dOtasWUO1atVITEykZ8+eRscTG6GZBRER4d1338VkMhEeHk5eXh5w8+Scl156SRthyyltabRtubm5vPDCC4wfP54PP/zQ6Dhiw7TBWUSknMvPz2fXrl0EBgbi5OTEyZMnMZvN+Pv74+LiYnQ8MYibmxs//vgjfn5+RkeRP8nDw4MDBw7odyh3RcuQRETKuQoVKtC5c2f+/e9/4+LiQmBgIE2bNlWhIGLjevbsybp164yOITZOy5BERITAwEBSU1Px9fU1OoqIWIm/vz+TJ09m9+7dtGjRosgxuVFRUQYlE1uiZUgiIsLmzZsZM2YMkydPLvahwt3d3aBkYhR3d3cOHTqkJSw27HbFv8lkIjU19R6mEVulYkFERAodj/n7s/bNZjMmk4n8/HwjYomBtGdBREDLkEREhJKfvS/lR3JyMrVq1TI6hljJrc+G1XhP7pRmFkRERMqRnj17FvvAaDKZcHJywt/fn/79+9OgQQMD0om1LV26lFmzZnHixAkA6tevz/Dhw4mMjDQ4mdgKzSyIiAgAV65cYenSpaSkpGAymWjUqBGDBg3C09PT6GhiRZUrV2bdunV4eHjQokULzGYzBw8e5OrVqzz++OOsXLmSGTNmsHXrVkJDQ42OK3dh/PjxzJo1i2HDhhESEgLAnj17GDFiBKdPn2bKlCkGJxRboJkFERFhx44dhIWFUblyZYKDgwHYv38/V69e5YsvvqBDhw4GJxRrGTt2LNeuXeO9996z7FUpKCjg1Vdfxc3NjalTpzJkyBCOHj3Krl27DE4rd6NatWrMmzevSBf2jz/+mGHDhnHx4kWDkoktUbEgIiI0adKEtm3bEhcXR4UKFYCbzdqGDh1KQkICSUlJBicUa6levToJCQkEBAQUGj9+/Dht27bl4sWLHDlyhIceeoirV68aE1KsokqVKiQmJlK/fv1C48ePH6dVq1b6/UqJqCmbiIhw8uRJRo4caSkU4GaztujoaE6ePGlgMrG2vLw8jh07VmT82LFjllOvnJyctBG2DHjmmWeIi4srMr5o0SIGDBhgQCKxRdqzICIiNG/enJSUlCKbWlNSUnjwwQeNCSWl4tlnnyUiIoI33niDli1bYjKZSExMJCYmhvDwcODmsrQHHnjA4KRiDUuXLmXz5s20adMGgL1795Kenk54eDjR0dGW+2JjY42KKH9xWoYkIiKsXLmS1157jWHDhhV6qJg/fz7Tp0+nUaNGlnubNm1qVEyxgvz8fKZPn857773H+fPnAahZsybDhg1jzJgxVKhQgbS0NOzs7KhTp47BaeVudOzYsUT3mUwmtm3bVsppxFapWBARkUJN2YpjMpnUoK0MunbtGqAO3eXd2bNnqVWr1v/8e0DKJxULIiLCmTNnSnyvj49PKSYRkXvN3d2dQ4cOqVu3FEt7FkREpMQFQNeuXVmyZAleXl6lnEhKy/nz5xk1ahRbt24lMzOT//7MUDNH5Y8+N5bbUbEgIiIltnPnTnJycoyOIXdh4MCBpKWlMX78eLy8vHTqkYjclooFERGRcmTXrl189913OuVKREpEO1lERETKEW9vby07EZESU7EgIiJSjsyePZuxY8dy+vRpo6PIX4SWosntaBmSiIhIOdK3b1+ys7OpV68eLi4uODg4FLp++fJlg5KJUTTTJLejYkFERKQcmT17ttER5B6Jj4+nT58+uLi43Pa+5ORkatWqdY9Sia1RnwURESErK4tKlSr9z/umTZvGSy+9hIeHR+mHEpG74uXlRVZWFr179yYiIoK2bdsaHUlskIoFERHB1dWVPn36MHjwYNq1a2d0HLGya9euWbo03+ra/EfUzbnsyM/P56uvviI+Pp6vvvoKX19fBg0axHPPPcd9991ndDyxESoWRESE9evXEx8fz5dffomPjw+DBw8mPDxcSxPKiAoVKnDu3Dlq1KiBnZ1dsRtazWYzJpNJTdnKqMzMTD788EPi4+M5duwYTzzxBBEREXTv3h07O513I39MxYKIiFhcunSJFStWEB8fT3JyMp07d2bw4MGEhYVhb69tbrZqx44dhIaGYm9vz44dO257b4cOHe5RKrnXvv/+e/75z3+yfPlyvLy8uHr1Kh4eHixbtoyHH37Y6HjyF6ViQUREijVv3jxGjx7NjRs3qFatGkOGDGHs2LH/c7OkiPx1nD9/ng8++IBly5aRmppKjx49iIiI4LHHHiMnJ4c333yT1atXc+bMGaOjyl+UigUREbHIyMhgxYoVLFu2jLS0NHr27ElERAS//vor06dPx8vLi82bNxsdU+7Cxo0bcXV1texNmT9/PosXL6Zx48bMnz+fKlWqGJxQrKV79+5s2rSJgIAAIiMjCQ8Px9PTs9A9v/76K3Xq1KGgoMCglPJXp2JBRERYs2YNy5YtY9OmTTRu3JjIyEieeeaZQqceHT16lKCgIG7cuGFcULlrgYGBzJgxgy5dunDkyBGCg4MZOXIk27Zto1GjRixbtszoiGIlERERREZGEhIS8of3mM1m0tLS8PHxuYfJxJZoAaqIiDBo0CD69etHQkICLVu2LPYePz8/xo0bd4+TibWdOnWKxo0bA/DZZ5/RvXt3YmJiOHDgAF26dDE4nVhLbm4uqampVK1a9bb3mUwmFQpyWyoWRETKuby8PKZNm0avXr1ue5yis7MzEyZMuIfJpDRUrFiR7OxsAL755hvCw8MB8PT0/J/HqortcHBwICkpqdiTr0TuhM7KEhEp5+zt7Rk1ahS//fab0VHkHmjXrh3R0dFMnjyZxMREunbtCsDx48epU6eOwenEmsLDw1m6dKnRMcTGaWZBRERo3bo1Bw8e1HKEcuC9995j6NChrF69mri4OGrXrg3A119/zRNPPGFwOrGmGzdusGTJErZs2UJwcHCRLu2xsbEGJRNbog3OIiLCp59+ytixYxkxYgQtWrQo8lDRtGlTg5KJyJ/VsWPHP7xmMpnYtm3bPUwjtkrFgoiIFNvB1WQyqatvGZWfn8/atWtJSUnBZDLRsGFDevToocZ7IlKE/lYQERFOnTpldAS5R5KSkggLC+P8+fM0aNAAuLlfoXr16nzxxRcEBgYanFBE/ko0syAiIlKOtGnThho1arB8+XJLA7YrV64wcOBAMjMz2bNnj8EJxZp++OEHPv30U9LS0or0SFmzZo1BqcSWqFgQERGL5OTkYh8qwsLCDEok1ubs7My+fft44IEHCo0nJSXRsmVLcnJyDEom1vbJJ58QHh7O448/zpYtW3j88cc5ceIEGRkZ9OzZUw34pES0DElEREhNTaVnz54cOXLEslcBsJzRrj0LZUeDBg04f/58kWIhMzMTf39/g1JJaYiJiWHWrFm8/PLLuLm5MWfOHHx9fXnxxRfx8vIyOp7YCPVZEBERXn31VXx9fTl//jwuLi4cPXqUnTt3EhwczLfffmt0PLGimJgYoqKiWL16NWfPnuXs2bOsXr2a4cOHM2PGDK5du2b5Ett28uRJSx8NR0dHsrKyMJlMjBgxgkWLFhmcTmyFZhZERIQ9e/awbds2qlevjp2dHXZ2drRr145p06YRFRXFwYMHjY4oVtKtWzcA+vTpY5k5ujWT1L17d8trnYJl+zw9PfnPf/4DQO3atUlKSiIwMJCrV69auniL/C8qFkREhPz8fFxdXQGoVq0av/76Kw0aNMDHx4effvrJ4HRiTdu3bzc6gtwjDz30EFu2bCEwMJA+ffrw6quvsm3bNrZs2cKjjz5qdDyxESoWRESEJk2acPjwYfz8/GjdujXvvPMOFStWZNGiRfj5+RkdT6yoQ4cORkeQe+S9997j+vXrALz++us4ODiwa9cuevXqxfjx4w1OJ7ZCpyGJiAibNm0iKyuLXr16kZqaSrdu3Th27BhVq1Zl5cqVPPLII0ZHFCvLzs4u9uQrdesWkd9TsSAiIsW6fPkyVapUsaxrl7LhwoULDBo0iK+//rrY69qnYNvuZGO6u7t7KSaRskLLkEREpFienp5GR5BSMHz4cK5cucLevXvp2LEja9eu5fz580yZMoWZM2caHU/ukoeHR4kLfBWGUhIqFkREhOvXrzNv3jy2b99OZmYmBQUFha4fOHDAoGRibdu2bePzzz+nZcuW2NnZ4ePjQ6dOnXB3d2fatGmWozbFNv1+A/vp06cZO3YsAwcOJCQkBLh58tny5cuZNm2aURHFxqhYEBERBg8ezJYtW/j73/9Oq1attPSoDMvKyqJGjRrAzdmjCxcuEBAQQGBgoIrCMuD3G9jffvttYmNj6devn2UsLCyMwMBAFi1axHPPPWdERLExKhZERISvvvqKDRs2EBoaanQUKWUNGjTgp59+om7dujz44IMsXLiQunXrsmDBAnX1LWP27NnDggULiowHBwcTGRlpQCKxRergLCIi1K5dGzc3N6NjyD0wfPhwzp07B8CECRPYuHEj999/P3PnziUmJsbgdGJN3t7exRYLCxcuxNvb24BEYot0GpKIiPD1118zd+5cFixYgI+Pj9Fx5B7Kzs7m2LFj3H///VSrVs3oOGJFGzZs4G9/+xv16tWjTZs2AOzdu5eTJ0/y2Wef0aVLF4MTii1QsSAiIly4cIE+ffqwc+dOXFxccHBwKHT98uXLBiUTkbtx9uxZ3n//fY4dO4bZbKZx48YMGTJEMwtSYioWRESExx57jLS0NCIiIqhZs2aRDc7aCGnboqOjS3xvbGxsKSYREVujDc4iIsLu3bvZs2cPzZo1MzqKlIKDBw+W6D6dglX2XL16lcTExGKPRA4PDzcoldgSFQsiIkLDhg3JyckxOoaUkt+fvS/lx/r16xkwYABZWVm4ubkVKgZNJpOKBSkRLUMSERE2b97MpEmTmDp1KoGBgUX2LLi7uxuUTErLzz//zMmTJ2nfvj3Ozs6YzWbNLJQxAQEBdOnShZiYGFxcXIyOIzZKxYKIiGBnd/Mk7f9+WLz1AJmfn29ELCkFly5dok+fPmzfvh2TycSJEyfw8/MjIiICDw8PZs6caXREsZJKlSpx5MgR/Pz8jI4iNkzLkERERMtUypERI0bg4OBAWloajRo1soz37duXESNGqFgoQzp37sy+fftULMhdUbEgIiJ06NDB6Ahyj2zevJlNmzZRp06dQuP169fnzJkzBqWS0tC1a1dGjx5NcnJyscsLw8LCDEomtkTFgohIOXX48GGaNGmCnZ0dhw8fvu29TZs2vUeppLRlZWUVu3794sWLODo6GpBISsvzzz8PwNtvv13kmpYXSklpz4KISDllZ2dHRkYGNWrUwM7ODpPJRHH/S9BDRdnStWtXmjdvzuTJk3Fzc+Pw4cP4+Pjw9NNPU1BQwOrVq42OKCJ/IZpZEBEpp06dOkX16tUt/yzlw7vvvkuHDh3Yt28fN27c4LXXXuPo0aNcvnyZhIQEo+OJyF+MndEBRETEGD4+PphMJnJzc5k4cSL5+fn4+PgU+yVlQ25uLkOHDuWLL76gVatWdOrUiaysLHr16sXBgwepV6+e0RHFynbs2EH37t3x9/enfv36hIWF8d133xkdS2yIliGJiAgeHh4cOHBAp6aUA9WrV2f37t3Ur1/f6ChSyj788EMGDRpEr169CA0NxWw2s3v3btauXUt8fDz9+/c3OqLYABULIiLCoEGDCAwMJDo62ugoUspGjhyJg4MD06dPNzqKlLJGjRrxwgsvMGLEiELjsbGxLF68mJSUFIOSiS3RngUREcHf35/Jkyeze/duWrRoQaVKlQpdj4qKMiiZWNuNGzdYsmQJW7ZsITg4uMjvOjY21qBkYm2pqal07969yHhYWBhvvPGGAYnEFqlYEBERlixZgoeHB/v372f//v2FrplMJhULZUhSUhLNmzcH4Pjx44Wu/XcHb7Ft3t7ebN26FX9//0LjW7duxdvb26BUYmtULIiISKHTkG6tTtWDY9mkbt3lx8iRI4mKiuLQoUO0bdsWk8nErl27iI+PZ86cOUbHExuhPQsiIgLA0qVLmTVrFidOnABudvQdPnw4kZGRBicTkT9r7dq1zJw507I/oVGjRowePZqnnnrK4GRiK1QsiIgI48ePZ9asWQwbNoyQkBAA9uzZw3vvvcerr77KlClTDE4oIiJGULEgIiJUq1aNefPm0a9fv0LjH3/8McOGDePixYsGJRORP+uHH36goKCA1q1bFxr//vvvqVChAsHBwQYlE1uipmwiIkJ+fn6xDw4tWrQgLy/PgEQicrdefvll0tPTi4z/8ssvvPzyywYkElukYkFERHjmmWeIi4srMr5o0SIGDBhgQCIRuVvJycmWk69+LygoiOTkZAMSiS3SaUgiIgLc3OC8efNm2rRpA8DevXtJT08nPDy8ULM2ncMvYhscHR05f/58kc7s586dw95ej4BSMtqzICIidOzYsUT3mUwmtm3bVsppRMQann76aTIyMvj888+pXLkyAFevXqVHjx7UqFGDVatWGZxQbIGKBREREZEy6JdffqF9+/ZcunSJoKAgAA4dOkTNmjXZsmWLGrNJiahYEBERESmjsrKy+Oijj/jxxx9xdnamadOm9OvXDwcHB6OjiY1QsSAiIiIiIsXSaUgiIiIiZdQHH3xAu3btqFWrFmfOnAFg1qxZfP755wYnE1uhYkFERESkDIqLiyM6Oponn3ySK1eukJ+fD0CVKlWYPXu2seHEZqhYEBERESmD5s2bx+LFixk3blyho1KDg4M5cuSIgcnElqhYEBERESmDTp06ZTkF6fccHR3JysoyIJHYIhULIiIiImWQr68vhw4dKjL+9ddf07hx43sfSGyS2veJiIiIlEGjR4/m5Zdf5vr165jNZhITE/n444+ZNm0aS5YsMTqe2AgdnSoiIiJSRi1evJgpU6aQnp4OQJ06dZgwYQIREREGJxNboWJBREREpAzKycnBbDbj4uLCxYsXSU1NJSEhgcaNG9O5c2ej44mN0J4FERERkTLoqaeeYsWKFQDY29sTFhZGbGwsPXr0IC4uzuB0YitULIiIiIiUQQcOHOChhx4CYPXq1dSsWZMzZ86wYsUK5s6da3A6sRUqFkRERETKoOzsbNzc3ADYvHkzvXr1ws7OjjZt2li6OYv8LyoWRERERMogf39/1q1bR3p6Ops2beLxxx8HIDMzE3d3d4PTia1QsSAiIiJSBr311luMGjWKunXr0rp1a0JCQoCbswzFNWsTKY5OQxIREREpozIyMjh37hzNmjXDzu7mZ8SJiYm4u7vTsGFDg9OJLVCxICIiIiIixdIyJBERERERKZaKBRERERERKZaKBRERERERKZaKBRERERERKZaKBRERkb+wiRMn8uCDD1peDxw4kB49etzzHKdPn8ZkMnHo0KE/vKdu3brMnj27xO8ZHx+Ph4fHXWczmUysW7furt9HRIpSsSAiInKHBg4ciMlkwmQy4eDggJ+fH6NGjSIrK6vUv/ecOXOIj48v0b0lecAXEbkde6MDiIiI2KInnniCZcuWkZuby3fffUdkZCRZWVnExcUVuTc3NxcHBwerfN/KlStb5X1EREpCMwsiIiJ/gqOjI/fddx/e3t7079+fAQMGWJbC3Fo69M9//hM/Pz8cHR0xm838+9//5oUXXqBGjRq4u7vzyCOP8OOPPxZ63+nTp1OzZk3c3NyIiIjg+vXrha7/9zKkgoICZsyYgb+/P46Ojtx///1MnToVAF9fXwCCgoIwmUw8/PDDlj+3bNkyGjVqhJOTEw0bNuT9998v9H0SExMJCgrCycmJ4OBgDh48eMc/o9jYWAIDA6lUqRLe3t4MHTqU//u//yty37p16wgICMDJyYlOnTqRnp5e6Pr69etp0aIFTk5O+Pn5MWnSJPLy8u44j4jcORULIiIiVuDs7Exubq7l9c8//8yqVav47LPPLMuAunbtSkZGBhs2bGD//v00b96cRx99lMuXLwOwatUqJkyYwNSpU9m3bx9eXl5FHuL/2+uvv86MGTMYP348ycnJ/Otf/6JmzZrAzQd+gG+++YZz586xZs0aABYvXsy4ceOYOnUqKSkpxMTEMH78eJYvXw5AVlYW3bp1o0GDBuzfv5+JEycyatSoO/6Z2NnZMXfuXJKSkli+fDnbtm3jtddeK3RPdnY2U6dOZfny5SQkJHDt2jWefvppy/VNmzbxzDPPEBUVRXJyMgsXLiQ+Pt5SEIlIKTOLiIjIHXnuuefMTz31lOX1999/b65ataq5T58+ZrPZbJ4wYYLZwcHBnJmZabln69atZnd3d/P169cLvVe9evXMCxcuNJvNZnNISIh5yJAhha63bt3a3KxZs2K/97Vr18yOjo7mxYsXF5vz1KlTZsB88ODBQuPe3t7mf/3rX4XGJk+ebA4JCTGbzWbzwoULzZ6enuasrCzL9bi4uGLf6/d8fHzMs2bN+sPrq1atMletWtXyetmyZWbAvHfvXstYSkqKGTB///33ZrPZbH7ooYfMMTExhd7ngw8+MHt5eVleA+a1a9f+4fcVkT9PexZERET+hC+//BJXV1fy8vLIzc3lqaeeYt68eZbrPj4+VK9e3fJ6//79/N///R9Vq1Yt9D45OTmcPHkSgJSUFIYMGVLoekhICNu3by82Q0pKCr/99huPPvpoiXNfuHCB9PR0IiIieP755y3jeXl5lv0QKSkpNGvWDBcXl0I57tT27duJiYkhOTmZa9eukZeXx/Xr18nKyqJSpUoA2NvbExwcbPkzDRs2xMPDg5SUFFq1asX+/fv54YcfCs0k5Ofnc/36dbKzswtlFBHrU7EgIiLyJ3Ts2JG4uDgcHByoVatWkQ3Mtx6GbykoKMDLy4tvv/22yHv92eNDnZ2d7/jPFBQUADeXIrVu3brQtQoVKgBgNpv/VJ7fO3PmDF26dGHIkCFMnjwZT09Pdu3aRURERKHlWnDz6NP/dmusoKCASZMm0atXryL3ODk53XVOEbk9FQsiIiJ/QqVKlfD39y/x/c2bNycjIwN7e3vq1q1b7D2NGjVi7969hIeHW8b27t37h+9Zv359nJ2d2bp1K5GRkUWuV6xYEbj5SfwtNWvWpHbt2qSmpjJgwIBi37dx48Z88MEH5OTkWAqS2+Uozr59+8jLy2PmzJnY2d3cIrlq1aoi9+Xl5bFv3z5atWoFwE8//cTVq1dp2LAhcPPn9tNPP93Rz1pErEfFgoiIyD3w2GOPERISQo8ePZgxYwYNGjTg119/ZcOGDfTo0YPg4GBeffVVnnvuOYKDg2nXrh0fffQRR48exc/Pr9j3dHJyYsyYMbz22mtUrFiR0NBQLly4wNGjR4mIiKBGjRo4OzuzceNG6tSpg5OTE5UrV2bixIlERUXh7u7Ok08+yW+//ca+ffu4cuUK0dHR9O/fn3HjxhEREcGbb77J6dOneffdd+/o37devXrk5eUxb948unfvTkJCAgsWLChyn4ODA8OGDWPu3Lk4ODjwyiuv0KZNG0vx8NZbb9GtWze8vb3p3bs3dnZ2HD58mCNHjjBlypQ7/0WIyB3RaUgiIiL3gMlkYsOGDbRv357BgwcTEBDA008/zenTpy2nF/Xt25e33nqLMWPG0KJFC86cOcNLL7102/cdP348I0eO5K233qJRo0b07duXzMxM4OZ+gLlz57Jw4UJq1arFU089BUBkZCRLliwhPj6ewMBAOnToQHx8vOWoVVdXV9avX09ycjJBQUGMGzeOGTNm3NG/74MPPkhsbCwzZsygSZMmfPTRR0ybNq3IfS4uLowZM4b+/fsTEhKCs7Mzn3zyieV6586d+fLLL9myZQstW7akTZs2xMbG4uPjc0d5ROTPMZmtsTBRRERERETKHM0siIiIiIhIsVQsiIiIiIhIsVQsiIiIiIhIsVQsiIiIiIhIsVQsiIiIiIhIsVQsiIiIiIhIsVQsiIiIiIhIsVQsiIiIiIhIsVQsiIiIiIhIsVQsiIiIiIhIsVQsiIiIiIhIsf4flJ3JTeuZPnMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_predictions(encoded_dataset[\"test\"][\"labels\"], test_preds, display_labels=label2id, xticks_rotation=\"vertical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry:  2\n",
      "Label relapsing_remitting_multiple_sclerosis\n",
      "Predicted primary_progressive_multiple_sclerosis\n",
      "Text:  Multiple Sklerose, EM 11.1.2009, ED 31.1.2012 nach revidierten McDonald Kriterien 2010 INDENT klinisch neurologisch: keine Defizite, EDSS 0  INDENT Verlauf: INDENT 10/2011: Taubheit linke Kopf- und Gesichtshälfte bis 12/2011 INDENT 01/2009 ein Schub mit Abduzensparese rechtes Auge, sensomotorisches proximal und Beinbetontes Hemisyndrom links. Unter Cortison Besserung INDENT diagnostisch: INDENT 01/2009 Liquor: Zellzahl 2/ul, Protein 0.9 g/l, IgG-Index erhöht (0.74), pos. OKB- 01/2009 Immunserologie: Antinukleäre AK 1:80, ansonsten normwertig INDENT 1/2009 MRI Kopf: multiple demyelinisierend Läsionen supra- und infratentoriell   (u..Radiatio optica, Zentrum semiovale, periventrikulär, juxtakortikal);    pontomedullär, Forceps major li. u. rechts-frontal mit KM-Aufnahme (Bethanien)- 02/2011 MRI Schädel und spinal (Bahnhofplatz): Keine akuten zerebralen oder   spinalen Läsionen. Mässiger Lesion load mit periventrikulären temporo-parieto-  okzipitalen konfluierenden an das Ventrikelependym angrenzenden T1-  hypointensen Entmarkungsherden, einzelnen im Centrum semiovale, subkortikal sowie infratentoriellen Läsionen. Wenige im Bereich der Hinter-Seitenstränge befindliche zervikale intramedulläre Entmarkungsherde C5 und C6 sowie Th3/4  und Th7-8  Leichte Ventrikelerweiterung parieto-okzipital, keine zerebrale, keine Myelonatrophie. Geringe erweiterung des Zentralkanals als anatomische Variante INDENT therapeutisch: INDENT 17.01. - 19.01.2009 Solu-Medrol i.v. 1000mg 3 Tage mit Ausschleichen INDENT  02/2009 Avonex i.m. 1x/Woche bis 02/2010 (grippeähnliche Symptome) INDENT Aktuell: Gilenya 0.5 mg/die seit 10.05.12 \n",
      "\n",
      "\n",
      "Entry:  6\n",
      "Label secondary_progressive_multiple_sclerosis\n",
      "Predicted relapsing_remitting_multiple_sclerosis\n",
      "Text:  Schubförmige, unvollständig remittierende Multiple Sklerose mit sekundär progredientem Verlauf, EM 2007, ED 2010, EDSS 6.5 INDENT aktuell: klinisch: nicht aktiv, radiologisch: nicht aktiv, Progression: ja (nach Lublin et al. 2013) anamnestisch: zunehmende Einschränkung der Gehfähigkeit mit deutlicher Progredienz seit 2017 klinisch: rechts- und beinbetonte Tetraparese (Arm distal betont), überwiegend rollstuhlmobil Verlauf: INDENT 2007: febriler Infekt mit Steifheit der Beine, spontane Regredienz nach 2-3 Tagen INDENT 2008: Hinken des linken Beines nach 30min Gehzeit mit sukzessiver Verkürzung der Latenz bis zum Auftreten des Hinkens bis 09/2009 -> erstmaliges Ansprechen von Aussenstehenden INDENT 12/2009: zentraler Lagerungsschwindel INDENT 2017: progrediente Verschlechterung der Gehfähigkeit mit 2 Walkingstöcken, längere Strecken mit dem Rollstuhl diagnostisch: Labor INDENT Liquor: Befund nicht vorliegend INDENT Virologisches & bakteriologisches Screening: HIV, HBV, HCV; Treponema pallidum, Borrelia burdorferi; Befunde nicht vorliegend INDENT Immunologisches Screening: ANA, RF, Anti-CCP, Anti-native DNA, Anti-SSA/B (Ro/La), Anti-Cardiolipin, ANCA; Befunde nicht vorliegend INDENT JC-Virus: Befund nicht vorliegend Bildgebung INDENT cMRI vom 07.10.2015 (Spital Netz Bern): Status idem zu 11/2014 INDENT cMRI vom 06.04.2016 (Spital Aarberg): Stable disease zu 10/2015, keine Progression, keine Aktivität. Kein Anhalt für PML INDENT MRI gesamte WS vom 16.08.2016 (InselSpital Bern): Kein Nachweis einer akuten Pathologie im Rahmen der intrathekalen Kathetereinlage INDENT cMRI vom 09.12.2016 (InselSpital Bern): Multiple infra- und supratentorielle Läsionen bei bekannter MS. Keine Schrankenstörung. Keine Anhaltspunkte für eine PML; keine Voruntersuchungen zum Vergleich vorliegend. INDENT MRI Neuroachse vom 30.06.2017 (InselSpital Bern): Stationäre Läsionslast supra- und infratentoriell sowie im zervikalen Myelon ohne Aktivitätszeichen. Keine Anhaltspunkte für eine PML. INDENT cMRI und MRI HWS vom 08.12.2017 (InselSpital Bern): idem zu 06/2017 INDENT cMRI vom 24.08.2018 (InselSpital Bern): stabiler Befund mit supra- und infratentoriellen Läsionen ohne KM-Aufnahme und ohne H.a. PML  therapeutisch: INDENT Cortisonstoss 03/2010 (UAW: agrressiv, insomnent) INDENT Gilenya 01/2014-04/2015 (Wechsel bei erneutem Schub) INDENT 04/2015-05/2018 Tysabri; insgesamt ca. 33 Infusionen (Wechsel bei Progredienz) INDENT Ocrevus 08.05. und 24.05.2018 1. Zyklus, 27.11.2018 2. Zyklus erfolgt symptomatisch:  INDENT Baclofenpumpe seit 16.08.2016 (sukzessive Steigerung bis 145 µg/d 12/2017, reduziert auf aktuell 125 µg/d) INDENT St.n. Sativex (UAW 'Flash' beim Autofahren), Sirdalud (keine Wirkung), Lioresal (keine Wirkung und Durchfälle) INDENT Neuroreha 08/2016 und Neuroreha 05/2018 (Klinik Bethesda); zuletzt nur passagere Besserung INDENT TENS seit 08/2018 INDENT Hanftropfen  Rezidivierende Synkopen unklarer Ätiologie INDENT anamnestisch: insgesamt 10-12 Synkopen seit der Schulzeit in Überforderungssituationen; zuletzt nach OPS 05/2016 im Liegen nach Einsatz der Probepumpe, keine Stürze oder Verletzungen Keratoplastik rechts, 2002\n",
      "\n",
      "\n",
      "Entry:  47\n",
      "Label secondary_progressive_multiple_sclerosis\n",
      "Predicted relapsing_remitting_multiple_sclerosis\n",
      "Text:  Schubförmige Multiple Sklerose (ED 1990), EDSS 6.5, V.a. sekundär-progredienten Verlauf INDENT aktuell: seit Ende 03/2018 aufgrund schwerer Belastungssituation am Arbeitsplatz zunehmende schmerzhafte Krämpfe aller Extremitäten rechts- und beinbetont, Zunahme der Beinschwäche, zusätzlich Gedächtnis- und Konzentrationsstörungen, am Eintrittstag bei allgemeinem Schwächegefühl nicht mehr aufstehen können INDENT anamnestisch: Gehstrecke zuvor 150 m mit Gehhilfe, vermehrte Sturzanfälligkeit, Harninkontinenz, leichte Fatigue-Symptomatik INDENT klinisch: rechts- und beinbetonte spastische Tetraparese, Gang- und Standataxie, Sensibilitätsminderung der rechten Extremität, neurogene Blasenfunktionsstörung  INDENT Verlauf: INDENT EM: Müdigkeit schon als Kind INDENT ED 1990: beide Beine plötzlich gelähmt INDENT 05/2009: deutliche Verschlechterung der vorbestehenden Gang- und Standunsicherheit durch starken Schwankschwindel INDENT 2010-2011: Gehhilfe INDENT 2012: Valens INDENT 2016: Gehstrecke 150m mit Gehhilfe INDENT 11/2016: Schub mit linksseitiger sensomotorischer Hemisymptomatik INDENT 2017: Valens INDENT 06/2017: letzter Schub ausgelöst durch Kieferhöhlenentzündung INDENT Diagnostik: INDENT 1990: Lumbalpunktion (Dr. Bevermeier) INDENT 03/2015 MRI spinal: T2-Läsionen im Myelon auf Höhe C1, C3, und C5/6, bei C3 und C5/6 ist das Myelon zusätzlich atroph, im STIR-Bild zeigen sich weitere Signalveränderungen im hochthorakalen Myelon, keine KM-Aufnahme INDENT 11/2016 cMRI: Nachweis von multiplen Signalveränderungen im Gehirn, passend zu Entmarkungen im Rahmen der bekannten MS, keine KM-Anreicherung, Diffusionsstörungen kortikal frontal rechts ohne ADC-Senkung, dese Diffusionsstörung ist bereits am 23.01.2014 in gleicher Art und Weise sichtbar gewesen, normal weite intrakrankielle Liquorräume, keine Veränderugen zu 23.01.2014 INDENT 01/2018: HBV-, HCV- und HIV Serologie negativ INDENT MRI Neuroachse vom 25.05.2018: leichte Progredienz der intrazerebralen MS-Plaques im Centrum semiovale beidseitig, im Pons, im Vermis links und am Forceps major rechts, keine PML, deutliche Beteiligung des gesamten mitabgebildeten Myelons wobei gegenüber der Voruntersuchung MR-HWS keine wesentliche Progredienz besteht, auf Höhe HWK 4 leicht zunehmende Signalintensität und auf Höhe BWK 4 auf neu aufgetretene Plaques, segmental geringe Myelonatrophie INDENT Therapie: INDENT Betainterferon INDENT 2009: Avonex (nach wenigen Tagen wegen UAW abgesetzt) INDENT 2012: Copaxone (nach wenigen Tagen wegen UAW abgesetzt) INDENT 2013-2016: mit Unterbrechungen Tecfidera (UAW Übelkeit, Schwindel, Grippesymptomatik) INDENT zwischen 2015 und 2015 6x Kortisonstosstherapie INDENT seit 02/2018: Rituximab 375 mg/kg KG (07.02.2018: 656 mg) INDENT 05/2018 Umstellung vom 4-Amynopiridin auf Fampyra 20 mg/d\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv(os.path.join(paths.DATA_PATH_PREPROCESSED,'ms-diag/ms-diag_clean_test.csv'))\n",
    "for i, label in enumerate(labels):\n",
    "    if label != test_preds[i]:\n",
    "        print(\"Entry: \", i)\n",
    "        print(\"Label\", id2label[label])\n",
    "        print(\"Predicted\", id2label[test_preds[i]])\n",
    "        print(\"Text: \", df[\"test\"][i][\"text\"])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "# config = PeftConfig.from_pretrained(paths.MODEL_PATH/'peft_llama2_msdiag')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class PromptLearningConfig(PeftConfig):\n",
    "    \"\"\"\n",
    "    This is the base configuration class to store the configuration of [`PrefixTuning`], [`PromptEncoder`], or\n",
    "    [`PromptTuning`].\n",
    "\n",
    "    Args:\n",
    "        num_virtual_tokens (`int`): The number of virtual tokens to use.\n",
    "        token_dim (`int`): The hidden embedding dimension of the base transformer model.\n",
    "        num_transformer_submodules (`int`): The number of transformer submodules in the base transformer model.\n",
    "        num_attention_heads (`int`): The number of attention heads in the base transformer model.\n",
    "        num_layers (`int`): The number of layers in the base transformer model.\n",
    "    \"\"\"\n",
    "\n",
    "    num_virtual_tokens: int = field(default=None, metadata={\"help\": \"Number of virtual tokens\"})\n",
    "    token_dim: int = field(\n",
    "        default=None, metadata={\"help\": \"The hidden embedding dimension of the base transformer model\"}\n",
    "    )\n",
    "    num_transformer_submodules: Optional[int] = field(\n",
    "        default=None, metadata={\"help\": \"Number of transformer submodules\"}\n",
    "    )\n",
    "    num_attention_heads: Optional[int] = field(default=None, metadata={\"help\": \"Number of attention heads\"})\n",
    "    num_layers: Optional[int] = field(default=None, metadata={\"help\": \"Number of transformer layers\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "configuratio interpretation:\n",
    "- num_virtual_tokens: number of prefix tokens to append to (each layer?)\n",
    "- num_transformer_submodules: 1 or 2? for decoder/encoder? or is it number of layers? I think decoder encoder architecture because if it is not set, they set it to 1 except in a Seq2Seq task. And they create embeddings for the virtual tokens only once for each submodule?\n",
    "- num_layers: to how many layers we should add the prefix tuning?\n",
    "- The peft model will automatically try to infer all the model specification from model.config in the `get_peft_model()` call. It will also automatically construct the PEFT Model class from the task type: return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model, peft_config, adapter_name=adapter_name)\n",
    "- you can add multiple adapters (must be all of same peft type though) to a mode each with their own configurations. You must give them unique names, otherwise gives name \"default\". Each prompt encoding and the whole process will be done for every single adapter I think. It first prepares the peft config by matching non specified elements with the model config. It then sets up the prompt encoder (_set_up_prompt_encoder(adapter_name)) using the created config for the adapter\n",
    "- If specification is PROMPT_TUNING we just create a trainable word embeddings matrix. By inserting an init text you can initialize the embedding from the base tokenizer and base model. \n",
    "- I think the prompt_token ids are initialized separately from the existing ids for the tokenizer. Because we have a different embedding matrix we can initialize from 0 to 19 say and then put it through, then concatenate the weights again to the embeddings of the base model without the virtual tokens. As they don't have any meaning and we will always have the same order of them (first will be virtual token 0, then 1 etc.) we also don't really neew to match them in the forward pass order will always be same.\n",
    "\n",
    "``` python\n",
    "if config.num_transformer_submodules is None:\n",
    "            config.num_transformer_submodules = 2 if config.task_type == TaskType.SEQ_2_SEQ_LM else 1\n",
    "total_virtual_tokens = config.num_virtual_tokens * config.num_transformer_submodules\n",
    "```\n",
    "\n",
    "```\n",
    "def _prepare_prompt_learning_config(peft_config, model_config):\n",
    "    if peft_config.num_layers is None:\n",
    "        if \"num_hidden_layers\" in model_config:\n",
    "            num_layers = model_config[\"num_hidden_layers\"]\n",
    "        elif \"num_layers\" in model_config:\n",
    "            num_layers = model_config[\"num_layers\"]\n",
    "        elif \"n_layer\" in model_config:\n",
    "            num_layers = model_config[\"n_layer\"]\n",
    "        else:\n",
    "            raise ValueError(\"Please specify `num_layers` in `peft_config`\")\n",
    "        peft_config.num_layers = num_layers\n",
    "\n",
    "    if peft_config.token_dim is None:\n",
    "        if \"hidden_size\" in model_config:\n",
    "            token_dim = model_config[\"hidden_size\"]\n",
    "        elif \"n_embd\" in model_config:\n",
    "            token_dim = model_config[\"n_embd\"]\n",
    "        elif \"d_model\" in model_config:\n",
    "            token_dim = model_config[\"d_model\"]\n",
    "        else:\n",
    "            raise ValueError(\"Please specify `token_dim` in `peft_config`\")\n",
    "        peft_config.token_dim = token_dim\n",
    "\n",
    "    if peft_config.num_attention_heads is None:\n",
    "        if \"num_attention_heads\" in model_config:\n",
    "            num_attention_heads = model_config[\"num_attention_heads\"]\n",
    "        elif \"n_head\" in model_config:\n",
    "            num_attention_heads = model_config[\"n_head\"]\n",
    "        elif \"num_heads\" in model_config:\n",
    "            num_attention_heads = model_config[\"num_heads\"]\n",
    "        elif \"encoder_attention_heads\" in model_config:\n",
    "            num_attention_heads = model_config[\"encoder_attention_heads\"]\n",
    "        else:\n",
    "            raise ValueError(\"Please specify `num_attention_heads` in `peft_config`\")\n",
    "        peft_config.num_attention_heads = num_attention_heads\n",
    "\n",
    "    if getattr(peft_config, \"encoder_hidden_size\", None) is None:\n",
    "        setattr(peft_config, \"encoder_hidden_size\", peft_config.token_dim)\n",
    "\n",
    "    return peft_config\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prefix tuning forward:\n",
    "- The prefixes are inserted into the model using the past_key_values which is the cached keys and values of the whole sequence before the current token in the autoregressive generation procedure. Instead of computing these keys and values with a model forward pass, we simply compute them directly with our MLP model. This is also why we need the number of attention heads (as we need a pair for every head).\n",
    "- The prompt encoder returns num_layers * 2 * token_dim past key values, so for every layer a key and value. Reading this one could think only \"one token\" (one key value pair) is generated. But later this is reshaped into:\n",
    "\n",
    "past_key_values = past_key_values.view(\n",
    "                batch_size,\n",
    "                peft_config.num_virtual_tokens,\n",
    "                peft_config.num_layers * 2,\n",
    "                peft_config.num_attention_heads,\n",
    "                peft_config.token_dim // peft_config.num_attention_heads,\n",
    "            )\n",
    "\n",
    "so we again have num_virtual_tokens as a dimension. This matrix is then permuted and split which I don't know the specifics of but it ensures that it is in the correct format.\n",
    "\n",
    "def _prefix_tuning_forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        batch_size = _get_batch_size(input_ids, inputs_embeds)\n",
    "        past_key_values = self.get_prompt(batch_size)\n",
    "        fwd_params = list(inspect.signature(self.base_model.forward).parameters.keys())\n",
    "        kwargs.update(\n",
    "            {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"inputs_embeds\": inputs_embeds,\n",
    "                \"output_attentions\": output_attentions,\n",
    "                \"output_hidden_states\": output_hidden_states,\n",
    "                \"return_dict\": return_dict,\n",
    "                \"past_key_values\": past_key_values,\n",
    "            }\n",
    "        )\n",
    "        if \"past_key_values\" in fwd_params:\n",
    "            return self.base_model(labels=labels, **kwargs)\n",
    "        else:\n",
    "            transformer_backbone_name = self.base_model.get_submodule(self.transformer_backbone_name)\n",
    "            fwd_params = list(inspect.signature(transformer_backbone_name.forward).parameters.keys())\n",
    "            if \"past_key_values\" not in fwd_params:\n",
    "                raise ValueError(\"Model does not support past key values which are required for prefix tuning.\")\n",
    "            outputs = transformer_backbone_name(**kwargs)\n",
    "            pooled_output = outputs[1] if len(outputs) > 1 else outputs[0]\n",
    "            if \"dropout\" in [name for name, _ in list(self.base_model.named_children())]:\n",
    "                pooled_output = self.base_model.dropout(pooled_output)\n",
    "            logits = self.base_model.get_submodule(self.cls_layer_name)(pooled_output)\n",
    "\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                if self.config.problem_type is None:\n",
    "                    if self.base_model.num_labels == 1:\n",
    "                        self.config.problem_type = \"regression\"\n",
    "                    elif self.base_model.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                        self.config.problem_type = \"single_label_classification\"\n",
    "                    else:\n",
    "                        self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "                if self.config.problem_type == \"regression\":\n",
    "                    loss_fct = MSELoss()\n",
    "                    if self.base_model.num_labels == 1:\n",
    "                        loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                    else:\n",
    "                        loss = loss_fct(logits, labels)\n",
    "                elif self.config.problem_type == \"single_label_classification\":\n",
    "                    loss_fct = CrossEntropyLoss()\n",
    "                    loss = loss_fct(logits.view(-1, self.base_model.num_labels), labels.view(-1))\n",
    "                elif self.config.problem_type == \"multi_label_classification\":\n",
    "                    loss_fct = BCEWithLogitsLoss()\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            if not return_dict:\n",
    "                output = (logits,) + outputs[2:]\n",
    "                return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "            return SequenceClassifierOutput(\n",
    "                loss=loss,\n",
    "                logits=logits,\n",
    "                hidden_states=outputs.hidden_states,\n",
    "                attentions=outputs.attentions,\n",
    "            )\n",
    "\n",
    "def get_prompt(self, batch_size: int, task_ids: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the virtual prompts to use for Peft. Only applicable when using a prompt learning method.\n",
    "        \"\"\"\n",
    "        peft_config = self.active_peft_config\n",
    "        prompt_encoder = self.prompt_encoder[self.active_adapter]\n",
    "        prompt_tokens = (\n",
    "            self.prompt_tokens[self.active_adapter]\n",
    "            .unsqueeze(0)\n",
    "            .expand(batch_size, -1)\n",
    "            .to(prompt_encoder.embedding.weight.device)\n",
    "        )\n",
    "        if peft_config.peft_type == PeftType.PREFIX_TUNING:\n",
    "            prompt_tokens = prompt_tokens[:, : peft_config.num_virtual_tokens]\n",
    "            if peft_config.inference_mode:\n",
    "                past_key_values = prompt_encoder.embedding.weight.repeat(batch_size, 1, 1)\n",
    "            else:\n",
    "                past_key_values = prompt_encoder(prompt_tokens)\n",
    "            if self.base_model_torch_dtype is not None:\n",
    "                past_key_values = past_key_values.to(self.base_model_torch_dtype)\n",
    "            past_key_values = past_key_values.view(\n",
    "                batch_size,\n",
    "                peft_config.num_virtual_tokens,\n",
    "                peft_config.num_layers * 2,\n",
    "                peft_config.num_attention_heads,\n",
    "                peft_config.token_dim // peft_config.num_attention_heads,\n",
    "            )\n",
    "            if peft_config.num_transformer_submodules == 2:\n",
    "                past_key_values = torch.cat([past_key_values, past_key_values], dim=2)\n",
    "            past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(\n",
    "                peft_config.num_transformer_submodules * 2\n",
    "            )\n",
    "            if TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING.get(self.config.model_type, None) is not None:\n",
    "                post_process_fn = TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING[self.config.model_type]\n",
    "                past_key_values = post_process_fn(past_key_values)\n",
    "            return past_key_values\n",
    "        else:\n",
    "            if peft_config.peft_type == PeftType.MULTITASK_PROMPT_TUNING:\n",
    "                prompts = prompt_encoder(prompt_tokens, task_ids)\n",
    "            else:\n",
    "                if peft_config.inference_mode:\n",
    "                    prompts = prompt_encoder.embedding.weight.repeat(batch_size, 1, 1)\n",
    "                else:\n",
    "                    prompts = prompt_encoder(prompt_tokens)\n",
    "            return prompts\n",
    "\n",
    "\n",
    "class PrefixEncoder(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    The `torch.nn` model to encode the prefix.\n",
    "\n",
    "    Args:\n",
    "        config ([`PrefixTuningConfig`]): The configuration of the prefix encoder.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    ```py\n",
    "    >>> from peft import PrefixEncoder, PrefixTuningConfig\n",
    "\n",
    "    >>> config = PrefixTuningConfig(\n",
    "    ...     peft_type=\"PREFIX_TUNING\",\n",
    "    ...     task_type=\"SEQ_2_SEQ_LM\",\n",
    "    ...     num_virtual_tokens=20,\n",
    "    ...     token_dim=768,\n",
    "    ...     num_transformer_submodules=1,\n",
    "    ...     num_attention_heads=12,\n",
    "    ...     num_layers=12,\n",
    "    ...     encoder_hidden_size=768,\n",
    "    ... )\n",
    "    >>> prefix_encoder = PrefixEncoder(config)\n",
    "    ```\n",
    "\n",
    "    **Attributes**:\n",
    "        - **embedding** (`torch.nn.Embedding`) -- The embedding layer of the prefix encoder.\n",
    "        - **transform** (`torch.nn.Sequential`) -- The two-layer MLP to transform the prefix embeddings if\n",
    "          `prefix_projection` is `True`.\n",
    "        - **prefix_projection** (`bool`) -- Whether to project the prefix embeddings.\n",
    "\n",
    "    Input shape: (`batch_size`, `num_virtual_tokens`)\n",
    "\n",
    "    Output shape: (`batch_size`, `num_virtual_tokens`, `2*layers*hidden`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.prefix_projection = config.prefix_projection\n",
    "        token_dim = config.token_dim\n",
    "        num_layers = config.num_layers\n",
    "        encoder_hidden_size = config.encoder_hidden_size\n",
    "        num_virtual_tokens = config.num_virtual_tokens\n",
    "        if self.prefix_projection and not config.inference_mode:\n",
    "            # Use a two-layer MLP to encode the prefix\n",
    "            self.embedding = torch.nn.Embedding(num_virtual_tokens, token_dim)\n",
    "            self.transform = torch.nn.Sequential(\n",
    "                torch.nn.Linear(token_dim, encoder_hidden_size),\n",
    "                torch.nn.Tanh(),\n",
    "                torch.nn.Linear(encoder_hidden_size, num_layers * 2 * token_dim),\n",
    "            )\n",
    "        else:\n",
    "            self.embedding = torch.nn.Embedding(num_virtual_tokens, num_layers * 2 * token_dim)\n",
    "\n",
    "    def forward(self, prefix: torch.Tensor):\n",
    "        if self.prefix_projection:\n",
    "            prefix_tokens = self.embedding(prefix)\n",
    "            past_key_values = self.transform(prefix_tokens)\n",
    "        else:\n",
    "            past_key_values = self.embedding(prefix)\n",
    "        return past_key_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
