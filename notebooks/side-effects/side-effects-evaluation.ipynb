{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "from src import paths\n",
    "\n",
    "from src.utils import get_default_pydantic_model, SideEffect, SideEffectList\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import auc, roc_curve, precision_recall_curve, average_precision_score\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "default_model = get_default_pydantic_model(\"side_effects\")\n",
    "model = BertModel.from_pretrained(os.path.join(paths.MODEL_PATH, \"medbert-512\")).to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(os.path.join(paths.MODEL_PATH, \"medbert-512\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity(ground_truth:str, prediction:str, model:BertModel, tokenizer:BertTokenizer):\n",
    "    \"\"\"\n",
    "    Calculate if the cosine similarity between the ground truth and the prediction is higher than the threshhold.\n",
    "\n",
    "    Args:\n",
    "        ground_truth (str): The ground truth text.\n",
    "        prediction (str): The predicted text.\n",
    "        model (BertModel): BertModel object to calculate the cosine similarity.\n",
    "        tokenizer (BertTokenizer): BertTokenizer object to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the cosine similarity is higher than the threshhold, False otherwise.\n",
    "    \"\"\"\n",
    "    prediction = prediction.split(\",\")\n",
    "    prediction = [\", \".join(tup) for tup in list(permutations(prediction))]\n",
    "    ground_truth = [ground_truth]*len(prediction)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ground_truth = model(**tokenizer(ground_truth, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device))[\"last_hidden_state\"].to(\"cpu\")\n",
    "        prediction = model(**tokenizer(prediction, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device))[\"last_hidden_state\"].to(\"cpu\")\n",
    "        ground_truth = ground_truth.mean(dim=1)\n",
    "        prediction = prediction.mean(dim=1)\n",
    "    \n",
    "    similarity = cosine_similarity(ground_truth, prediction)\n",
    "    max_similarity = np.max(similarity)\n",
    "\n",
    "    return max_similarity\n",
    "\n",
    "def preprocess_group(grouped_df: pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess a group of rows (grouped by obs index) from the dataframe. The preprocessing consists of:\n",
    "    - Fusing all the unique side effects per medication with \",\" (so only one row per medication is left)\n",
    "    - Splitting the medications by \"/\" if necessary\n",
    "\n",
    "    Args:\n",
    "        grouped_df (pd.DataFrame): A group of rows from the dataframe.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with the group preprocessed.\n",
    "    \"\"\"\n",
    "\n",
    "    df = []\n",
    "    \n",
    "    # group by medication name\n",
    "    for med, group in grouped_df.groupby(\"medication\"):\n",
    "       # Fuse all the unique side effects with \",\"\n",
    "        side_effects = group[\"side_effect\"].unique()\n",
    "        side_effects = \", \".join(side_effects)\n",
    "        text = group[\"text\"].iloc[0]\n",
    "        original_text = group[\"original_text\"].iloc[0]\n",
    "        index = group[\"index\"].iloc[0]\n",
    "\n",
    "        new_row = {\"medication\": med, \"side_effect\": side_effects, \"text\": text, \"original_text\": original_text, \"index\": index, \"successful\": group[\"successful\"].iloc[0]}\n",
    "        # Append the new row to the dataframe\n",
    "        df.append(new_row)\n",
    "\n",
    "    return pd.DataFrame(df)\n",
    "\n",
    "def prepare_results(path: str)->pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Prepare the results from the model to be compared with the labels. The preprocessing consists of:\n",
    "    - Fixing the model answers, replacing them with the default model answer if they are invalid (in regards to JSON structure).\n",
    "    - Grouping the results by index and preprocessing the group. (See preprocess_group() function for more details.)\n",
    "    - Lowercasing and converting everything to string.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the results file.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with the results preprocessed.\n",
    "    \"\"\"\n",
    "    results = torch.load(path)\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    df[\"successful\"] = True\n",
    "\n",
    "    # Fix model_answers\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            # Does model answer have key \"side_effects\"?\n",
    "            answer = json.loads(row[\"model_answers\"])\n",
    "\n",
    "            # Is answer a valid SideEffectList?\n",
    "            SideEffectList(**answer)\n",
    "\n",
    "            # Is the element an empty list?\n",
    "            if len(answer[\"side_effects\"]) == 0:\n",
    "                raise ValueError(\"Empty list\")\n",
    "            \n",
    "            # Are all the elements valid SideEffect?\n",
    "            for med in answer[\"side_effects\"]:\n",
    "                SideEffect(**med)\n",
    "            \n",
    "            df.at[idx, \"model_answers\"] = json.dumps(answer)\n",
    "\n",
    "            \n",
    "        except:\n",
    "            df.at[idx, \"model_answers\"] = default_model.model_dump_json()\n",
    "            df.at[idx, \"successful\"] = False\n",
    "    \n",
    "    dfs = []\n",
    "    for idx, row in df.iterrows():\n",
    "        row = row.to_dict()\n",
    "        answer = json.loads(row[\"model_answers\"])\n",
    "        medications = answer[\"side_effects\"]\n",
    "        for med in medications:\n",
    "            med[\"text\"] = row.get(\"text\", \"\")\n",
    "            med[\"original_text\"] = row.get(\"original_text\", \"\")\n",
    "            med[\"index\"] = row.get(\"index\", \"\")\n",
    "            med[\"successful\"] = row.get(\"successful\", \"\")\n",
    "            dfs.append((med))\n",
    "\n",
    "    res = pd.DataFrame(dfs)\n",
    "\n",
    "    res = res.sort_values(by=\"index\")\n",
    "\n",
    "    # Group by index and preprocess\n",
    "    _dfs = []\n",
    "\n",
    "    for index, group in res.groupby(\"index\"):\n",
    "        _dfs.append(preprocess_group(group))\n",
    "\n",
    "    res = pd.concat(_dfs)\n",
    "\n",
    "    # Convert everything to string and lowercase\n",
    "    res = res.map(lambda x: str(x).lower())\n",
    "\n",
    "    return res\n",
    "\n",
    "def prepare_labels(path: str)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare the labels to be compared with the results. The preprocessing consists of:\n",
    "    - Lowercasing and converting everything to string.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the labels file.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        pd.DataFrame: A dataframe with the labels preprocessed.\n",
    "    \"\"\"\n",
    "    labels = pd.read_excel(path) \n",
    "\n",
    "    # Convert everything to string and lowercase\n",
    "    labels = labels.map(lambda x: str(x).lower())\n",
    "\n",
    "    return labels\n",
    "\n",
    "def calculate_precision_recall_f1(ground_truth:list[str], predicted:list[str])->tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculate precision and recall from two lists of strings, allowing for partial string matches.\n",
    "    A string does not have to be exactly the same to be considered a match, it is enough if one string is a substring of the other.\n",
    "    This function is used to evaluate the medication names.\n",
    "\n",
    "    Args:\n",
    "        ground_truth (list[str]): List of ground truth medication names\n",
    "        predicted (list[str]): List of predicted medication names\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, float, float]: A tuple with the precision, recall and f1 score.\n",
    "    \"\"\"\n",
    "    # Convert lists to sets for easier comparison\n",
    "    ground_truth_set = set(ground_truth)\n",
    "    predicted_set = set(predicted)\n",
    "\n",
    "    # Initialize counters\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "\n",
    "    # Calculate True Positives and False Positives\n",
    "    for pred_med in predicted_set:\n",
    "        found_match = False\n",
    "        for truth_med in ground_truth_set:\n",
    "            if pred_med in truth_med or truth_med in pred_med:\n",
    "                true_positives += 1\n",
    "                found_match = True\n",
    "                break\n",
    "        if not found_match:\n",
    "            false_positives += 1\n",
    "\n",
    "    # Calculate False Negatives\n",
    "    false_negatives = len(ground_truth_set) - true_positives\n",
    "\n",
    "    # Calculate Precision\n",
    "    if true_positives + false_positives == 0:\n",
    "        precision = 0  # Handle division by zero\n",
    "    else:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "\n",
    "    # Calculate Recall\n",
    "    if true_positives + false_negatives == 0:\n",
    "        recall = 0  # Handle division by zero\n",
    "    else:\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    # Calculate F1 Score\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def string_match(ground_truth: list[dict], predicted: list[dict], model: BertModel, tokenizer: BertTokenizer, threshold: float = 0.9) -> tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculate if the strings match (based on cosine similarity) from two lists of dictionaries.\n",
    "    \"\"\"\n",
    "    tp = 0  # True Positives\n",
    "\n",
    "    # Copy the ground truth and predicted lists to avoid modifying the original lists\n",
    "    remaining_gt = ground_truth.copy()\n",
    "    matched_gt_indices = set()\n",
    "\n",
    "    for pred in predicted:\n",
    "        for gt_index, gt in enumerate(remaining_gt):\n",
    "            if gt_index in matched_gt_indices:\n",
    "                # Skip already matched ground truth items\n",
    "                continue\n",
    "            \n",
    "            if pred[\"medication\"] in gt[\"medication\"] or gt[\"medication\"] in pred[\"medication\"]:\n",
    "                if get_cosine_similarity(pred[\"side_effect\"], gt[\"side_effect\"], model, tokenizer) > threshold:\n",
    "                    tp += 1\n",
    "                    matched_gt_indices.add(gt_index)\n",
    "                    break\n",
    "\n",
    "\n",
    "    precision = tp/len(predicted)\n",
    "    recall = tp/len(ground_truth)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def create_metrics_df(labels_path:str, results_path:str, *args, **kwargs)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a dataframe with the metrics for each index. The metrics are:\n",
    "    - Precision, Recall and F1 score for medication names\n",
    "    - Precision, Recall and F1 score for side effects\n",
    "\n",
    "    Args:\n",
    "        labels_path (str): Path to the labels file.\n",
    "        results_path (str): Path to the results file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with the metrics for each index.\n",
    "    \"\"\"\n",
    "    \n",
    "    labels = prepare_labels(labels_path)\n",
    "    res = prepare_results(results_path)\n",
    "    model = kwargs.get(\"model\")\n",
    "    tokenizer = kwargs.get(\"tokenizer\")\n",
    "    threshold = kwargs.get(\"threshold\", 0.9)\n",
    "\n",
    "    dfs = []\n",
    "    for index, group in res.groupby(\"index\"):\n",
    "        text = group[\"text\"].iloc[0]\n",
    "        original_text = group[\"original_text\"].iloc[0]\n",
    "        index = group[\"index\"].iloc[0]\n",
    "        preds_med = group[\"medication\"].to_list()\n",
    "        preds_se = group[\"side_effect\"].to_list()\n",
    "        labels_med = labels[labels[\"index\"] == index][\"medication\"].to_list()\n",
    "        labels_se = labels[labels[\"index\"] == index][\"side_effect\"].to_list()\n",
    "\n",
    "        # Side effects\n",
    "        ground_truth = labels[labels[\"index\"] == index].to_dict(orient=\"records\")\n",
    "        predicted = group.to_dict(orient=\"records\")\n",
    "        assert model is not None and tokenizer is not None, \"Model and tokenizer must be provided as keyword arguments for side effects evaluation.\"\n",
    "        string_match(ground_truth, predicted, model, tokenizer, threshold=threshold)\n",
    "        side_effects_precision, side_effects_recall, side_effects_f1 = string_match(ground_truth, predicted, model, tokenizer, threshold=threshold)\n",
    "\n",
    "        # Medication\n",
    "        ground_truth = labels[labels[\"index\"] == index][\"medication\"].to_list()\n",
    "        predicted = group[\"medication\"].to_list()\n",
    "        medication_precision, medication_recall, medication_f1 = calculate_precision_recall_f1(ground_truth, predicted)\n",
    "\n",
    "        \n",
    "        if group[\"successful\"].iloc[0] == \"false\":\n",
    "            successful = False\n",
    "        else:\n",
    "            successful = True\n",
    "\n",
    "        dfs.append({\"text\": text, \n",
    "                    \"original_text\": original_text, \n",
    "                    \"index\": index, \n",
    "                    \"preds_med\": preds_med,\n",
    "                    \"labels_med\": labels_med,\n",
    "                    \"preds_se\": preds_se,\n",
    "                    \"labels_se\": labels_se,\n",
    "                    \"successful\": successful,\n",
    "                    \"side_effects_precision\": side_effects_precision, \"side_effects_recall\": side_effects_recall, \"side_effects_f1\": side_effects_f1, \"medication_precision\": medication_precision, \"medication_recall\": medication_recall, \"medication_f1\": medication_f1})\n",
    "\n",
    "    return pd.DataFrame(dfs)\n",
    "\n",
    "def create_agg_metrics_df(labels_path:str, results_path:str, *args, **kwargs)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a dataframe with the aggregated metrics for the whole dataset. The metrics are:\n",
    "    - Precision, Recall and F1 score for medication names\n",
    "    - Precision, Recall and F1 score for side effects\n",
    "\n",
    "    Args:\n",
    "        labels_path (str): Path to the labels file.\n",
    "        results_path (str): Path to the results file.\n",
    "        scorer (BERTScorer): BERTScorer object to score the similarity between the side effects.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with the aggregated metrics for the whole dataset.\n",
    "    \"\"\"\n",
    "    metrics = create_metrics_df(labels_path, results_path, *args, **kwargs)\n",
    "\n",
    "    # Calculate the average of the metrics\n",
    "    agg_metrics = metrics.agg({\"side_effects_precision\": \"mean\", \"side_effects_recall\": \"mean\", \"side_effects_f1\": \"mean\", \"medication_precision\": \"mean\", \"medication_recall\": \"mean\", \"medication_f1\": \"mean\", \"successful\": \"sum\"})\n",
    "\n",
    "    return agg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(paths:list[str], labels:str, *args, **kwargs)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarize the results from the model. The summary consists of:\n",
    "    - Precision, Recall and F1 score for medication names\n",
    "    - Precision, Recall and F1 score for side effects\n",
    "    - Aggregated metrics for the whole dataset\n",
    "\n",
    "    Args:\n",
    "        paths (list): List of paths to the results files.\n",
    "        labels (str): Path to the labels file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with the summary of the results.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for path in paths:\n",
    "        metrics = create_agg_metrics_df(labels, path, *args, **kwargs)\n",
    "        path = str(path)\n",
    "        if path.endswith(\"rag.pt\"):\n",
    "            metrics[\"approach\"] = \"S2A-1\"\n",
    "        elif path.endswith(\"s2a.pt\"):\n",
    "            metrics[\"approach\"] = \"S2A-2\"\n",
    "        else:\n",
    "            metrics[\"approach\"] = \"Base\"\n",
    "        \n",
    "        if \"few_shot_vanilla\" in path:\n",
    "            metrics[\"strategy\"] = \"Few-Shot Base\"\n",
    "        elif \"few_shot_instruction\" in path:\n",
    "            metrics[\"strategy\"] = \"Few-Shot Instruction\"\n",
    "        elif \"zero_shot_vanilla\" in path:\n",
    "            metrics[\"strategy\"] = \"Zero-Shot Base\"\n",
    "        elif \"zero_shot_instruction\" in path:\n",
    "            metrics[\"strategy\"] = \"Zero-Shot Instruction\"\n",
    "        else:\n",
    "            ValueError(\"Unknown strategy\")\n",
    "        dfs.append(metrics)\n",
    "    return pd.DataFrame(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 13B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_vanilla.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot instruction\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_instruction.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_vanilla_10_examples.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_10_examples.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_vanilla_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot instruction\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_instruction_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_vanilla_10_examples_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_10_examples_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_vanilla_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot instruction\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_instruction_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_vanilla_10_examples_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Instruction\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_10_examples_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latex_metrics(metrics:pd.DataFrame)->str:\n",
    "    \"\"\"\n",
    "    Convert the metrics dataframe to a latex table.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for strat, group in metrics.groupby(\"strategy\"):\n",
    "        cols = [col for col in metrics.columns if col not in [\"strategy\", \"approach\"]]\n",
    "        df_base = group[group[\"approach\"] == \"Base\"][cols].reset_index(drop=True)\n",
    "        df_base.columns = [f\"{col}_base\" for col in cols]\n",
    "        df_rag = group[group[\"approach\"] == \"S2A-1\"][cols].reset_index(drop=True)\n",
    "        df_rag.columns = [f\"{col}_rag\" for col in cols]\n",
    "        df_s2a = group[group[\"approach\"] == \"S2A-2\"][cols].reset_index(drop=True)\n",
    "        df_s2a.columns = [f\"{col}_s2a\" for col in cols]\n",
    "        df = pd.concat([df_base, df_rag, df_s2a], axis=1)\n",
    "        df[\"strategy\"] = strat\n",
    "        dfs.append(df)\n",
    "\n",
    "    return pd.concat(dfs, axis=0)\n",
    "\n",
    "def p_r_f_data(results: pd.DataFrame):\n",
    "    thesis13b = latex_metrics(results)\n",
    "    thesis13b_precision = thesis13b[[\"strategy\"] + [col for col in thesis13b.columns if \"precision\" in col]]\n",
    "    thesis13b_recall = thesis13b[[\"strategy\"] + [col for col in thesis13b.columns if \"recall\" in col]]\n",
    "    thesis13b_f1 = thesis13b[[\"strategy\"] + [col for col in thesis13b.columns if \"f1\" in col]]\n",
    "    return thesis13b_precision, thesis13b_recall, thesis13b_f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_vanilla.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_instruction.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_vanilla_10_examples.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_10_examples.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_vanilla_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_instruction_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_vanilla_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_vanilla_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_zero_shot_instruction_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_vanilla_10_examples_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_10_examples_s2a.pt\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = summarize(file_paths, paths.RESULTS_PATH/\"side-effects/labels.xlsx\", model=model, tokenizer=tokenizer, threshold = 0.78)\n",
    "results = results.round(2)\n",
    "# results.to_csv(paths.RESULTS_PATH/\"side-effects/summary13b.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesis13b_precision,thesis13b_recall, thesis13b_f1 =  p_r_f_data(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thesis13b = latex_metrics(results)\n",
    "# thesis13b_precision = thesis13b[[\"strategy\"] + [col for col in thesis13b.columns if \"precision\" in col]]\n",
    "# thesis13b_recall = thesis13b[[\"strategy\"] + [col for col in thesis13b.columns if \"recall\" in col]]\n",
    "# thesis13b_f1 = thesis13b[[\"strategy\"] + [col for col in thesis13b.columns if \"f1\" in col]]\n",
    "\n",
    "# thesis13b_precision.to_csv(paths.RESULTS_PATH/\"side-effects/thesis13b_precision.csv\", index=False)\n",
    "# thesis13b_recall.to_csv(paths.RESULTS_PATH/\"side-effects/thesis13b_recall.csv\", index=False)\n",
    "# thesis13b_f1.to_csv(paths.RESULTS_PATH/\"side-effects/thesis13b_f1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model seems to have difficulties if there is no medication mentioned with side effects. The tuned model does not have this problem I think, show example\n",
    "- also split medications that contain \"/\" into two lists\n",
    "- concatenate the side effects for the same medications with \",\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_vanilla.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot instruction\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_instruction.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_vanilla_10_examples.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_instruction_10_examples.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_vanilla_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot instruction\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_instruction_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_vanilla_10_examples_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_instruction_10_examples_rag.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_vanilla_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot instruction\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_instruction_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Vanilla\n",
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_vanilla_10_examples_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_agg_metrics_df(paths.RESULTS_PATH/\"side-effects/labels.xlsx\", paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_instruction_10_examples_s2a.pt\", model=model, tokenizer=tokenizer, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths7b = [paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_vanilla.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_instruction.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_vanilla_10_examples.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_instruction_10_examples.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_vanilla_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_instruction_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_vanilla_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_instruction_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_vanilla_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_zero_shot_instruction_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_vanilla_10_examples_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-7b_4bit_few_shot_instruction_10_examples_s2a.pt\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results7b = summarize(file_paths7b, paths.RESULTS_PATH/\"side-effects/labels.xlsx\", model=model, tokenizer=tokenizer, threshold = 0.9)\n",
    "results7b = results7b.round(2)\n",
    "# results7b.to_csv(paths.RESULTS_PATH/\"side-effects/summary7b.csv\", index=False)\n",
    "# results7b.to_csv(paths.THESIS_PATH/\"se_summary7b.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesis7b = latex_metrics(results7b)\n",
    "thesis7b_precision = thesis7b[[\"strategy\"] + [col for col in thesis7b.columns if \"precision\" in col]]\n",
    "thesis7b_recall = thesis7b[[\"strategy\"] + [col for col in thesis7b.columns if \"recall\" in col]]\n",
    "thesis7b_f1 = thesis7b[[\"strategy\"] + [col for col in thesis7b.columns if \"f1\" in col]]\n",
    "\n",
    "# thesis7b_precision.to_csv(paths.RESULTS_PATH/\"side-effects/thesis7b_precision.csv\", index=False)\n",
    "# thesis7b_recall.to_csv(paths.RESULTS_PATH/\"side-effects/thesis7b_recall.csv\", index=False)\n",
    "# thesis7b_f1.to_csv(paths.RESULTS_PATH/\"side-effects/thesis7b_f1.csv\", index=False)\n",
    "\n",
    "thesis7b_precision.to_csv(paths.THESIS_PATH/\"se_thesis7b_precision.csv\", index=False)\n",
    "thesis7b_recall.to_csv(paths.THESIS_PATH/\"se_thesis7b_recall.csv\", index=False)\n",
    "thesis7b_f1.to_csv(paths.THESIS_PATH/\"se_thesis7b_f1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 13B Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths13b_lora_1024 = [\n",
    "                paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_zero_shot_vanilla.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_zero_shot_instruction.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_few_shot_vanilla_10_examples.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_few_shot_instruction_10_examples.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_zero_shot_vanilla_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_zero_shot_instruction_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_few_shot_vanilla_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_few_shot_instruction_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_zero_shot_vanilla_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_zero_shot_instruction_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_few_shot_vanilla_10_examples_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_few_shot_instruction_10_examples_s2a.pt\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results13b_lora_1024 = summarize(file_paths13b_lora_1024, paths.RESULTS_PATH/\"side-effects/labels.xlsx\", model=model, tokenizer=tokenizer, threshold = 0.78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results13b_lora_1024 = results13b_lora_1024.round(2)\n",
    "results13b_lora_1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesis13b_1024_precision,thesis13b_1024_recall, thesis13b_1024_f1 =  p_r_f_data(results13b_lora_1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths13b_lora_512 = [\n",
    "                paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_zero_shot_vanilla.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_zero_shot_instruction.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_few_shot_vanilla_10_examples.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_few_shot_instruction_10_examples.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_zero_shot_vanilla_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_zero_shot_instruction_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_few_shot_vanilla_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_few_shot_instruction_10_examples_rag.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_zero_shot_vanilla_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_zero_shot_instruction_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_few_shot_vanilla_10_examples_s2a.pt\",\n",
    "              paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_few_shot_instruction_10_examples_s2a.pt\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results13b_lora_512 = summarize(file_paths13b_lora_512, paths.RESULTS_PATH/\"side-effects/labels.xlsx\", model=model, tokenizer=tokenizer, threshold = 0.78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results13b_lora_512 = results13b_lora_512.round(2)\n",
    "results13b_lora_512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesis13b_512_precision,thesis13b_512_recall, thesis13b_512_f1 =  p_r_f_data(results13b_lora_512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Thesis\n",
    "Concat results of 13B models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_13b_precision = pd.concat([thesis13b_precision, thesis13b_512_precision, thesis13b_1024_precision])\n",
    "overall_13b_precision[\"model\"] = [\"base\"] * len(thesis13b_precision) + [\"512\"] * len(thesis13b_precision) + [\"1024\"] * len(thesis13b_precision)\n",
    "overall_13b_recall = pd.concat([thesis13b_recall, thesis13b_512_recall, thesis13b_1024_recall])\n",
    "overall_13b_recall[\"model\"] = [\"base\"] * len(thesis13b_recall) + [\"512\"] * len(thesis13b_recall) + [\"1024\"] * len(thesis13b_recall)\n",
    "overall_13b_f1 = pd.concat([thesis13b_f1, thesis13b_512_f1, thesis13b_1024_f1])\n",
    "overall_13b_f1[\"model\"] = [\"base\"] * len(thesis13b_f1) + [\"512\"] * len(thesis13b_f1) + [\"1024\"] * len(thesis13b_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_13b_f1[[\"instruction\" in strat.lower() for strat in overall_13b_f1[\"strategy\"]]].to_csv(paths.THESIS_PATH/\"se_lora_f1.csv\")\n",
    "overall_13b_precision[[\"instruction\" in strat.lower() for strat in overall_13b_precision[\"strategy\"]]].to_csv(paths.THESIS_PATH/\"se_lora_precision.csv\")\n",
    "overall_13b_recall[[\"instruction\" in strat.lower() for strat in overall_13b_recall[\"strategy\"]]].to_csv(paths.THESIS_PATH/\"se_lora_recall.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_13b_f1[[\"instruction\" in strat.lower() for strat in overall_13b_f1[\"strategy\"]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold\n",
    "Will compare effects of threshold for few shot instruction s2a on results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(paths.RESULTS_PATH/\"side-effects/similarities.xlsx\")\n",
    "df[\"cosine_sim\"] = df.apply(lambda x: get_cosine_similarity(x[\"gt\"], x[\"pred\"], model, tokenizer), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "# Calculate best threshold\n",
    "fpr, tpr, thresholds = roc_curve(df[\"Similar\"], df[\"cosine_sim\"])\n",
    "\n",
    "# Calculate using the Youden's J statistic; sensitivity + specificity - 1, sensitivity = tpr, specificity = 1 - fpr\n",
    "optimal_idx_y = np.argmax(tpr - fpr)\n",
    "optimal_threshold_y = thresholds[optimal_idx_y]\n",
    "\n",
    "# Calculate using shortest distance to top left\n",
    "dist = np.sqrt(fpr**2 + (1-tpr)**2)\n",
    "optimal_idx_d = np.argmin(dist)\n",
    "optimal_threshold_d = thresholds[optimal_idx_d]\n",
    "\n",
    "# Calculate by not exceeding 0.1 for fpr\n",
    "optimal_idx_f = np.abs(fpr - 0.1).argmin()\n",
    "optimal_threshold_f = thresholds[optimal_idx_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_score, title:str = \"ROC Curve\", save_dir:str=None):\n",
    "    \"\"\"\n",
    "    Plot the ROC curve.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): True labels.\n",
    "        y_score (np.ndarray): Predicted scores.\n",
    "        title (str): Title of the plot.\n",
    "        save_dir (str): Path to save the plot.\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Use viridis palette for ROC curve - for example, at the 0.75 mark of the palette\n",
    "    roc_color = cm.viridis(0.75)\n",
    "    plt.plot(fpr, tpr, color=roc_color, lw=3, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    \n",
    "    # Use viridis palette for the diagonal line - for example, at the 0.25 mark of the palette\n",
    "    diagonal_color = cm.viridis(0.25)\n",
    "    plt.plot([0, 1], [0, 1], color=diagonal_color, lw=3, linestyle='--')\n",
    "\n",
    "    # Optimal threshold\n",
    "    plt.axvline(x=fpr[optimal_idx_d], color=\"blue\", linestyle=\"--\", label=\"Balanced Threshold\")\n",
    "    plt.axvline(x=fpr[optimal_idx_f], color=\"green\", linestyle=\"--\", label=\"FPR < 0.1 Threshold\")\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=20, fontweight='bold', labelpad=10)\n",
    "    plt.ylabel('True Positive Rate', fontsize=20, fontweight='bold', labelpad=15)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    if save_dir:\n",
    "        plt.savefig(save_dir)\n",
    "    plt.show()\n",
    "\n",
    "def plot_prc_curve(y_true, y_score, title:str = \"PRC Curve\", save_dir:str=None):\n",
    "    \"\"\"\n",
    "    Plot the Precision-Recall curve.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): True labels.\n",
    "        y_score (np.ndarray): Predicted scores.\n",
    "        title (str): Title of the plot.\n",
    "        save_dir (str): Path to save the plot.\n",
    "    \"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_score)\n",
    "    prc_auc = average_precision_score(y_true, y_score)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Use viridis palette for PRC curve - for example, at the 0.75 mark of the palette\n",
    "    prc_color = cm.viridis(0.5)\n",
    "    plt.plot(recall, precision, color=prc_color, label='PRC curve (area = %0.2f)' % prc_auc, linewidth=3)\n",
    "\n",
    "    # Optimal threshold\n",
    "    closest_threshold_d = np.argmin(np.abs(thresholds - optimal_threshold_d))\n",
    "    closest_threshold_f = np.argmin(np.abs(thresholds - optimal_threshold_f))\n",
    "    plt.axvline(x=recall[closest_threshold_d], color=\"blue\", linestyle=\"--\", label=\"Balanced Threshold\")\n",
    "    plt.axvline(x=recall[closest_threshold_f], color=\"green\", linestyle=\"--\", label=\"FPR < 0.1 Threshold\")\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall', fontsize=20, fontweight='bold', labelpad=10)\n",
    "    plt.ylabel('Precision', fontsize=20, fontweight='bold', labelpad=15)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    if save_dir:\n",
    "        plt.savefig(save_dir)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(df[\"Similar\"], df[\"cosine_sim\"], title=\"\", save_dir=paths.THESIS_PATH/\"se_roc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prc_curve(df[\"Similar\"], df[\"cosine_sim\"], title=\"\", save_dir=paths.THESIS_PATH/\"se_prc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_threshold_y, optimal_threshold_d, optimal_threshold_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_files = [\n",
    "    paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_10_examples_s2a.pt\",\n",
    "    paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_few_shot_instruction_10_examples_s2a.pt\",\n",
    "    paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_few_shot_instruction_10_examples_s2a.pt\",\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 11)\n",
    "dfs = []\n",
    "for threshold in thresholds:\n",
    "    _df = summarize(threshold_files, paths.RESULTS_PATH/\"side-effects/labels.xlsx\", model=model, tokenizer=tokenizer, threshold = threshold)\n",
    "    _df[\"model\"] = [\"Llama2-MedTuned-13B\", \"Llama2-MedTuned-13B-lora-512\", \"Llama2-MedTuned-13B-lora-1024\"]\n",
    "    _df[\"threshold\"] = threshold\n",
    "    dfs.append(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thresholds = pd.concat(dfs, axis=0)\n",
    "df_thresholds.drop(columns=[\"successful\", \"medication_precision\", \"medication_recall\", \"medication_f1\",\t\"successful\",\"approach\", \"strategy\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line_plot_group(x:pd.Series, y:pd.Series, groups:pd.Series, title:str = \"Line Plot\",\n",
    "                             xlabel:str = \"Threshold\", ylabel:str = \"Recall\", save_dir:str=None):\n",
    "    \"\"\"\n",
    "    Plot a line plot with different color for group.\n",
    "\n",
    "    Args:\n",
    "        x (pd.Series): x values.\n",
    "        y (pd.Series): y values.\n",
    "        groups (pd.Series): group values.\n",
    "        title (str): Title of the plot.\n",
    "        color_palette (np.ndarray): Color palette to use.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(x) == len(y) == len(groups), \"Length of x, y and group must be the same.\"\n",
    "\n",
    "    unique_groups = groups.unique()\n",
    "\n",
    "    data = pd.DataFrame({\"x\": x, \"y\": y, \"groups\": groups})\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    sns.lineplot(data=data, x=\"x\", y=\"y\", hue=\"groups\", palette=\"viridis\")\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel, fontsize=20, fontweight='bold', labelpad=10)\n",
    "    plt.ylabel(ylabel, fontsize=20, fontweight='bold', labelpad=15)\n",
    "\n",
    "    # Optimal threshold\n",
    "    plt.axvline(x=optimal_threshold_d, color=\"blue\", linestyle=\"--\", label=\"Balanced Threshold\")\n",
    "    plt.axvline(x=optimal_threshold_f, color=\"green\", linestyle=\"--\", label=\"FPR < 0.1 Threshold\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    if save_dir:\n",
    "        plt.savefig(save_dir)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_line_plot_group(df_thresholds[\"threshold\"], df_thresholds[\"side_effects_recall\"], df_thresholds[\"model\"], title=\"\", ylabel=\"Recall\", xlabel=\"Threshold\", save_dir = paths.THESIS_PATH/\"se_threshold_recall.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_line_plot_group(df_thresholds[\"threshold\"], df_thresholds[\"side_effects_precision\"], df_thresholds[\"model\"], title=\"\", ylabel=\"Precision\", xlabel=\"Threshold\", save_dir = paths.THESIS_PATH/\"se_threshold_precision.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_line_plot_group(df_thresholds[\"threshold\"], df_thresholds[\"side_effects_f1\"], df_thresholds[\"model\"], title=\"\", ylabel=\"F1\", xlabel=\"Threshold\", save_dir = paths.THESIS_PATH/\"se_threshold_f1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(df, title=\"Precision-Recall Curve\", xlabel=\"Recall\", ylabel=\"Precision\", save_dir=None):\n",
    "    \"\"\"\n",
    "    Plot the Precision-Recall curve for different models.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing precision, recall, and model name.\n",
    "        title (str): Title of the plot.\n",
    "        xlabel (str): X-axis label.\n",
    "        ylabel (str): Y-axis label.\n",
    "        save_dir (str, optional): Directory path to save the plot. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Use seaborn lineplot to draw precision-recall curves for each model\n",
    "    sns.lineplot(data=df, x=\"side_effects_recall\", y=\"side_effects_precision\", hue=\"model\", palette=\"viridis\", style=\"model\", markers=True, dashes=False)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "\n",
    "    if save_dir:\n",
    "        plt.savefig(save_dir, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(df_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1024 = prepare_results(paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-1024-lora-merged_4bit_few_shot_instruction_10_examples_s2a.pt\")\n",
    "res_512 = prepare_results(paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b-512-lora-merged_4bit_few_shot_instruction_10_examples_s2a.pt\")\n",
    "res = prepare_results(paths.RESULTS_PATH/\"side-effects/side-effects_outlines_Llama2-MedTuned-13b_4bit_few_shot_instruction_10_examples_s2a.pt\")\n",
    "labels = prepare_labels(paths.RESULTS_PATH/\"side-effects/labels.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, data in res_1024.groupby(\"index\"):\n",
    "    if len(res_512[res_512[\"index\"] == index]) != len(data):\n",
    "        print(\"labels\")\n",
    "        display(labels[labels[\"index\"] == index])\n",
    "        print(\"1024\")\n",
    "        display(data)\n",
    "        print(\"res\")\n",
    "        display(res[res[\"index\"] == index])\n",
    "        print(\"512\")\n",
    "        display(res_512[res_512[\"index\"] == index])\n",
    "        print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance difference:\n",
    "- index 17: 1024 model understand better, when no medication is given like \"unerwnschte nebenwirkung ona arzneimittel oder droge\", to just put \"unknown\", meanwhile the other 2 repeated the examples from few-shot\n",
    "- index 22 another example: \"indent labor: crp <0.5 mg/ml, bsr 8 mm/h, nw leukozyten + thrombozyten\", also unknown\n",
    "\n",
    "Good examples:\n",
    "- index 3\n",
    "- index 13\n",
    "\n",
    "Bad examples:\n",
    "- index: 55; \"indent st.n. symptomatischer therapie mit fampyra (insgesamt 2 therapieversuche, gestoppt bei wirkungslosogkeit und i.r. der kardialen vorbefunden), therapieversuch mit modasomil/remeron (keine besserung), venlafaxin (bei nebenwirkungen gestoppt), sirdalud\n",
    "indent rebif 2002, abgesetzt 6 monate spter wegen unvertrglichkeit (grippale nebenwirkungen)\"; only venlafaxin and rebif have se (model extracts venlafaxin with unknown, misses rebif and puts all the other medications like fampyra, modasomil, remeron, sirdalud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_1024[res_1024[\"index\"] == \"3\"][res_1024.columns[:-4]].to_dict(orient = \"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_1024[res_1024[\"index\"] == \"13\"][res_1024.columns[:-4]].to_dict(orient = \"records\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_1024[res_1024[\"index\"] == \"22\"][res_1024.columns[:-4]].to_dict(orient = \"records\"))\n",
    "print(res_512[res_512[\"index\"] == \"22\"][res_512.columns[:-4]].to_dict(orient = \"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_1024[res_1024[\"index\"] == \"55\"][res_1024.columns[:-4]].to_dict(orient = \"records\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
