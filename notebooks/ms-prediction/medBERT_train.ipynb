{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, DataCollatorWithPadding, get_scheduler\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "\n",
    "from src import paths\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import evaluate \n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_artifical_data_for_label(label:str):\n",
    "    label_dict = {\n",
    "        \"rrms\": \"relapsing_remitting_multiple_sclerosis\",\n",
    "        \"ppms\": \"primary_progressive_multiple_sclerosis\",\n",
    "        \"spms\": \"secondary_progressive_multiple_sclerosis\"\n",
    "    }\n",
    "    generated_data = pd.read_csv(paths.DATA_PATH_PREPROCESSED/f'ms-diag/artificial_{label}.csv')\n",
    "    generated_data[\"labels\"] = label_dict[label]\n",
    "    generated_data = generated_data[[\"0\", \"labels\"]].rename(columns = {\"0\":\"text\"})\n",
    "\n",
    "    return generated_data\n",
    "\n",
    "def get_artifical_data_all():\n",
    "    artifical_data = []\n",
    "    for label in [\"rrms\", \"ppms\", \"spms\"]:\n",
    "        try: \n",
    "            artifical_data.append(get_artifical_data_for_label(label))\n",
    "        except:\n",
    "            print(f\"Could not find data for {label}\")\n",
    "    artifical_data = pd.concat(artifical_data)\n",
    "    artifical_data = Dataset.from_pandas(artifical_data).remove_columns('__index_level_0__')\n",
    "    return artifical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_files = {\"train\": \"ms-diag_clean_train.csv\", \"validation\": \"ms-diag_clean_val.csv\", \"test\": \"ms-diag_clean_test.csv\", \"augmented\": \"ms-diag_augmented.csv\"}\n",
    "df = load_dataset(os.path.join(paths.DATA_PATH_PREPROCESSED,'ms-diag'), data_files = data_files)\n",
    "df[\"train\"] = concatenate_datasets([df[\"augmented\"], df[\"train\"]])\n",
    "\n",
    "# Number of labels\n",
    "num_labels = len(set(df['train']['labels']))\n",
    "\n",
    "# Label to id\n",
    "label2id = {'primary_progressive_multiple_sclerosis': 0,\n",
    "            'relapsing_remitting_multiple_sclerosis': 1,\n",
    "            'secondary_progressive_multiple_sclerosis': 2}\n",
    "id2label = {v:k for k,v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run this cell if you want to download and fine-tune the model\n",
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# # Login to Hugging Face Hub as model is gated\n",
    "# notebook_login()\n",
    "\n",
    "# # Checkpoint\n",
    "# checkpoint = \"GerMedBERT/medbert-512\"\n",
    "\n",
    "# # Load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# # Save tokenizer\n",
    "# tokenizer.save_pretrained(paths.MODEL_PATH/'medbert')\n",
    "\n",
    "# # Load model for embedding\n",
    "# model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "# # Save model\n",
    "# model.save_pretrained(paths.MODEL_PATH/'medbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /mnt/c/Users/marc_/OneDrive/ETH/MSC_Thesis/inf-extr/resources/models/medbert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(paths.MODEL_PATH/'medbert')\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(paths.MODEL_PATH/'medbert', num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c73ed5a7384cffbb6994c2b0ce72ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_data(data):\n",
    "    \n",
    "    # Label\n",
    "    data['labels'] = [label2id[label] for label in data['labels']]\n",
    "\n",
    "    # Tokenize\n",
    "    # data[\"text\"] = [text[:256] for text in data[\"text\"]]\n",
    "    data = tokenizer(data['text'], padding=True, truncation=True, return_tensors='pt', max_length = 512)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Tokenize dataset\n",
    "dataset = df.map(prepare_data, batched=True, remove_columns=['rid', 'text', 'date'], batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "EPOCHS = 8\n",
    "BATCH_SIZE = 4\n",
    "TRAIN_STEPS = EPOCHS * len(dataset['train']) // BATCH_SIZE\n",
    "NUM_GRADIENT_ACCUMULATION_STEPS = 2\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Collator\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer, padding=True, return_tensors='pt')\n",
    "\n",
    "# Dataloader\n",
    "train_loader = DataLoader(dataset['train'], batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "val_loader = DataLoader(dataset['validation'], batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "test_loader = DataLoader(dataset['test'], batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "# Accelerator\n",
    "accelerator = Accelerator(mixed_precision='fp16')\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optim = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optim,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=TRAIN_STEPS\n",
    ")\n",
    "\n",
    "# Prepare with accelerator\n",
    "model, optim, train_loader, val_loader, test_loader = accelerator.prepare(\n",
    "    model, optim, train_loader, val_loader, test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/686 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 13%|█▎        | 86/686 [01:31<08:06,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: F1 score: 0.1851851851851852 Loss: 1.8994598388671875 Accuracy: 0.35714285714285715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 172/686 [02:48<07:16,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: F1 score: 0.2933333333333333 Loss: 1.0122718811035156 Accuracy: 0.7857142857142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 258/686 [03:43<04:05,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: F1 score: 0.30769230769230765 Loss: 0.8724145889282227 Accuracy: 0.8571428571428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 344/686 [04:43<02:33,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: F1 score: 0.30769230769230765 Loss: 1.1784780025482178 Accuracy: 0.8571428571428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 430/686 [05:35<02:09,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: F1 score: 0.30769230769230765 Loss: 1.2231240272521973 Accuracy: 0.8571428571428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 516/686 [06:25<01:32,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: F1 score: 0.30769230769230765 Loss: 0.8991823196411133 Accuracy: 0.8571428571428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 602/686 [07:12<00:47,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: F1 score: 0.30769230769230765 Loss: 0.9818661212921143 Accuracy: 0.8571428571428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "688it [08:02,  1.78it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: F1 score: 0.32 Loss: 0.9557268619537354 Accuracy: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "progress_bar = tqdm.tqdm(range(TRAIN_STEPS))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        optim.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss/NUM_GRADIENT_ACCUMULATION_STEPS\n",
    "        accelerator.backward(loss)\n",
    "        if step % NUM_GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            optim.step()\n",
    "            lr_scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = []\n",
    "        labels = []\n",
    "        val_loss = 0\n",
    "        acc = 0\n",
    "        for batch in val_loader:\n",
    "            outputs = model(**batch)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            \n",
    "            val_loss += outputs.loss\n",
    "            acc += (predictions == batch['labels']).sum().item()\n",
    "            \n",
    "            preds.extend(predictions.tolist())\n",
    "            labels.extend(batch['labels'].tolist())\n",
    "\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    if epoch == 0:\n",
    "            min_val_loss = val_loss\n",
    "            largest_f1 = f1\n",
    "    elif (val_loss < min_val_loss):\n",
    "        min_val_loss = val_loss\n",
    "        model.save_pretrained(paths.MODEL_PATH/'ms_diag_medbert_valloss')\n",
    "    elif (largest_f1 < f1):\n",
    "        largest_f1 = f1\n",
    "        model.save_pretrained(paths.MODEL_PATH/'ms_diag_medbert_f1')\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: F1 score: {f1} Loss: {val_loss/len(val_loader)} Accuracy: {acc/len(dataset['validation'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
