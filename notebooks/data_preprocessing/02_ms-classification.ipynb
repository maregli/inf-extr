{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS Preprocessing\n",
    "!To get dataset for this task, first line classifier model has to be available and used to generate line labels on the reports in seantis_kisim.csv (corresponds to inference on the \"all\" split in the line-label/line-label_clean_dataset)!\n",
    "\n",
    "Necessary steps before this:\n",
    "1. notebooks/00_preprocessing_old_project.ipynb\n",
    "2. notebooks/01_classifying_text_lines.ipynb\n",
    "3. scripts/line-label/inference.py --model_name \"trained-line-classifier\" --split all\n",
    "\n",
    "The data/preprocessed/midatams/seantis_kisim.csv file was created by the original project (00_preprocessing_old_project.ipynb).\n",
    "This file contains the longest report per rid splitted into lines. Their approach:\n",
    "\n",
    "1. Extract the longest diagnosis per rid (most lines) from the csv and if the rid had a manually line labelled text, they used this instead.\n",
    "2. Results in dataset consisting of text lines per row with a label for the line.\n",
    "\n",
    "Further processing:\n",
    "\n",
    "3. Label this dataset with clf 1. Merge diagnoses.csv and line labelled dataset by rid. Clean the labels. Correct SPMS and PPMS labels that are wrong.\n",
    "4. Get a list of eligible rid (rids with text that have at least one dm line).\n",
    "5. Df1: concatenate all dm lines per eligible rid for the text.\n",
    "6. Df2: concatenate all text per eligible rid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "\n",
    "from src import paths\n",
    "from src.utils import ms_label2id, line_label_id2label\n",
    "\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line Labelled dataset Token Finetuned\n",
    "data = torch.load(paths.RESULTS_PATH/\"line-label/line-label_medbert-512_token_all.pt\")\n",
    "\n",
    "data_list = []\n",
    "for obs in data:\n",
    "    _df = pd.DataFrame(obs[\"text\"], columns=[\"text\"])\n",
    "    _df[\"class2\"] = obs[\"preds\"]\n",
    "    _df[\"rid\"] = obs[\"rid\"]\n",
    "    data_list.append(_df)\n",
    "\n",
    "data_token_df = pd.concat(data_list)\n",
    "\n",
    "data_token_df = data_token_df[[\"rid\", \"text\", \"class2\"]]\n",
    "\n",
    "\n",
    "# Make directory if it doesn't exist\n",
    "os.makedirs(paths.DATA_PATH_PREPROCESSED/\"ms-diag\", exist_ok=True)\n",
    "\n",
    "data_token_df.to_csv(paths.DATA_PATH_PREPROCESSED/\"ms-diag/line-label_medbert-512_token_finetuned_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line Labelled dataset Line Finetuned\n",
    "data_line = torch.load(paths.RESULTS_PATH/\"line-label/line-label_medbert-512_class_all.pt\")\n",
    "data_line.pop(\"last_hidden_state\")\n",
    "\n",
    "data_line_df = pd.DataFrame(data_line)\n",
    "data_line_df.rename(columns={\"preds\": \"class2\"}, inplace=True)\n",
    "data_line_df.drop(columns=[\"labels\"], inplace=True)\n",
    "data_line_df = data_line_df[[\"rid\", \"index_within_rid\", \"text\", \"class2\"]]\n",
    "\n",
    "# Remap values in class2 using ms_id2label\n",
    "data_line_df[\"class2\"] = data_line_df[\"class2\"].map(line_label_id2label)\n",
    "\n",
    "# Make directory if it doesn't exist\n",
    "os.makedirs(paths.DATA_PATH_PREPROCESSED/\"ms-diag\", exist_ok=True)\n",
    "\n",
    "data_line_df.to_csv(paths.DATA_PATH_PREPROCESSED/\"ms-diag/line-label_medbert-512_class_finetuned_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The line labelled will have more lines because no line truncation\n",
    "print(\"Length Line Data: \", len(data_line_df))\n",
    "print(\"Length Token Data: \", len(data_token_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_line_df[data_line_df[\"rid\"] == \"016B6D16-2BBA-4C05-A8E4-30F761C95813\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_token_df[data_token_df[\"rid\"] == \"016B6D16-2BBA-4C05-A8E4-30F761C95813\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing CLF1 Dataset for MS Diag\n",
    "\n",
    "- I decide to go with line classifier, even though it is a bit worse, but no truncation issues for longer reports\n",
    "- The steps are:\n",
    "\n",
    "1. Merging Datasets on rid (one row is one line in text)\n",
    "2. Cleaning up labels (only using confirmed diganoses, rewriting German labels, correcting wrong labels for small count classes)\n",
    "3. Construction of \"no_ms\" class: all lines labelled as something other than \"dm\" are \"no_ms\".\n",
    "4. Construct 2 datasets:\n",
    "    - df1: contains only single lines per row.\n",
    "    - df2: contains all text per rid. If at least one line was \"dm\" then label was set to the MS Type for this rid. If not \"no_ms\". More \"no_ms\" by creating texts, that don't contain the \"dm\" line. (So there might be multiple examples from same rid, one time with dm line, one time without.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line labelled dataset from classifier 1\n",
    "df_text = pd.read_csv(os.path.join(paths.DATA_PATH_PREPROCESSED, \"ms-diag\", \"line-label_medbert-512_class_finetuned_all.csv\"))[[\"rid\", \"text\", \"class2\", \"index_within_rid\"]]\n",
    "df_labels = pd.read_csv(os.path.join(paths.DATA_PATH_SEANTIS, \"diagnoses.csv\"))\n",
    "\n",
    "# In old approach they only used confirmed diagnosis\n",
    "df_labels = df_labels[df_labels[\"diagnosis_reliability\"] == \"confirmed\"]\n",
    "df_labels = df_labels[[\"research_id\", \"disease\"]].rename(columns={\"disease\": \"labels\", \"research_id\": \"rid\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with diagnoses.csv\n",
    "df_merged = pd.merge(df_text, df_labels, on=\"rid\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English Labels\n",
    "english_labels = set([\"relapsing_remitting_multiple_sclerosis\", \"secondary_progressive_multiple_sclerosis\", \"primary_progressive_multiple_sclerosis\"])\n",
    "other_labels = set(df_merged[\"labels\"].unique()) - english_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check non english labels\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "for rid, rid_data in df_merged.groupby(\"rid\"):\n",
    "    if rid_data.labels.isin(other_labels).any():\n",
    "        print(rid_data.labels.unique())\n",
    "        print(rid_data[\"text\"].str.cat(sep = \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remap non english labels if possible\n",
    "map_dict = {\n",
    "    \"Multiple Sklerose a.e. primär progredient\": \"primary_progressive_multiple_sclerosis\",\n",
    "    \"Multiple Sklerose mit a.e. primär-progredientem Verlauf\": \"primary_progressive_multiple_sclerosis\",\n",
    "    \"Schubförmig remittierende Multiple Sklerose (RRMS)\": \"relapsing_remitting_multiple_sclerosis\",\n",
    "}\n",
    "df_merged = df_merged.replace(map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all non english labels\n",
    "df_merged = df_merged[df_merged[\"labels\"].isin(english_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because mapping was done manually, check if label matches text for classes with low counts like SPMS\n",
    "for rid, rid_data in df_merged[df_merged[\"labels\"] == \"secondary_progressive_multiple_sclerosis\"].groupby(\"rid\"):\n",
    "    print(rid)\n",
    "    print(rid_data[\"text\"].str.cat(sep = \"\\n\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spms_wrong = [\"2A9F4832-B09D-470A-B05F-519854310DBB\",\n",
    "              \"39D432B0-902B-49D9-B727-12EDC053B09E\",\n",
    "              \"AF834D8D-F7DB-4B22-BB01-29F10EE6A828\",\n",
    "              \"B886879A-5109-46FD-A2B0-9DCA2DA733F8\",\n",
    "              \"C0784569-1E15-4FBE-A4B2-F9473975D199\"\n",
    "                ]\n",
    "df_merged[df_merged[\"labels\"] == \"secondary_progressive_multiple_sclerosis\"].rid.unique().shape\n",
    "# Because of this exclusion we end up with less training examples than in their original approach\n",
    "df_merged[(df_merged[\"labels\"] == \"relapsing_remitting_multiple_sclerosis\") & df_merged[\"text\"].str.lower().str.contains(\"spms|sekundär\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop entries with wrong label\n",
    "df_merged = df_merged[~df_merged[\"rid\"].isin(spms_wrong)]\n",
    "\n",
    "# Remap entry 157 to secondary_progressive_multiple_sclerosis\n",
    "df_merged.loc[157, \"labels\"] = \"secondary_progressive_multiple_sclerosis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check primary_progressive_multiple_sclerosis\n",
    "for rid, rid_data in df_merged[df_merged[\"labels\"] == \"primary_progressive_multiple_sclerosis\"].groupby(\"rid\"):\n",
    "    print(rid)\n",
    "    print(rid_data[\"text\"].str.cat(sep = \"\\n\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rids with diagnosis\n",
    "rids_dm = set(df_merged[df_merged[\"class2\"] == \"dm\"][\"rid\"].unique())\n",
    "\n",
    "# Rids without diagnosis\n",
    "rids_no_dm = set(df_merged[\"rid\"].unique()) - rids_dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set labels of rids without diagnosis to no_ms\n",
    "df_merged.loc[df_merged[\"rid\"].isin(rids_no_dm), \"labels\"] = \"no_ms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take all non dm lines and set label to no dm\n",
    "df1_no_dm = df_merged[df_merged[\"class2\"] != \"dm\"]\n",
    "df1_no_dm.loc[:, \"labels\"] = \"no_ms\"\n",
    "\n",
    "# For the rids in rids_dm, extract all lines with class2 == dm\n",
    "df1_dm = df_merged[df_merged[\"class2\"] == \"dm\"].groupby(\"rid\").agg({\"text\": \"\\n\".join, \"labels\": \"first\", \"index_within_rid\": \"first\"}).reset_index()\n",
    "\n",
    "# Concat both dataframes\n",
    "df1 = pd.concat([df1_no_dm, df1_dm]).drop(columns=[\"class2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the rids in rids_dm, extract all lines and keep original label\n",
    "df2 = df_merged.groupby(\"rid\").agg({\"text\": \"\\n\".join, \"labels\": \"first\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3 will be df2 but put first text line last\n",
    "df3 = df2.copy()\n",
    "df3[\"text\"] = df3[\"text\"].apply(lambda x: x.split(\"\\n\"))\n",
    "df3[\"text\"] = df3[\"text\"].apply(lambda x: x[1:] + [x[0]])\n",
    "df3[\"text\"] = df3[\"text\"].apply(lambda x: \"\\n\".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.labels.value_counts(), df3.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df1.rid.unique()), len(df2.rid.unique()), len(df3.rid.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Val Test split\n",
    "df2train, df2test = train_test_split(df2, test_size=0.3, random_state=42, stratify=df2[\"labels\"])\n",
    "df2train, df2val = train_test_split(df2train, test_size=0.1, random_state=42, stratify=df2train[\"labels\"])\n",
    "\n",
    "train_rids = set(df2train[\"rid\"].unique())\n",
    "val_rids = set(df2val[\"rid\"].unique())\n",
    "test_rids = set(df2test[\"rid\"].unique())\n",
    "\n",
    "df3train = df3[df3[\"rid\"].isin(train_rids)]\n",
    "df3val = df3[df3[\"rid\"].isin(val_rids)]\n",
    "df3test = df3[df3[\"rid\"].isin(test_rids)]\n",
    "\n",
    "df1train = df1[df1[\"rid\"].isin(train_rids)]\n",
    "df1val = df1[df1[\"rid\"].isin(val_rids)]\n",
    "df1test = df1[df1[\"rid\"].isin(test_rids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df1test.rid.unique()), len(df2test.rid.unique()), len(df3test.rid.unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HuggingFace Dataset\n",
    "def create_hf_dataset(train:pd.DataFrame, val:pd.DataFrame, test:pd.DataFrame):\n",
    "    \"\"\"Create HuggingFace Dataset from train, val and test dataframes. Remaps labels to ids and drops unnecessary columns.\n",
    "    \n",
    "    Args:\n",
    "        train (pd.DataFrame): Training dataframe\n",
    "        val (pd.DataFrame): Validation dataframe\n",
    "        test (pd.DataFrame): Test dataframe\n",
    "        \n",
    "        Returns:\n",
    "            DatasetDict: HuggingFace DatasetDict\n",
    "            \n",
    "    \"\"\"\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": Dataset.from_pandas(train),\n",
    "        \"val\": Dataset.from_pandas(val),\n",
    "        \"test\": Dataset.from_pandas(test),\n",
    "    })\n",
    "\n",
    "    # Map the labels to ids\n",
    "    dataset = dataset.map(lambda e: {\"labels\": [ms_label2id[l] for l in e[\"labels\"]]}, batched=True)\n",
    "\n",
    "    # Drop __index_level_0__ column\n",
    "    dataset = dataset.remove_columns([\"__index_level_0__\"])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset1 = create_hf_dataset(df1train, df1val, df1test)\n",
    "dataset2 = create_hf_dataset(df2train, df2val, df2test)\n",
    "dataset3 = create_hf_dataset(df3train, df3val, df3test)\n",
    "\n",
    "# Save the dataset\n",
    "dataset1.save_to_disk(os.path.join(paths.DATA_PATH_PREPROCESSED, \"ms-diag/ms_diag_line\"))\n",
    "dataset2.save_to_disk(os.path.join(paths.DATA_PATH_PREPROCESSED, \"ms-diag/ms_diag_all\"))\n",
    "dataset3.save_to_disk(os.path.join(paths.DATA_PATH_PREPROCESSED, \"ms-diag/ms_diag_all_first_line_last\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set no ms label\n",
    "\n",
    "To later evaluate the validity of the \"no_ms\" label I will manually check if the \"no_ms\" labels in the test set are correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_ms_text = dataset2[\"test\"].filter(lambda e: e[\"labels\"] == 3)[\"text\"]\n",
    "no_ms_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution\n",
    "print(\"Label distribution all:\")\n",
    "print(df2.labels.value_counts(), \"\\n\\n\")\n",
    "\n",
    "print(\"Label distribution all_first_line_last:\")\n",
    "print(df3.labels.value_counts(), \"\\n\\n\")\n",
    "\n",
    "print(\"Label distribution line:\")\n",
    "print(df1.labels.value_counts(), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Approach\n",
    "\n",
    "To get a fair comparison, I need to retrain the line-classifier with the test rids excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test rids in a file\n",
    "with open(os.path.join(paths.DATA_PATH_PREPROCESSED, \"ms-diag/test_rids.txt\"), \"w\") as f:\n",
    "    f.write(\"\\n\".join(test_rids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load line labelled dataset\n",
    "from src.utils import load_line_label_data\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "line_labels = load_line_label_data()\n",
    "line_labels_all = concatenate_datasets([line_labels[\"train\"], line_labels[\"val\"], line_labels[\"test\"]])\n",
    "\n",
    "# Create test set from line labelled dataset by using the test rids\n",
    "line_labels_test = line_labels_all.filter(lambda e: e[\"rid\"] in test_rids)\n",
    "\n",
    "# Remove test rids from line labelled dataset\n",
    "line_labels_all = line_labels_all.filter(lambda e: e[\"rid\"] not in test_rids)\n",
    "\n",
    "# Cast labels column to ClassLabel and split into train and test\n",
    "line_labels_all = line_labels_all.class_encode_column(\"labels\").train_test_split(test_size=0.1, shuffle=True, seed=42, stratify_by_column=\"labels\")\n",
    "\n",
    "# Assign correct splits\n",
    "line_labels_all[\"val\"] = line_labels_all[\"test\"]\n",
    "line_labels_all[\"test\"] = line_labels_test\n",
    "line_labels_all[\"all\"] = line_labels[\"all\"]\n",
    "\n",
    "# Save the dataset\n",
    "line_labels_all.save_to_disk(os.path.join(paths.DATA_PATH_PREPROCESSED, \"line-label/line-label_clean_dataset_pipeline\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting\n",
    "\n",
    "Following the task instruction, system prompt and examples for the ms type extraction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import zero_shot_base, zero_shot_instruction, few_shot_base, few_shot_instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_instruction = \"\"\"Your task is to extract the diagnosis corresponding to a type of multiple sclerosis (MS) stated in a German medical report. The input for this task is a German medical report, and the output should be the type of MS.\n",
    "There are three types of multiple sclerosis in German:\n",
    "- primär progrediente Multiple Sklerose (PPMS)\n",
    "- sekundär progrediente Multiple Sklerose (SPMS)\n",
    "- schubförmig remittierende Multiple Sklerose (RRMS)\n",
    "\n",
    "The type is provided in the text, and your task is to extract it. If you cannot match a type exactly, please answer with 'other'.\n",
    "Your answer should solely consist of one of the following:\n",
    "- primär progrediente Multiple Sklerose\n",
    "- sekundär progrediente Multiple Sklerose\n",
    "- schubförmige remittierende Multiple Sklerose\n",
    "- other\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.\n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not makeany sense, or is not factually coherent, explain why instead of answering something not correct. \n",
    "If you don’t know the answer to a question, please don’t share false information.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open(paths.DATA_PATH_PREPROCESSED/\"ms-diag/task_instruction.txt\", \"w\") as file:\n",
    "    file.write(task_instruction)\n",
    "\n",
    "with open(paths.DATA_PATH_PREPROCESSED/\"ms-diag/system_prompt.txt\", \"w\") as file:\n",
    "    file.write(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples\n",
    "\n",
    "ppms_text = dataset2[\"test\"].filter(lambda e: e[\"labels\"] == 0)[\"text\"][0][:200] + \"...\"\n",
    "rrms_text = dataset2[\"test\"].filter(lambda e: e[\"labels\"] == 1)[\"text\"][0][:200] + \"...\"\n",
    "spms_text = dataset2[\"test\"].filter(lambda e: e[\"labels\"] == 2)[\"text\"][0][:200] + \"...\"\n",
    "other_text = dataset2[\"test\"].filter(lambda e: e[\"labels\"] == 3)[\"text\"][0][:200] + \"...\"\n",
    "\n",
    "examples = [{\"text\": ppms_text, \"labels\": \"primär progrediente Multiple Sklerose\"},\n",
    "            {\"text\": rrms_text, \"labels\": \"schubförmige remittierende Multiple Sklerose\"},\n",
    "            {\"text\": spms_text, \"labels\": \"sekundär progrediente Multiple Sklerose\"},\n",
    "            {\"text\": other_text, \"labels\": \"other\"}]\n",
    "\n",
    "with open(paths.DATA_PATH_PREPROCESSED/\"ms-diag/examples.json\", \"w\") as file:\n",
    "    json.dump(examples, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
