{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade10451-dc2a-464d-87e1-f60956e08232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, DataCollatorWithPadding\n",
    "\n",
    "from datasets import DatasetDict, Features, Sequence, Value, load_dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "from src import paths\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0b3a89-5059-4d4e-a738-0502ccc1cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        for gpu_id in range(num_gpus):\n",
    "            free_mem, total_mem = torch.cuda.mem_get_info(gpu_id)\n",
    "            gpu_properties = torch.cuda.get_device_properties(gpu_id)\n",
    "            print(f\"GPU {gpu_id}: {gpu_properties.name}\")\n",
    "            print(f\"   Total Memory: {total_mem / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Free Memory: {free_mem / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Allocated Memory : {torch.cuda.memory_allocated(gpu_id) / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Reserved Memory : {torch.cuda.memory_reserved(gpu_id) / (1024 ** 3):.2f} GB\")\n",
    "    else:\n",
    "        print(\"No GPU available.\")\n",
    "# Call the function to check GPU memory\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1591e7e-7cb5-437d-9453-676879f2e417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low precision config\n",
    "print(\"Memory before Model is loaded:\\n\")\n",
    "check_gpu_memory()\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(paths.MODEL_PATH/'llama2-chat', device_map=\"auto\", torch_dtype=torch.float16)\n",
    "print(\"Memory after Model is loaded:\\n\")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dbd102-cee5-4aeb-90e2-d2929a1b3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(paths.MODEL_PATH/'llama2', padding_side='left')\n",
    "print(\"Vocabulary Size without Pad Token: \", len(tokenizer))\n",
    "\n",
    "# Check if the pad token is already in the tokenizer vocabulary\n",
    "if '<pad>' not in tokenizer.get_vocab():\n",
    "    # Add the pad token\n",
    "    tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "\n",
    "#Resize the embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#Configure the pad token in the model\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Check if they are equal\n",
    "assert model.config.pad_token_id == tokenizer.pad_token_id, \"The model's pad token ID does not match the tokenizer's pad token ID!\"\n",
    "\n",
    "# Print the pad token ids\n",
    "print('Tokenizer pad token ID:', tokenizer.pad_token_id)\n",
    "print('Model pad token ID:', model.config.pad_token_id)\n",
    "print('Model config pad token ID:', model.config.pad_token_id)\n",
    "print(\"Vocabulary Size with Pad Token: \", len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da05cbc-9b13-45da-a330-8470449bff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_files = {\"train\": \"ms-diag_clean_train.csv\", \"validation\": \"ms-diag_clean_val.csv\", \"test\": \"ms-diag_clean_test.csv\"}\n",
    "df = load_dataset(os.path.join(paths.DATA_PATH_PREPROCESSED,'ms-diag'), data_files = data_files)\n",
    "\n",
    "# Number of labels\n",
    "num_labels = len(set(df['train']['labels']))\n",
    "\n",
    "# Label to id\n",
    "label2id = {'primary_progressive_multiple_sclerosis': 0,\n",
    "            'relapsing_remitting_multiple_sclerosis': 1,\n",
    "            'secondary_progressive_multiple_sclerosis': 2}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "\n",
    "BASE_PROMPT = \"<s>[INST]\\n<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{user_prompt}[/INST]\\n\\n{answer_init}\"\n",
    "SYSTEM_PROMPT = \"Generiere einen fast identischen Text und behalte die genaue Diagnose der multiplen Sklerose bei.\"\n",
    "ANSWER_INIT = \"Generierung: \"\n",
    "\n",
    "def construct_prompt(class_id:int, truncation_size:int=300) -> list[str]:\n",
    "\n",
    "    def format_prompt(text:str)->str:\n",
    "        \"\"\"Truncates the text to the given truncation size and formats the prompt.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text\n",
    "        \n",
    "        Returns:\n",
    "            str: Returns the formatted prompt\n",
    "        \"\"\"\n",
    "        if len(text) > truncation_size:\n",
    "            text = text[:truncation_size]\n",
    "        else:\n",
    "            text = text\n",
    "        input = BASE_PROMPT.format(system_prompt = SYSTEM_PROMPT,\n",
    "                                user_prompt = text,\n",
    "                                answer_init = ANSWER_INIT)\n",
    "\n",
    "        return input\n",
    "    \n",
    "    prompts = df[\"train\"].filter(lambda example: example[\"labels\"] == id2label[class_id])[\"text\"]\n",
    "    prompts = [format_prompt(prompt) for prompt in prompts]\n",
    "\n",
    "    return prompts    \n",
    "\n",
    "# Default collate function \n",
    "collate_fn = DataCollatorWithPadding(tokenizer, padding=True) #padding=True, 'max_length'\n",
    "\n",
    "prompts_ppms = construct_prompt(0, truncation_size = 300)\n",
    "prompts_spms = construct_prompt(2, truncation_size = 300)\n",
    "\n",
    "dataloader_ppms = torch.utils.data.DataLoader(dataset=[tokenizer(example) for example in prompts_ppms], collate_fn=collate_fn, batch_size=2, shuffle = False)\n",
    "dataloader_spms = torch.utils.data.DataLoader(dataset=[tokenizer(example) for example in prompts_spms], collate_fn=collate_fn, batch_size=2, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d5efc-252a-48c9-a215-c590778da67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179a30f0-1642-4b25-937a-631e3edd8cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_words_ppms = [\"primäre multiple sklerose\", \"primär progressive multiple sklerose\", \"primär progrediente multiple sklerose\"]\n",
    "force_words_ids_ppms = [tokenizer(force_words_ppms, add_special_tokens=False)[\"input_ids\"]]\n",
    "force_words_spms = [\"sekundär progressive multiple sklerose\", \"sekundär progrediente multiple sklerose\"]\n",
    "force_words_ids_spms = [tokenizer(force_words_spms, add_special_tokens = False)[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd762a-fdce-4df2-8618-9410b4adba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tokenizer(prompts_spms[0], return_tensors = \"pt\")\n",
    "test = {k:v.to(device) for k, v in test.items()}\n",
    "test_ids = model.generate(**test, max_new_tokens = 128, temperature = 1, num_beams =2, do_sample = True, force_words_ids = force_words_ids_spms).to(\"cpu\")\n",
    "print(tokenizer.batch_decode(test_ids, skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e431e5-93ed-416c-86ad-7f59022a0304",
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62bc344-fc97-4c5d-be14-f17d3b2ba7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_samples = []\n",
    "\n",
    "for i in tqdm(range(20)):\n",
    "    for batch in dataloader_spms:\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        try:\n",
    "            generated_ids = model.generate(**batch, \n",
    "                                           max_new_tokens=128, \n",
    "                                           num_beams=2, \n",
    "                                           do_sample=True, \n",
    "                                           num_return_sequences=1, \n",
    "                                           temperature = 1, \n",
    "                                           top_p = 0.8,\n",
    "                                          force_words_ids = force_words_ids_spms).to(\"cpu\")\n",
    "            generated_samples.append(tokenizer.batch_decode(generated_ids, skip_special_tokens = True))\n",
    "        except:\n",
    "            continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4bd381-0bbb-4e44-ab34-48e7c11cd972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "outputs = list(chain.from_iterable(generated_samples))\n",
    "outputs = [text.split(\"[/INST]\\n\\nGenerierung: \")[1] for text in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8923f30d-2b91-439f-8cae-1119feea8d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f0c56-f42a-492d-ad9c-d37210c6a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.Series(outputs).to_csv(paths.DATA_PATH_PREPROCESSED/'ms-diag/artificial_spms.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
