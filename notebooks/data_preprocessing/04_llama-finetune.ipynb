{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised text data\n",
    "Before you can run this must run notebooks/data_preprocessing/05_side-effects to filter out test rids.\n",
    "\n",
    "Some of the preprocessing of old project will be used (like using INDENT for \"-\" etc.) to ensure comparability with rest of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "from src import paths\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test rids for side-effect task\n",
    "se = pd.read_csv(os.path.join(paths.DATA_PATH_PREPROCESSED, \"side-effects/kisim_diagnoses_combined_se_sample.csv\"))\n",
    "test_rids = se[\"rid\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nan values in text: 2\n"
     ]
    }
   ],
   "source": [
    "# Kisim diagnoses\n",
    "kisim_diagnoses = pd.read_csv(os.path.join(paths.DATA_PATH_SEANTIS, \"kisim_diagnoses.csv\")).rename(columns={\"diagnosis_label\": \"text\"})\n",
    "\n",
    "# Some text will be doubled in dataset as doctors just append, and there is now way to know from this data what text belongs to what.\n",
    "# LastUpdateDateTime is shared with first part of diagnosis id and then the second part is the indicator of the individual texts so 4085680|1 for example\n",
    "\n",
    "def filter_duplicate_entries(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    As text is sometimes just appended to a report, we need to find reports that start the same way and only keep the most recent/longest one.\n",
    "    Per default, this function considers the first 100 characters of the text as criteria for filtering.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFra me): The dataframe to filter. Must contain a column \"text\", \"research_id\" and \"LastUpdateDateTime\"\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The filtered dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort by LastUpdateDateTime\n",
    "    df = df.sort_values(\"LastUpdateDateTime\", ascending=False)\n",
    "\n",
    "    # Group by research_id and text_start\n",
    "    text_start = df[\"text\"].apply(lambda x: x[:100] if len(x) > 100 else x)\n",
    "    grouped = df.groupby([\"research_id\", text_start]).head(1)\n",
    "\n",
    "    return grouped\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the text by removing empty lines and adding 'INDENT' to lines starting with '-', '·', '··', and removing '·'\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to preprocess\n",
    "\n",
    "    Returns:\n",
    "        str: The preprocessed text\n",
    "    \"\"\"\n",
    "\n",
    "    text = text.split(\"\\n\")\n",
    "\n",
    "    # Remove empty strings and whitespace-only strings\n",
    "    try:\n",
    "        lines = [str(item) for item in text if not (not item or item.isspace())]\n",
    "\n",
    "    except:\n",
    "        print(text)\n",
    "    \n",
    "    # Add 'INDENT' to lines starting with '-', '·', '··', and remove '·'\n",
    "    lines = ['INDENT ' + item.replace('·', '') if item.startswith(('-', '·', '··')) else item for item in lines]\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def preprocess_df(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocesses the dataframe by removing nan values and filtering duplicate entries.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to preprocess\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The preprocessed dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of nan values in text\n",
    "    print(f\"Number of nan values in text: {df['text'].isna().sum()}\")\n",
    "    df.dropna(subset=[\"text\"], inplace=True)\n",
    "\n",
    "    df = filter_duplicate_entries(df)\n",
    "\n",
    "    df[\"text\"] = df[\"text\"].apply(preprocess_text)\n",
    "\n",
    "    return df\n",
    "\n",
    "kisim_diagnoses = preprocess_df(kisim_diagnoses).rename(columns={\"research_id\": \"rid\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nan values in text: 14\n"
     ]
    }
   ],
   "source": [
    "# Reports kisim diagnoses also contains more texts\n",
    "kisim_reports_diagnoses = pd.read_csv(os.path.join(paths.DATA_PATH_RSD, \"reports_kisim_diagnoses.csv\")).rename(columns={\"diagnosis_label\": \"text\"})\n",
    "kisim_reports_diagnoses = preprocess_df(kisim_reports_diagnoses).rename(columns={\"research_id\": \"rid\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 OP Medrol 32 mg \t1-0-0\n",
      "2 OP Keppra 500 mg\t3-0-3\n",
      "1 OP Urbanyl 10 mg \t0-0-0.5 noch für eine Woche\n",
      "1 OP Rivotril 0.5 mg\t1-1-0 noch für eine Woche, dann 1-1-1\n"
     ]
    }
   ],
   "source": [
    "# Medications, also contain useful text\n",
    "kisim_medications = pd.read_csv(os.path.join(paths.DATA_PATH_SEANTIS, \"kisim_medication.csv\")).rename(columns={\"medication_name\": \"text\", \"research_id\": \"rid\"})\n",
    "print(kisim_medications.iloc[0][\"text\"])\n",
    "kisim_medications = kisim_medications.dropna(subset=[\"text\"])\n",
    "kisim_medications = kisim_medications[['rid', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d766515fb145ae9cba39e37aed2a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/41149 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combine the text and rid three datasets\n",
    "df_all = pd.concat([kisim_diagnoses, kisim_reports_diagnoses], axis=0)\n",
    "\n",
    "# Drop double diagnoses ids\n",
    "df_all.drop_duplicates(subset=\"diagnosis_id\", inplace=True)\n",
    "\n",
    "# Keep only text and rid\n",
    "df_all = df_all[[\"text\", \"rid\"]]\n",
    "\n",
    "# Add the medications\n",
    "df_all = pd.concat([df_all, kisim_medications], axis=0)\n",
    "\n",
    "# Remove test rids\n",
    "df_all = df_all[~df_all[\"rid\"].isin(test_rids)]\n",
    "\n",
    "# Save the combined dataset\n",
    "os.makedirs(paths.DATA_PATH_PREPROCESSED/\"text-finetune\", exist_ok=True)\n",
    "df_all.to_csv(os.path.join(paths.DATA_PATH_PREPROCESSED, \"text-finetune/kisim_diagnoses_combined.csv\"), index=False)\n",
    "\n",
    "# Huggingface\n",
    "df = Dataset.from_dict({\"text\": df_all[\"text\"]})\n",
    "df.save_to_disk(os.path.join(paths.DATA_PATH_PREPROCESSED, \"text-finetune/kisim_diagnoses\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a169c5aa33c946acb77981d6822225f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41149 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Number of tokens\n",
    "df = Dataset.load_from_disk(os.path.join(paths.DATA_PATH_PREPROCESSED, \"text-finetune/kisim_diagnoses\"))\n",
    "\n",
    "# tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(paths.MODEL_PATH/\"Llama2-MedTuned-13b\")\n",
    "\n",
    "def tokenize(example):\n",
    "    example = tokenizer(example[\"text\"], \n",
    "                        add_special_tokens=False)\n",
    "    return example\n",
    "    \n",
    "\n",
    "df_tokenized = df.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training tokens:  4455141\n",
      "Number of reports:  41149\n"
     ]
    }
   ],
   "source": [
    "# Number of tokens\n",
    "num_tokens = [len(input) for input in df_tokenized[\"input_ids\"]]\n",
    "print(\"Number of training tokens: \", sum(num_tokens))\n",
    "print(\"Number of reports: \", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# I will adapt the code from original project, that also produced preprocessed data for previous tasks.\n",
    "# Then c\n",
    "\n",
    "def _get_diag_lines(diag):\n",
    "    \n",
    "    '''\n",
    "    get list of text lines for a kisim_diagnoses.csv 'diag_label' entry\n",
    "    \n",
    "    input:\n",
    "    - diag: str with text from one 'diag_label' entry\n",
    "    \n",
    "    ouptut:\n",
    "    - diag_lines: list of text lines\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    diag_lines = diag.splitlines()\n",
    "    diag_lines = [item for item in diag_lines if not (not item or item.isspace())]\n",
    "    diag_lines = ['INDENT ' + item.replace('·', '') if item.startswith(('-', '·', '··')) else item for item in diag_lines]\n",
    "    \n",
    "    return diag_lines\n",
    "\n",
    "\n",
    "def extract_longest_diag_per_rid(df, var_date, var_diag):\n",
    "    '''\n",
    "    function to extract longest lines of all diagnosis texts for each research id\n",
    "    \n",
    "    input:\n",
    "    - df: dataframe containing all information\n",
    "    - var_date: column name of date column\n",
    "    - var_diag: column name of diagnosis text column\n",
    "    \n",
    "    ouptut:\n",
    "    - dict_diags: dictionary of diagnosis text (key: research id-diag_index-date, value: list of text lines)   \n",
    "    '''\n",
    "    \n",
    "    dict_diags = dict()\n",
    "    list_vars = [var_date, var_diag]\n",
    "\n",
    "    for rid in df['research_id'].unique():\n",
    "\n",
    "        list_lines = list()\n",
    "        date = ''\n",
    "        _df = df[df['research_id'] == rid].sort_values([var_date])[list_vars]\n",
    "\n",
    "        for diag_index, row in _df.iterrows():\n",
    "\n",
    "            diag = row[var_diag]\n",
    "\n",
    "            if isinstance(diag, str):\n",
    "                \n",
    "                diag_lines = _get_diag_lines(diag)\n",
    "                \n",
    "                if len(diag_lines) >= len(list_lines):\n",
    "                    \n",
    "                    list_lines = diag_lines[:]\n",
    "                    date = str(row[var_date])[:11]\n",
    "                    \n",
    "        key = '_'.join((rid, str(diag_index), date))\n",
    "                \n",
    "        dict_diags[key] = list_lines\n",
    "\n",
    "    return dict_diags"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
