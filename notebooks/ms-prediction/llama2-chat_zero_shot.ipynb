{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, DataCollatorWithPadding\n",
    "\n",
    "from datasets import DatasetDict, Features, Sequence, Value, load_dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import gc\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "from src import paths\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model\n",
    "# checkpoint = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# # Save model and tokenizer\n",
    "# model.save_pretrained(paths.MODEL_PATH/'llama2-chat')\n",
    "# tokenizer.save_pretrained(paths.MODEL_PATH/'llama2-chat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b0fb214d684e0892c4a379c986c081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Low precision config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(paths.MODEL_PATH/'llama2-chat', device_map=\"auto\", quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size:  32000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(paths.MODEL_PATH/'llama2', padding_side='left')\n",
    "print(\"Vocabulary Size: \", len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad was not in Vocab\n",
      "Tokenizer pad token ID: 32000\n",
      "Model pad token ID: 32000\n",
      "Model config pad token ID: 32000\n"
     ]
    }
   ],
   "source": [
    "# Check if the pad token is already in the tokenizer vocabulary\n",
    "if '<pad>' not in tokenizer.get_vocab():\n",
    "    # Add the pad token\n",
    "    tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "\n",
    "#Resize the embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#Configure the pad token in the model\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Check if they are equal\n",
    "assert model.config.pad_token_id == tokenizer.pad_token_id, \"The model's pad token ID does not match the tokenizer's pad token ID!\"\n",
    "\n",
    "# Print the pad token ids\n",
    "print('Tokenizer pad token ID:', tokenizer.pad_token_id)\n",
    "print('Model pad token ID:', model.config.pad_token_id)\n",
    "print('Model config pad token ID:', model.config.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device of _orig_mod.model.embed_tokens.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.0.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.0.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.0.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.0.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.0.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.0.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.0.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.0.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.0.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.1.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.1.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.1.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.1.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.1.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.1.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.1.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.1.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.1.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.2.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.2.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.2.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.2.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.2.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.2.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.2.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.2.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.2.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.3.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.3.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.3.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.3.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.3.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.3.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.3.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.3.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.3.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.4.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.4.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.4.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.4.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.4.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.4.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.4.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.4.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.4.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.5.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.5.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.5.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.5.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.5.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.5.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.5.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.5.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.5.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.6.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.6.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.6.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.6.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.6.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.6.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.6.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.6.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.6.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.7.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.7.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.7.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.7.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.7.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.7.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.7.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.7.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.7.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.8.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.8.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.8.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.8.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.8.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.8.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.8.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.8.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.8.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.9.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.9.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.9.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.9.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.9.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.9.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.9.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.9.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.9.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.10.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.10.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.10.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.10.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.10.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.10.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.10.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.10.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.10.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.11.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.11.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.11.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.11.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.11.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.11.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.11.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.11.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.11.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.12.self_attn.q_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.12.self_attn.k_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.12.self_attn.v_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.12.self_attn.o_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.12.mlp.gate_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.12.mlp.up_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.12.mlp.down_proj.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.12.input_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.12.post_attention_layernorm.weight:  cuda:0\n",
      "Device of _orig_mod.model.layers.13.self_attn.q_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.13.self_attn.k_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.13.self_attn.v_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.13.self_attn.o_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.13.mlp.gate_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.13.mlp.up_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.13.mlp.down_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.13.input_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.13.post_attention_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.14.self_attn.q_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.14.self_attn.k_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.14.self_attn.v_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.14.self_attn.o_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.14.mlp.gate_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.14.mlp.up_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.14.mlp.down_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.14.input_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.14.post_attention_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.15.self_attn.q_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.15.self_attn.k_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.15.self_attn.v_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.15.self_attn.o_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.15.mlp.gate_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.15.mlp.up_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.15.mlp.down_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.15.input_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.15.post_attention_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.16.self_attn.q_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.16.self_attn.k_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.16.self_attn.v_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.16.self_attn.o_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.16.mlp.gate_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.16.mlp.up_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.16.mlp.down_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.16.input_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.16.post_attention_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.17.self_attn.q_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.17.self_attn.k_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.17.self_attn.v_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.17.self_attn.o_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.17.mlp.gate_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.17.mlp.up_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.17.mlp.down_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.17.input_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.17.post_attention_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.18.self_attn.q_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.18.self_attn.k_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.18.self_attn.v_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.18.self_attn.o_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.18.mlp.gate_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.18.mlp.up_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.18.mlp.down_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.18.input_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.18.post_attention_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.19.self_attn.q_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.19.self_attn.k_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.19.self_attn.v_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.19.self_attn.o_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.19.mlp.gate_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.19.mlp.up_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.19.mlp.down_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.19.input_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.19.post_attention_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.20.self_attn.q_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.20.self_attn.k_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.20.self_attn.v_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.20.self_attn.o_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.20.mlp.gate_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.20.mlp.up_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.20.mlp.down_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.20.input_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.20.post_attention_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.21.self_attn.q_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.21.self_attn.k_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.21.self_attn.v_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.21.self_attn.o_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.21.mlp.gate_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.21.mlp.up_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.21.mlp.down_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.21.input_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.21.post_attention_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.22.self_attn.q_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.22.self_attn.k_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.22.self_attn.v_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.22.self_attn.o_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.22.mlp.gate_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.22.mlp.up_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.22.mlp.down_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.22.input_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.22.post_attention_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.23.self_attn.q_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.23.self_attn.k_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.23.self_attn.v_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.23.self_attn.o_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.23.mlp.gate_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.23.mlp.up_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.23.mlp.down_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.23.input_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.23.post_attention_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.24.self_attn.q_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.24.self_attn.k_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.24.self_attn.v_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.24.self_attn.o_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.24.mlp.gate_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.24.mlp.up_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.24.mlp.down_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.24.input_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.24.post_attention_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.25.self_attn.q_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.25.self_attn.k_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.25.self_attn.v_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.25.self_attn.o_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.25.mlp.gate_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.25.mlp.up_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.25.mlp.down_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.25.input_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.25.post_attention_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.26.self_attn.q_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.26.self_attn.k_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.26.self_attn.v_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.26.self_attn.o_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.26.mlp.gate_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.26.mlp.up_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.26.mlp.down_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.26.input_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.26.post_attention_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.27.self_attn.q_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.27.self_attn.k_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.27.self_attn.v_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.27.self_attn.o_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.27.mlp.gate_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.27.mlp.up_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.27.mlp.down_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.27.input_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.27.post_attention_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.28.self_attn.q_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.28.self_attn.k_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.28.self_attn.v_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.28.self_attn.o_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.28.mlp.gate_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.28.mlp.up_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.28.mlp.down_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.28.input_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.28.post_attention_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.29.self_attn.q_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.29.self_attn.k_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.29.self_attn.v_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.29.self_attn.o_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.29.mlp.gate_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.29.mlp.up_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.29.mlp.down_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.29.input_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.29.post_attention_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.30.self_attn.q_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.30.self_attn.k_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.30.self_attn.v_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.30.self_attn.o_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.30.mlp.gate_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.30.mlp.up_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.30.mlp.down_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.30.input_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.30.post_attention_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.31.self_attn.q_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.31.self_attn.k_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.31.self_attn.v_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.31.self_attn.o_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.31.mlp.gate_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.31.mlp.up_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.31.mlp.down_proj.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.31.input_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.layers.31.post_attention_layernorm.weight:  cuda:1\n",
      "Device of _orig_mod.model.norm.weight:  cuda:1\n",
      "Device of _orig_mod.lm_head.weight:  cuda:1\n"
     ]
    }
   ],
   "source": [
    "# Check device allocation\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Device of {name}: \", param.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = \"<s>[INST]\\n<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{user_prompt}[/INST]\\nBased on the information provided in the text, the most likely diagnosis for the patient is: \"\n",
    "def get_classification_llama(text):\n",
    "    input = base_prompt.format(system_prompt = \"Is the MS diagnosis in the text of type \\\"Sekundär progrediente Multiple Sklerose (SPMS)\\\", \\\"primäre progrediente Multiple Sklerose (PPMS)\\\" or \\\"schubförmig remittierende Multiple Sklerose (RRMS)\\\"?\",\n",
    "                               user_prompt = text)\n",
    "    return input\n",
    "\n",
    "def preprocess(example):\n",
    "    example = tokenizer(get_classification_llama(example[\"text\"]), return_tensors =\"pt\")\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[INST]\\n<<SYS>>\\nIs the MS diagnosis in the text of type \"Sekundär progrediente Multiple Sklerose (SPMS)\", \"primäre progrediente Multiple Sklerose (PPMS)\" or \"schubförmig remittierende Multiple Sklerose (RRMS)\"?\\n<</SYS>>\\n\\ntext[/INST]\\nBased on the information provided in the text, the most likely diagnosis for the patient is: \\n\\n\"Sekundär progrediente Multiple Sklerose (SPMS)\"\\n', '[INST]\\n<<SYS>>\\nIs the MS diagnosis in the text of type \"Sekundär progrediente Multiple Sklerose (SPMS)\", \"primäre progrediente Multiple Sklerose (PPMS)\" or \"schubförmig remittierende Multiple Sklerose (RRMS)\"?\\n<</SYS>>\\n\\ntexttext[/INST]\\nBased on the information provided in the text, the most likely diagnosis for the patient is: \\n\\n\"Sekundär progrediente Multiple Sklerose (SPMS)\"\\n']\n",
      "['[INST]\\n<<SYS>>\\nIs the MS diagnosis in the text of type \"Sekundär progrediente Multiple Sklerose (SPMS)\", \"primäre progrediente Multiple Sklerose (PPMS)\" or \"schubförmig remittierende Multiple Sklerose (RRMS)\"?\\n<</SYS>>\\n\\ntexttexttext[/INST]\\nBased on the information provided in the text, the most likely diagnosis for the patient is: \\n\\n\"Sekundär progrediente Multiple Sklerose (SPMS)\"\\n', '[INST]\\n<<SYS>>\\nIs the MS diagnosis in the text of type \"Sekundär progrediente Multiple Sklerose (SPMS)\", \"primäre progrediente Multiple Sklerose (PPMS)\" or \"schubförmig remittierende Multiple Sklerose (RRMS)\"?\\n<</SYS>>\\n\\ntexttexttexttext[/INST]\\nBased on the information provided in the text, the most likely diagnosis for the patient is: \\n\\nSekundär progrediente Multiple Sklerose (SPMS)\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    #model_inputs = {k: v.to(torch.int64).to(\"cuda\") for k, v in model_inputs.items()}\n",
    "    input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "    attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "    with torch.inference_mode():\n",
    "        generated_ids = model.generate(input_ids = input_ids, attention_mask = attention_mask, max_new_tokens=20, num_beams=4, do_sample=True, temperature = 1, num_return_sequences = 1).to(\"cpu\")\n",
    "    print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_files = {\"train\": \"ms-diag_clean_train.csv\", \"validation\": \"ms-diag_clean_val.csv\", \"test\": \"ms-diag_clean_test.csv\"}\n",
    "df = load_dataset(os.path.join(paths.DATA_PATH_PREPROCESSED,'ms-diag'), data_files = data_files)\n",
    "#df = df.map(preprocess, remove_columns=[\"rid\", \"date\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 873])\n"
     ]
    }
   ],
   "source": [
    "tokens = [tokenizer(get_classification_llama(t)) for t in df[\"train\"][\"text\"]]\n",
    "\n",
    "# Default collate function \n",
    "collate_fn = DataCollatorWithPadding(tokenizer, padding=True) #padding=True, 'max_length'\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset=tokens, collate_fn=collate_fn, batch_size=2, shuffle = False) \n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 1 has a total capacty of 10.75 GiB of which 71.62 MiB is free. Including non-PyTorch memory, this process has 10.68 GiB memory in use. Of the allocated memory 9.20 GiB is allocated by PyTorch, and 671.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m----> 8\u001b[0m     generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m outputs\u001b[38;5;241m.\u001b[39mappend(tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/transformers/generation/utils.py:1764\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1756\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1757\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1758\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1759\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1761\u001b[0m     )\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1772\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1773\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1774\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1779\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1781\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1782\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1788\u001b[0m     )\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/transformers/generation/utils.py:2861\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2858\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2860\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2861\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2862\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2864\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2865\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2866\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2869\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1174\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1171\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1186\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1061\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1051\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1052\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1053\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         use_cache,\n\u001b[1;32m   1059\u001b[0m     )\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1061\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1070\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:803\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    801\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    802\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 803\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    806\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:268\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 268\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/bitsandbytes/nn/modules.py:256\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    253\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    255\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 256\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:577\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul4Bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m/cluster/customapps/biomed/grlab/users/eglimar/conda/envs/inf-extr/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:516\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty(A\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m B_shape[:\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(A, \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdequantize_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mt(), bias)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n\u001b[1;32m    519\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m quant_state\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 1 has a total capacty of 10.75 GiB of which 71.62 MiB is free. Including non-PyTorch memory, this process has 10.68 GiB memory in use. Of the allocated memory 9.20 GiB is allocated by PyTorch, and 671.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "for batch in dataloader:\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "    attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "    with torch.inference_mode():\n",
    "        generated_ids = model.generate(input_ids = input_ids, attention_mask = attention_mask, max_new_tokens=10, num_beams=1, do_sample=True, temperature = 1, num_return_sequences = 1).to(\"cpu\")\n",
    "    outputs.append(tokenizer.batch_decode(generated_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df[\"train\"][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, entry in enumerate(df[\"train\"]):\n",
    "    if entry[\"labels\"] == list(set(df[\"train\"][\"labels\"]))[1]:\n",
    "        print(entry)\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of class labels: \")\n",
    "[len(label) for label in tokenizer(list(set(df[\"train\"][\"labels\"])))[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory : 5.71 GB\n",
      "   Used Memory : 7.33 GB\n",
      "GPU 1: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory : 8.32 GB\n",
      "   Used Memory : 9.91 GB\n"
     ]
    }
   ],
   "source": [
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        for gpu_id in range(num_gpus):\n",
    "            gpu_properties = torch.cuda.get_device_properties(gpu_id)\n",
    "            print(f\"GPU {gpu_id}: {gpu_properties.name}\")\n",
    "            print(f\"   Total Memory: {gpu_properties.total_memory / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Free Memory : {torch.cuda.memory_allocated(gpu_id) / (1024 ** 3):.2f} GB\")\n",
    "            print(f\"   Used Memory : {torch.cuda.memory_reserved(gpu_id) / (1024 ** 3):.2f} GB\")\n",
    "    else:\n",
    "        print(\"No GPU available.\")\n",
    "\n",
    "# Call the function to check GPU memory\n",
    "check_gpu_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
