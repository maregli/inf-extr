{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "from src import paths\n",
    "\n",
    "from src.utils import (load_model_and_tokenizer, \n",
    "                       load_ms_data,  \n",
    "                       check_gpu_memory, \n",
    ")\n",
    "\n",
    "import argparse\n",
    "\n",
    "from transformers import DataCollatorWithPadding, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, AutoModel\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 10.20 GB\n",
      "   Allocated Memory : 0.00 GB\n",
      "   Reserved Memory : 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LLAMA MedTuned 13B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model_and_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_and_tokenizer\u001b[49m(model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlama2-MedTuned-7b\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      2\u001b[0m                                             task_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m                                             quantization \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      5\u001b[0m check_gpu_memory()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_model_and_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"Llama2-MedTuned-13b\"\n",
    "model, tokenizer = load_model_and_tokenizer(model_name = MODEL_NAME,\n",
    "                                            task_type = \"clm\",\n",
    "                                            quantization = \"4bit\")\n",
    "model.config.use_cache = False\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding(labels:list[str], model, tokenizer)->list[torch.Tensor]:\n",
    "    \"\"\"Label encoding of labels\n",
    "    \n",
    "    Args:\n",
    "        labels (list(str)): list of labels\n",
    "        \n",
    "    Returns:\n",
    "        list(torch.Tensor): list of label encodings\n",
    "            \n",
    "            \n",
    "    \"\"\"\n",
    "    encodings = {label:[] for label in labels}\n",
    "\n",
    "    for label in labels:\n",
    "        input = tokenizer(label, return_tensors = \"pt\", add_special_tokens = False)\n",
    "        input.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**input, output_hidden_states = True)\n",
    "        last_hidden_state = outputs[\"hidden_states\"][-1]\n",
    "        last_hidden_state = torch.mean(last_hidden_state, dim = 1)\n",
    "        encodings[label] = last_hidden_state.to(\"cpu\").squeeze()\n",
    "        del outputs\n",
    "        del input\n",
    "\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"primary progressive multiple sclerosis\", \"secondary progressive multiple sclerosis\",\n",
    "          \"relapsing remitting multiple sclerosis\",\"not enough info\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = label_encoding(labels, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_line = load_ms_data(\"line\")\n",
    "df_all = load_ms_data(\"all\")\n",
    "df_first_last = load_ms_data(\"all_first_line_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to make output more consistent by stopping on MS, https://github.com/huggingface/transformers/issues/26959\n",
    "class EosListStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, eos_sequence = [835, 2799, 4080, 29901]):\n",
    "        self.eos_sequence = eos_sequence\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        last_ids = input_ids[:,-len(self.eos_sequence):].tolist()\n",
    "        return self.eos_sequence in last_ids\n",
    "ms_stop = EosListStoppingCriteria(tokenizer(\"multiple sclerosis\", add_special_tokens = False)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(bos_token_id = 1,\n",
    "                                     eos_token_id = 2,\n",
    "                                     pad_token_id = 32000,\n",
    "                                     use_cache = False,\n",
    "                                     temperature=1,\n",
    "                                     top_p=1,\n",
    "                                     do_sample=False,\n",
    "                                     output_hidden_states = True,\n",
    "                                     return_dict_in_generate = True\n",
    "                                    )\n",
    "def single_round_inference(reports:list[str], \n",
    "                           model:AutoModelForCausalLM, \n",
    "                           tokenizer:AutoTokenizer, \n",
    "                           format_fun:Callable[str,str], \n",
    "                           output_hidden_states:bool = True,\n",
    "                          max_new_tokens:int = 20)->pd.DataFrame:\n",
    "    \n",
    "    \"\"\" Single round inference for the MS extraction task\n",
    "    \n",
    "    Args:\n",
    "        reports (list[str]): list of medical reports\n",
    "        model (AutoModelForCausalLM): model\n",
    "        tokenizer (AutoTokenizer): tokenizer\n",
    "        format_fun (Callable[str,str]): function to convert input text to desired prompt format\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: results of inference\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    tokens = [tokenizer(format_fun(t), add_special_tokens = False) for t in reports]\n",
    "    \n",
    "    collate_fn = DataCollatorWithPadding(tokenizer, padding=True) #padding=True, 'max_length'\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset=tokens, collate_fn=collate_fn, batch_size=1, shuffle = False) \n",
    "\n",
    "    generation_config.max_new_tokens = max_new_tokens\n",
    "\n",
    "    if output_hidden_states:    \n",
    "        generation_config.output_hidden_states = True\n",
    "    else:\n",
    "        output_hidden_states = False\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "    whole_prompt = []\n",
    "    last_hidden_states = []\n",
    "    input_lengths = [len(t[\"input_ids\"]) for t in tokens]\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **batch,\n",
    "                generation_config=generation_config,\n",
    "                #stopping_criteria=[ms_stop]\n",
    "            )\n",
    "        if output_hidden_states:\n",
    "            for idx in range(len(outputs.sequences)):\n",
    "                # Find the index of eos_token_id in generated tokens if it exists\n",
    "                eos_index = torch.where(outputs.sequences[idx] == tokenizer.eos_token_id)[0]\n",
    "                # If eos_token_id does not exist in generated tokens, set to -1\n",
    "                eos_index = eos_index[-1] if eos_index.numel() > 0 else -1\n",
    "    \n",
    "                # Extract the last hidden states for all the tokens in the output sequence\n",
    "                # outputs[\"hidden_states\"][:eos_index] is a tuple of tuples of hidden states (one for each layer) for all the generated tokens in the output sequence, it has length of generated sequence\n",
    "                response_last_hidden_states_tuples = [hidden_state[-1][idx,:,:] for hidden_state in outputs[\"hidden_states\"][:eos_index]]\n",
    "                mean_last_hidden_states = torch.mean(torch.cat(response_last_hidden_states_tuples), dim=0)\n",
    "                last_hidden_states.append(mean_last_hidden_states.to(\"cpu\"))\n",
    "        else:\n",
    "            last_hidden_states.append([None] * len(outputs.sequences))\n",
    "\n",
    "\n",
    "        return_tokens = outputs[\"sequences\"].to(\"cpu\")\n",
    "        batch_result = tokenizer.batch_decode(return_tokens, skip_special_tokens=True)\n",
    "        whole_prompt.extend(batch_result)\n",
    "        batch_result = [result.split(\"[/INST]\")[-1].lower().strip() for result in batch_result]\n",
    "\n",
    "        results.extend(batch_result)\n",
    "        del outputs\n",
    "\n",
    "        \n",
    "    return {\"report\": reports, \n",
    "            \"prediction\": results, \n",
    "            \"last_hidden_states\": last_hidden_states, \n",
    "            \"input_lengths\":input_lengths,\n",
    "            \"whole_prompt\": whole_prompt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla\n",
    "\n",
    "Using the original prompt template of meta Llama2 creators. \\<s>[INST]<\\<SYS>>{system_prompt}<\\</SYS>>{instruction}{input}[/INST]\n",
    "You should set add special tokens to false for the tokenizer otherwise you will have double bos in the beginning of the prompt, if you state it. Gives more control.\n",
    "\n",
    "@misc{touvron2023llama,\r\n",
    "      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, \r\n",
    "      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},\r\n",
    "      year={2023},\r\n",
    "      eprint={2307.09288},\r\n",
    "      archivePrefix={arXiv},\r\n",
    "      primaryClarbage.\n",
    "\n",
    "This prompt template builds the foundation to all further strategies, otherwise the model's answers are kinda garbage.\n",
    "\n",
    "Hidden states is of format hidden_states (tuple(tuple(torch.FloatTensor)), optional, returned when output_hidden_states=True is passed or when config.output_hidden_states=True) — Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of torch.FloatTensor of shape (batch_size, generated_length, hidden_size). I will try working with the last hidden state of the first generated token as this is where the model will start it's generation/prediction from..CL}\r\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama-2 chat template\n",
    "\n",
    "def zero_shot_base(report:str)->str:\n",
    "    \"\"\"Zero-shot base for the MS extraction task\n",
    "\n",
    "    Args:\n",
    "        report (str): medical report\n",
    "\n",
    "    Returns:\n",
    "        str: reformatted medical report with base\n",
    "\n",
    "    \"\"\"\n",
    "    base_prompt = \"<s>[INST]<<SYS>>{system_prompt}<</SYS>>\\n\\n{instruction}{input}[/INST]\"\n",
    "    system_prompt =  (\"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \"\n",
    "                      \"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \"\n",
    "                       \"Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make \"\n",
    "                        \"any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t \"\n",
    "                        \"know the answer to a question, please don’t share false information.\\n\"\n",
    "                        )\n",
    "    instruction = (\"Your task is to extract the type of multiple Sclerosis (MS) stated in a German medical report. There are 3 types: \"\n",
    "                    \"primär progrediente Multiple Sklerose (PPMS), sekundär progrediente Multiple Sklerose (SPMS) and schubförmige Multiple Sklerose (RRMS).\"\n",
    "                    \"The type is provided in the text you just have to extract it. If you cannot match a type exactly answer with \\\"not enough info\\\".\"\n",
    "                    \"Your answer should solely consist of either \\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" \"\n",
    "                    \"\\schubförmige Multiple Sklerose (RRMS)\\\", or \\\"not enough info\\\".\"\n",
    "                    \"\\nHere is the medical report:\\n\"\n",
    "                    )\n",
    "    input = base_prompt.format(system_prompt = system_prompt, instruction = instruction, input =  report)\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:13<00:00,  6.93s/it]\n"
     ]
    }
   ],
   "source": [
    "results = single_round_inference(df_line[\"train\"].filter(lambda e: e[\"labels\"] != 3).select(range(2))[\"text\"], model, tokenizer, zero_shot_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction\n",
    "\n",
    "Based on the paper of the creators of Llama2-MedTuned\n",
    "\n",
    "@misc{rohanian2023exploring,\r\n",
    "      title={Exploring the Effectiveness of Instruction Tuning in Biomedical Language Processing}, \r\n",
    "      author={Omid Rohanian and Mohammadmahdi Nouriborji and David A. Clifton},\r\n",
    "      year={2023},\r\n",
    "      eprint={2401.00579},\r\n",
    "      archivePrefix={arXiv},\r\n",
    "      primaryClass={cs\n",
    "\n",
    "Formulating the task as an instruction is closer to the fine-tuning of the model..CL}\r\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_instruction(report:str)->str:\n",
    "    \"\"\"Zero-shot instruction for the MS extraction task\n",
    "    \n",
    "    Args:\n",
    "        report (str): medical report\n",
    "        \n",
    "        Returns:\n",
    "            str: reformatted medical report with instruction\n",
    "            \n",
    "            \"\"\"\n",
    "    # Llama-2 chat template\n",
    "    instruction_base_prompt = \"<s>[INST]\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Output:\\n[/INST]\"\n",
    "    task_instruction = (\"Your task is to extract the type of multiple Sclerosis (MS) stated in a German medical report. There are 3 types: \"\n",
    "                        \"primär progrediente Multiple Sklerose (PPMS), sekundär progrediente Multiple Sklerose (SPMS) and schubförmige Multiple Sklerose (RRMS).\"\n",
    "                        \"The type is provided in the text you just have to extract it. If you cannot match a type exactly answer with \\\"not enough info\\\".\"\n",
    "                        \"Your answer should solely consist of either \\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" \"\n",
    "                        \"\\schubförmige Multiple Sklerose (RRMS)\\\", or \\\"not enough info\\\".\"\n",
    "                        \"Here is the medical report: \"\n",
    "                    )\n",
    "    input = instruction_base_prompt.format(instruction = task_instruction, input =  report)\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:12<00:00,  6.23s/it]\n"
     ]
    }
   ],
   "source": [
    "results = single_round_inference(df_line[\"train\"].filter(lambda e: e[\"labels\"] != 3).select(range(2))[\"text\"], model, tokenizer, zero_shot_instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot\n",
    "\n",
    "Original Paper suggesting this:\n",
    "\n",
    "@misc{brown2020language,\r\n",
    "      title={Language Models are Few-Shot Learners}, \r\n",
    "      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},\r\n",
    "      year={2020},\r\n",
    "      eprint={2005.14165},\r\n",
    "      archivePrefix={arXiv},\r\n",
    "      primaryClass={cs.CL}\r\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama-2 chat template\n",
    "\n",
    "def few_shot_base(report:str)->str:\n",
    "    \"\"\"Few Shot base for the MS extraction task\n",
    "\n",
    "    Args:\n",
    "        report (str): medical report\n",
    "\n",
    "    Returns:\n",
    "        str: reformatted medical report with base\n",
    "\n",
    "    \"\"\"\n",
    "    base_prompt = \"<s>[INST]<<SYS>>{system_prompt}<</SYS>>\\n\\n{instruction}Report:\\n{input}\\nDiagnosis:\\n[/INST]\"\n",
    "\n",
    "    rrms = 'Schubförmig-remittierende Multiple Sklerose, EM 01/2013, ED 10/2015\\nINDENT EDSS 05/2020: 2.0 [...]'\n",
    "    spms = '1. Sekundär progrediente schubförmige Multiple Sklerose [...]'\n",
    "    ppms = '1. Primär progrediente Multiple Sklerose, EM 1992, ED 1996, aktuell EDSS 7.0 [...]'\n",
    "    no_ms = '[...] INDENT MRI 07/2014: Progrediente supratentorielle MS-Plaques mit Befund-Progredienz im Bereich der Radiatio optica beidseits. [...]'\n",
    "\n",
    "    examples = [ppms, spms, rrms, no_ms]\n",
    "\n",
    "    labels = [\"primary progressive multiple sclerosis\", \n",
    "              \"secondary progressive multiple sclerosis\",\n",
    "              \"relapsing remitting multiple sclerosis\",\n",
    "              \"no multiple sclerosis\"]\n",
    "    \n",
    "    system_prompt = (\n",
    "    \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \"\n",
    "    \"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \"\n",
    "    \"Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make \"\n",
    "    \"any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t \"\n",
    "    \"know the answer to a question, please don’t share false information.\\n\"\n",
    "    )\n",
    "\n",
    "    instruction = (\n",
    "       \"Your task is to extract the type of multiple Sclerosis (MS) stated in a German medical report. There are 3 types: \"\n",
    "        \"primär progrediente Multiple Sklerose (PPMS), sekundär progrediente Multiple Sklerose (SPMS) and schubförmige Multiple Sklerose (RRMS).\"\n",
    "        \"The type is provided in the text you just have to extract it. If you cannot match a type exactly answer with \\\"not enough info\\\".\"\n",
    "        \"Your answer should solely consist of either \\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" \"\n",
    "        \"\\schubförmige Multiple Sklerose (RRMS)\\\", or \\\"not enough info\\\".\"\n",
    "        \"To help you with your task, here are a few excerpts from reports that indiciate what output you should produce:\\n\\n\"\n",
    "        )\n",
    "    \n",
    "    for example, label in zip(examples, labels):\n",
    "        instruction += f\"Report:\\n{example}\\nDiagnosis:\\n{label}\\n\\n\"\n",
    "    \n",
    "    input = base_prompt.format(system_prompt = system_prompt, instruction = instruction, input =  report)\n",
    "    input + \"Diagnosis:\\n\"\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:12<00:00,  6.04s/it]\n"
     ]
    }
   ],
   "source": [
    "results = single_round_inference(df_line[\"train\"].filter(lambda e: e[\"labels\"] != 3).select(range(2))[\"text\"], model, tokenizer, few_shot_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_instruct(report:str)->str:\n",
    "    \"\"\"Few Shot base for the MS extraction task\n",
    "\n",
    "    Args:\n",
    "        report (str): medical report\n",
    "\n",
    "    Returns:\n",
    "        str: reformatted medical report with base\n",
    "\n",
    "    \"\"\"\n",
    "    base_prompt = \"<s>[INST]### Instruction:\\n{instruction}### Input:\\n{input}\\n### Output:\\n[/INST]\"\n",
    "\n",
    "    rrms = 'Schubförmig-remittierende Multiple Sklerose, EM 01/2013, ED 10/2015\\nINDENT EDSS 05/2020: 2.0 [...]'\n",
    "    spms = '1. Sekundär progrediente schubförmige Multiple Sklerose [...]'\n",
    "    ppms = '1. Primär progrediente Multiple Sklerose, EM 1992, ED 1996, aktuell EDSS 7.0 [...]'\n",
    "    no_ms = '[...] INDENT MRI 07/2014: Progrediente supratentorielle MS-Plaques mit Befund-Progredienz im Bereich der Radiatio optica beidseits. [...]'\n",
    "\n",
    "    examples = [ppms, spms, rrms, no_ms]\n",
    "\n",
    "    labels = [\"primary progressive multiple sclerosis\", \n",
    "              \"secondary progressive multiple sclerosis\",\n",
    "              \"relapsing remitting multiple sclerosis\",\n",
    "              \"not enough info\"]\n",
    "\n",
    "    instruction = (\n",
    "        \"Your task is to extract the type of multiple Sclerosis (MS) stated in a German medical report. There are 3 types: \"\n",
    "        \"primär progrediente Multiple Sklerose (PPMS), sekundär progrediente Multiple Sklerose (SPMS) and schubförmige Multiple Sklerose (RRMS).\"\n",
    "        \"The type is provided in the text you just have to extract it. If you cannot match a type exactly answer with \\\"not enough info\\\".\"\n",
    "        \"Your answer should solely consist of either \\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" \"\n",
    "        \"\\schubförmige Multiple Sklerose (RRMS)\\\", or \\\"not enough info\\\".\"\n",
    "        \"To help you with your task, here are a few excerpts from reports that indiciate what output you should produce:\\n\\n\"\n",
    "        )\n",
    "    \n",
    "    for example, label in zip(examples, labels):\n",
    "        instruction += f\"### Input:\\n{example}\\n### Output:\\n{label}\\n\\n\"\n",
    "    \n",
    "    input = base_prompt.format(instruction = instruction, input =  report)\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:10<00:00,  5.26s/it]\n"
     ]
    }
   ],
   "source": [
    "results = single_round_inference(df_line[\"train\"].filter(lambda e: e[\"labels\"] != 3).select(range(2))[\"text\"], model, tokenizer, few_shot_instruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_steps_one(report: str)->str:\n",
    "    base_prompt = \"<s>[INST]<<SYS>>{system_prompt}<</SYS>>\\n\\n{instruction}{input}[/INST]\"\n",
    "    system_prompt =  (\"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \"\n",
    "                      \"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \"\n",
    "                       \"Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make \"\n",
    "                        \"any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t \"\n",
    "                        \"know the answer to a question, please don’t share false information.\\n\"\n",
    "                        )\n",
    "    instruction = (\"Your task is to summarize all relevant information pertaining to the multiple sclerosis diagnosis \"\n",
    "                    \"from the provided German medical report. The German word for multiple sclerosis is: \\\"Multiple Sklerose\\\", \"\n",
    "                    \"watch for this keyword and extract all the text around it, especially words before and after. \"\n",
    "                    \"If the report contains no information regarding multiple sclerosis, \"\n",
    "                    \"please respond with \\\"not enough info.\\\" \"\n",
    "                    \"\\nHere is the medical report:\\n\\n\"\n",
    "                   )\n",
    "    input = base_prompt.format(system_prompt = system_prompt, instruction = instruction, input =  report)\n",
    "    return input\n",
    "\n",
    "def two_steps_two(chat_history: str)->str:\n",
    "    base_prompt = \"<s>[INST]\\n\\n{instruction}[/INST]\"\n",
    "    instruction = (\"Given your summary of the medical report, which of the following is the most likely label for this report: \"\n",
    "                  \"\\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\", \"\n",
    "                   \"\\\"schubförmige Multiple Sklerose (RRMS)\\\", or \\\"not enough info\\\". Your answer should have only consist of one of the mentioned labels.\"\n",
    "                   )\n",
    "    if not chat_history.endswith(tokenizer.eos_token):\n",
    "        chat_history += tokenizer.eos_token\n",
    "    input = chat_history + base_prompt.format(instruction = instruction)\n",
    "\n",
    "    return input\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_round_inference(reports:list[str], \n",
    "                           model:AutoModelForCausalLM, \n",
    "                           tokenizer:AutoTokenizer, \n",
    "                           format_fun1:Callable[str,str],\n",
    "                          format_fun2:Callable[str,str],\n",
    "                           output_hidden_states:bool = True,\n",
    "                          max_new_tokens:int = 20)->pd.DataFrame:\n",
    "    \n",
    "    \"\"\"Multi Round inference for the MS extraction task\n",
    "    \n",
    "    Args:\n",
    "        reports (list[str]): list of medical reports\n",
    "        model (AutoModelForCausalLM): model\n",
    "        tokenizer (AutoTokenizer): tokenizer\n",
    "        format_fun1 (Callable[str,str]): function to convert input text to desired prompt format\n",
    "        format_fun2 (Callable[str,str]): function to convert chat history to desired prompt format\n",
    "        output_hidden_states (bool); whether hidden states should be calculated. Defaults to True\n",
    "        max_new_tokens (int): The number of tokens to be generated.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: results of inference\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    output_round1 = single_round_inference(reports, model, tokenizer, format_fun1, output_hidden_states = False, max_new_tokens = 2)\n",
    "    chat_history = output_round1[\"whole_prompt\"]\n",
    "\n",
    "    return single_round_inference(chat_history, model, tokenizer, format_fun2, output_hidden_states = output_hidden_states, max_new_tokens = max_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_line' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m multi_round_inference(\u001b[43mdf_line\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m e: e[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m))[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], model, tokenizer, two_steps_one, two_steps_two, max_new_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_line' is not defined"
     ]
    }
   ],
   "source": [
    "results = multi_round_inference(df_line[\"train\"].filter(lambda e: e[\"labels\"] != 3).select(range(2))[\"text\"], model, tokenizer, two_steps_one, two_steps_two, max_new_tokens = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LLAMA MedTuned 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model_and_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_and_tokenizer\u001b[49m(model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlama2-MedTuned-7b\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      2\u001b[0m                                             task_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m                                             quantization \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      5\u001b[0m check_gpu_memory()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_model_and_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"Llama2-MedTuned-7b\"\n",
    "model, tokenizer = load_model_and_tokenizer(model_name = MODEL_NAME,\n",
    "                                            task_type = \"clm\",\n",
    "                                            quantization = \"4bit\")\n",
    "model.config.use_cache = False\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding(labels:list[str], model, tokenizer)->list[torch.Tensor]:\n",
    "    \"\"\"Label encoding of labels\n",
    "    \n",
    "    Args:\n",
    "        labels (list(str)): list of labels\n",
    "        \n",
    "    Returns:\n",
    "        list(torch.Tensor): list of label encodings\n",
    "            \n",
    "            \n",
    "    \"\"\"\n",
    "    encodings = {label:[] for label in labels}\n",
    "\n",
    "    for label in labels:\n",
    "        input = tokenizer(label, return_tensors = \"pt\", add_special_tokens = False)\n",
    "        input.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**input, output_hidden_states = True)\n",
    "        last_hidden_state = outputs[\"hidden_states\"][-1]\n",
    "        last_hidden_state = torch.mean(last_hidden_state, dim = 1)\n",
    "        encodings[label] = last_hidden_state.to(\"cpu\").squeeze()\n",
    "        del outputs\n",
    "        del input\n",
    "\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"primary progressive multiple sclerosis\", \"secondary progressive multiple sclerosis\",\n",
    "          \"relapsing remitting multiple sclerosis\",\"not enough info\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = label_encoding(labels, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_line = load_ms_data(\"line\")\n",
    "df_all = load_ms_data(\"all\")\n",
    "df_first_last = load_ms_data(\"all_first_line_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to make output more consistent by stopping on MS, https://github.com/huggingface/transformers/issues/26959\n",
    "class EosListStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, eos_sequence = [835, 2799, 4080, 29901]):\n",
    "        self.eos_sequence = eos_sequence\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        last_ids = input_ids[:,-len(self.eos_sequence):].tolist()\n",
    "        return self.eos_sequence in last_ids\n",
    "ms_stop = EosListStoppingCriteria(tokenizer(\"multiple sclerosis\", add_special_tokens = False)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(bos_token_id = 1,\n",
    "                                     eos_token_id = 2,\n",
    "                                     pad_token_id = 32000,\n",
    "                                     use_cache = False,\n",
    "                                     temperature=1,\n",
    "                                     top_p=1,\n",
    "                                     do_sample=False,\n",
    "                                     output_hidden_states = True,\n",
    "                                     return_dict_in_generate = True\n",
    "                                    )\n",
    "def single_round_inference(reports:list[str], \n",
    "                           model:AutoModelForCausalLM, \n",
    "                           tokenizer:AutoTokenizer, \n",
    "                           format_fun:Callable[str,str], \n",
    "                           output_hidden_states:bool = True,\n",
    "                          max_new_tokens:int = 20)->pd.DataFrame:\n",
    "    \n",
    "    \"\"\" Single round inference for the MS extraction task\n",
    "    \n",
    "    Args:\n",
    "        reports (list[str]): list of medical reports\n",
    "        model (AutoModelForCausalLM): model\n",
    "        tokenizer (AutoTokenizer): tokenizer\n",
    "        format_fun (Callable[str,str]): function to convert input text to desired prompt format\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: results of inference\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    tokens = [tokenizer(format_fun(t), add_special_tokens = False) for t in reports]\n",
    "    \n",
    "    collate_fn = DataCollatorWithPadding(tokenizer, padding=True) #padding=True, 'max_length'\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset=tokens, collate_fn=collate_fn, batch_size=1, shuffle = False) \n",
    "\n",
    "    generation_config.max_new_tokens = max_new_tokens\n",
    "\n",
    "    if output_hidden_states:    \n",
    "        generation_config.output_hidden_states = True\n",
    "    else:\n",
    "        output_hidden_states = False\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "    whole_prompt = []\n",
    "    last_hidden_states = []\n",
    "    input_lengths = [len(t[\"input_ids\"]) for t in tokens]\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **batch,\n",
    "                generation_config=generation_config,\n",
    "                #stopping_criteria=[ms_stop]\n",
    "            )\n",
    "        if output_hidden_states:\n",
    "            for idx in range(len(outputs.sequences)):\n",
    "                # Find the index of eos_token_id in generated tokens if it exists\n",
    "                eos_index = torch.where(outputs.sequences[idx] == tokenizer.eos_token_id)[0]\n",
    "                # If eos_token_id does not exist in generated tokens, set to -1\n",
    "                eos_index = eos_index[-1] if eos_index.numel() > 0 else -1\n",
    "    \n",
    "                # Extract the last hidden states for all the tokens in the output sequence\n",
    "                # outputs[\"hidden_states\"][:eos_index] is a tuple of tuples of hidden states (one for each layer) for all the generated tokens in the output sequence, it has length of generated sequence\n",
    "                response_last_hidden_states_tuples = [hidden_state[-1][idx,:,:] for hidden_state in outputs[\"hidden_states\"][:eos_index]]\n",
    "                mean_last_hidden_states = torch.mean(torch.cat(response_last_hidden_states_tuples), dim=0)\n",
    "                last_hidden_states.append(mean_last_hidden_states.to(\"cpu\"))\n",
    "        else:\n",
    "            last_hidden_states.append([None] * len(outputs.sequences))\n",
    "\n",
    "\n",
    "        return_tokens = outputs[\"sequences\"].to(\"cpu\")\n",
    "        batch_result = tokenizer.batch_decode(return_tokens, skip_special_tokens=True)\n",
    "        whole_prompt.extend(batch_result)\n",
    "        batch_result = [result.split(\"[/INST]\")[-1].lower().strip() for result in batch_result]\n",
    "\n",
    "        results.extend(batch_result)\n",
    "        del outputs\n",
    "\n",
    "        \n",
    "    return {\"report\": reports, \n",
    "            \"prediction\": results, \n",
    "            \"last_hidden_states\": last_hidden_states, \n",
    "            \"input_lengths\":input_lengths,\n",
    "            \"whole_prompt\": whole_prompt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla\n",
    "\n",
    "Using the original prompt template of meta Llama2 creators. \\<s>[INST]<\\<SYS>>{system_prompt}<\\</SYS>>{instruction}{input}[/INST]\n",
    "You should set add special tokens to false for the tokenizer otherwise you will have double bos in the beginning of the prompt, if you state it. Gives more control.\n",
    "\n",
    "@misc{touvron2023llama,\r\n",
    "      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, \r\n",
    "      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},\r\n",
    "      year={2023},\r\n",
    "      eprint={2307.09288},\r\n",
    "      archivePrefix={arXiv},\r\n",
    "      primaryClarbage.\n",
    "\n",
    "This prompt template builds the foundation to all further strategies, otherwise the model's answers are kinda garbage.\n",
    "\n",
    "Hidden states is of format hidden_states (tuple(tuple(torch.FloatTensor)), optional, returned when output_hidden_states=True is passed or when config.output_hidden_states=True) — Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of torch.FloatTensor of shape (batch_size, generated_length, hidden_size). I will try working with the last hidden state of the first generated token as this is where the model will start it's generation/prediction from..CL}\r\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama-2 chat template\n",
    "\n",
    "def zero_shot_base(report:str)->str:\n",
    "    \"\"\"Zero-shot base for the MS extraction task\n",
    "\n",
    "    Args:\n",
    "        report (str): medical report\n",
    "\n",
    "    Returns:\n",
    "        str: reformatted medical report with base\n",
    "\n",
    "    \"\"\"\n",
    "    base_prompt = \"<s>[INST]<<SYS>>{system_prompt}<</SYS>>\\n\\n{instruction}{input}[/INST]\"\n",
    "    system_prompt =  (\"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \"\n",
    "                      \"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \"\n",
    "                       \"Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make \"\n",
    "                        \"any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t \"\n",
    "                        \"know the answer to a question, please don’t share false information.\\n\"\n",
    "                        )\n",
    "    instruction = (\"Your task is to extract the type of multiple Sclerosis (MS) stated in a German medical report. There are 3 types: \"\n",
    "                    \"primär progrediente Multiple Sklerose (PPMS), sekundär progrediente Multiple Sklerose (SPMS) and schubförmige Multiple Sklerose (RRMS).\"\n",
    "                    \"The type is provided in the text you just have to extract it. If you cannot match a type exactly answer with \\\"not enough info\\\".\"\n",
    "                    \"Your answer should solely consist of either \\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" \"\n",
    "                    \"\\schubförmige Multiple Sklerose (RRMS)\\\", or \\\"not enough info\\\".\"\n",
    "                    \"\\nHere is the medical report:\\n\"\n",
    "                    )\n",
    "    input = base_prompt.format(system_prompt = system_prompt, instruction = instruction, input =  report)\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:13<00:00,  6.93s/it]\n"
     ]
    }
   ],
   "source": [
    "results = single_round_inference(df_line[\"train\"].filter(lambda e: e[\"labels\"] != 3).select(range(2))[\"text\"], model, tokenizer, zero_shot_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction\n",
    "\n",
    "Based on the paper of the creators of Llama2-MedTuned\n",
    "\n",
    "@misc{rohanian2023exploring,\r\n",
    "      title={Exploring the Effectiveness of Instruction Tuning in Biomedical Language Processing}, \r\n",
    "      author={Omid Rohanian and Mohammadmahdi Nouriborji and David A. Clifton},\r\n",
    "      year={2023},\r\n",
    "      eprint={2401.00579},\r\n",
    "      archivePrefix={arXiv},\r\n",
    "      primaryClass={cs\n",
    "\n",
    "Formulating the task as an instruction is closer to the fine-tuning of the model..CL}\r\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_instruction(report:str)->str:\n",
    "    \"\"\"Zero-shot instruction for the MS extraction task\n",
    "    \n",
    "    Args:\n",
    "        report (str): medical report\n",
    "        \n",
    "        Returns:\n",
    "            str: reformatted medical report with instruction\n",
    "            \n",
    "            \"\"\"\n",
    "    # Llama-2 chat template\n",
    "    instruction_base_prompt = \"<s>[INST]\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Output:\\n[/INST]\"\n",
    "    task_instruction = (\"Your task is to extract the type of multiple Sclerosis (MS) stated in a German medical report. There are 3 types: \"\n",
    "                        \"primär progrediente Multiple Sklerose (PPMS), sekundär progrediente Multiple Sklerose (SPMS) and schubförmige Multiple Sklerose (RRMS).\"\n",
    "                        \"The type is provided in the text you just have to extract it. If you cannot match a type exactly answer with \\\"not enough info\\\".\"\n",
    "                        \"Your answer should solely consist of either \\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" \"\n",
    "                        \"\\schubförmige Multiple Sklerose (RRMS)\\\", or \\\"not enough info\\\".\"\n",
    "                        \"Here is the medical report: \"\n",
    "                    )\n",
    "    input = instruction_base_prompt.format(instruction = task_instruction, input =  report)\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:12<00:00,  6.23s/it]\n"
     ]
    }
   ],
   "source": [
    "results = single_round_inference(df_line[\"train\"].filter(lambda e: e[\"labels\"] != 3).select(range(2))[\"text\"], model, tokenizer, zero_shot_instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot\n",
    "\n",
    "Original Paper suggesting this:\n",
    "\n",
    "@misc{brown2020language,\r\n",
    "      title={Language Models are Few-Shot Learners}, \r\n",
    "      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},\r\n",
    "      year={2020},\r\n",
    "      eprint={2005.14165},\r\n",
    "      archivePrefix={arXiv},\r\n",
    "      primaryClass={cs.CL}\r\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama-2 chat template\n",
    "\n",
    "def few_shot_base(report:str)->str:\n",
    "    \"\"\"Few Shot base for the MS extraction task\n",
    "\n",
    "    Args:\n",
    "        report (str): medical report\n",
    "\n",
    "    Returns:\n",
    "        str: reformatted medical report with base\n",
    "\n",
    "    \"\"\"\n",
    "    base_prompt = \"<s>[INST]<<SYS>>{system_prompt}<</SYS>>\\n\\n{instruction}Report:\\n{input}\\nDiagnosis:\\n[/INST]\"\n",
    "\n",
    "    rrms = 'Schubförmig-remittierende Multiple Sklerose, EM 01/2013, ED 10/2015\\nINDENT EDSS 05/2020: 2.0 [...]'\n",
    "    spms = '1. Sekundär progrediente schubförmige Multiple Sklerose [...]'\n",
    "    ppms = '1. Primär progrediente Multiple Sklerose, EM 1992, ED 1996, aktuell EDSS 7.0 [...]'\n",
    "    no_ms = '[...] INDENT MRI 07/2014: Progrediente supratentorielle MS-Plaques mit Befund-Progredienz im Bereich der Radiatio optica beidseits. [...]'\n",
    "\n",
    "    examples = [ppms, spms, rrms, no_ms]\n",
    "\n",
    "    labels = [\"primary progressive multiple sclerosis\", \n",
    "              \"secondary progressive multiple sclerosis\",\n",
    "              \"relapsing remitting multiple sclerosis\",\n",
    "              \"no multiple sclerosis\"]\n",
    "    \n",
    "    system_prompt = (\n",
    "    \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \"\n",
    "    \"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \"\n",
    "    \"Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make \"\n",
    "    \"any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t \"\n",
    "    \"know the answer to a question, please don’t share false information.\\n\"\n",
    "    )\n",
    "\n",
    "    instruction = (\n",
    "       \"Your task is to extract the type of multiple Sclerosis (MS) stated in a German medical report. There are 3 types: \"\n",
    "        \"primär progrediente Multiple Sklerose (PPMS), sekundär progrediente Multiple Sklerose (SPMS) and schubförmige Multiple Sklerose (RRMS).\"\n",
    "        \"The type is provided in the text you just have to extract it. If you cannot match a type exactly answer with \\\"not enough info\\\".\"\n",
    "        \"Your answer should solely consist of either \\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" \"\n",
    "        \"\\schubförmige Multiple Sklerose (RRMS)\\\", or \\\"not enough info\\\".\"\n",
    "        \"To help you with your task, here are a few excerpts from reports that indiciate what output you should produce:\\n\\n\"\n",
    "        )\n",
    "    \n",
    "    for example, label in zip(examples, labels):\n",
    "        instruction += f\"Report:\\n{example}\\nDiagnosis:\\n{label}\\n\\n\"\n",
    "    \n",
    "    input = base_prompt.format(system_prompt = system_prompt, instruction = instruction, input =  report)\n",
    "    input + \"Diagnosis:\\n\"\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:12<00:00,  6.04s/it]\n"
     ]
    }
   ],
   "source": [
    "results = single_round_inference(df_line[\"train\"].filter(lambda e: e[\"labels\"] != 3).select(range(2))[\"text\"], model, tokenizer, few_shot_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_instruct(report:str)->str:\n",
    "    \"\"\"Few Shot base for the MS extraction task\n",
    "\n",
    "    Args:\n",
    "        report (str): medical report\n",
    "\n",
    "    Returns:\n",
    "        str: reformatted medical report with base\n",
    "\n",
    "    \"\"\"\n",
    "    base_prompt = \"<s>[INST]### Instruction:\\n{instruction}### Input:\\n{input}\\n### Output:\\n[/INST]\"\n",
    "\n",
    "    rrms = 'Schubförmig-remittierende Multiple Sklerose, EM 01/2013, ED 10/2015\\nINDENT EDSS 05/2020: 2.0 [...]'\n",
    "    spms = '1. Sekundär progrediente schubförmige Multiple Sklerose [...]'\n",
    "    ppms = '1. Primär progrediente Multiple Sklerose, EM 1992, ED 1996, aktuell EDSS 7.0 [...]'\n",
    "    no_ms = '[...] INDENT MRI 07/2014: Progrediente supratentorielle MS-Plaques mit Befund-Progredienz im Bereich der Radiatio optica beidseits. [...]'\n",
    "\n",
    "    examples = [ppms, spms, rrms, no_ms]\n",
    "\n",
    "    labels = [\"primary progressive multiple sclerosis\", \n",
    "              \"secondary progressive multiple sclerosis\",\n",
    "              \"relapsing remitting multiple sclerosis\",\n",
    "              \"not enough info\"]\n",
    "\n",
    "    instruction = (\n",
    "        \"Your task is to extract the type of multiple Sclerosis (MS) stated in a German medical report. There are 3 types: \"\n",
    "        \"primär progrediente Multiple Sklerose (PPMS), sekundär progrediente Multiple Sklerose (SPMS) and schubförmige Multiple Sklerose (RRMS).\"\n",
    "        \"The type is provided in the text you just have to extract it. If you cannot match a type exactly answer with \\\"not enough info\\\".\"\n",
    "        \"Your answer should solely consist of either \\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\" \"\n",
    "        \"\\schubförmige Multiple Sklerose (RRMS)\\\", or \\\"not enough info\\\".\"\n",
    "        \"To help you with your task, here are a few excerpts from reports that indiciate what output you should produce:\\n\\n\"\n",
    "        )\n",
    "    \n",
    "    for example, label in zip(examples, labels):\n",
    "        instruction += f\"### Input:\\n{example}\\n### Output:\\n{label}\\n\\n\"\n",
    "    \n",
    "    input = base_prompt.format(instruction = instruction, input =  report)\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:10<00:00,  5.26s/it]\n"
     ]
    }
   ],
   "source": [
    "results = single_round_inference(df_line[\"train\"].filter(lambda e: e[\"labels\"] != 3).select(range(2))[\"text\"], model, tokenizer, few_shot_instruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_steps_one(report: str)->str:\n",
    "    base_prompt = \"<s>[INST]<<SYS>>{system_prompt}<</SYS>>\\n\\n{instruction}{input}[/INST]\"\n",
    "    system_prompt =  (\"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \"\n",
    "                      \"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \"\n",
    "                       \"Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make \"\n",
    "                        \"any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t \"\n",
    "                        \"know the answer to a question, please don’t share false information.\\n\"\n",
    "                        )\n",
    "    instruction = (\"Your task is to summarize all relevant information pertaining to the multiple sclerosis diagnosis \"\n",
    "                    \"from the provided German medical report. The German word for multiple sclerosis is: \\\"Multiple Sklerose\\\", \"\n",
    "                    \"watch for this keyword and extract all the text around it, especially words before and after. \"\n",
    "                    \"If the report contains no information regarding multiple sclerosis, \"\n",
    "                    \"please respond with \\\"not enough info.\\\" \"\n",
    "                    \"\\nHere is the medical report:\\n\\n\"\n",
    "                   )\n",
    "    input = base_prompt.format(system_prompt = system_prompt, instruction = instruction, input =  report)\n",
    "    return input\n",
    "\n",
    "def two_steps_two(chat_history: str)->str:\n",
    "    base_prompt = \"<s>[INST]\\n\\n{instruction}[/INST]\"\n",
    "    instruction = (\"Given your summary of the medical report, which of the following is the most likely label for this report: \"\n",
    "                  \"\\\"primär progrediente Multiple Sklerose (PPMS)\\\", \\\"sekundär progrediente Multiple Sklerose (SPMS)\\\", \"\n",
    "                   \"\\\"schubförmige Multiple Sklerose (RRMS)\\\", or \\\"not enough info\\\". Your answer should have only consist of one of the mentioned labels.\"\n",
    "                   )\n",
    "    if not chat_history.endswith(tokenizer.eos_token):\n",
    "        chat_history += tokenizer.eos_token\n",
    "    input = chat_history + base_prompt.format(instruction = instruction)\n",
    "\n",
    "    return input\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_round_inference(reports:list[str], \n",
    "                           model:AutoModelForCausalLM, \n",
    "                           tokenizer:AutoTokenizer, \n",
    "                           format_fun1:Callable[str,str],\n",
    "                          format_fun2:Callable[str,str],\n",
    "                           output_hidden_states:bool = True,\n",
    "                          max_new_tokens:int = 20)->pd.DataFrame:\n",
    "    \n",
    "    \"\"\"Multi Round inference for the MS extraction task\n",
    "    \n",
    "    Args:\n",
    "        reports (list[str]): list of medical reports\n",
    "        model (AutoModelForCausalLM): model\n",
    "        tokenizer (AutoTokenizer): tokenizer\n",
    "        format_fun1 (Callable[str,str]): function to convert input text to desired prompt format\n",
    "        format_fun2 (Callable[str,str]): function to convert chat history to desired prompt format\n",
    "        output_hidden_states (bool); whether hidden states should be calculated. Defaults to True\n",
    "        max_new_tokens (int): The number of tokens to be generated.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: results of inference\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    output_round1 = single_round_inference(reports, model, tokenizer, format_fun1, output_hidden_states = False, max_new_tokens = 2)\n",
    "    chat_history = output_round1[\"whole_prompt\"]\n",
    "\n",
    "    return single_round_inference(chat_history, model, tokenizer, format_fun2, output_hidden_states = output_hidden_states, max_new_tokens = max_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_line' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m multi_round_inference(\u001b[43mdf_line\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m e: e[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m))[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], model, tokenizer, two_steps_one, two_steps_two, max_new_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_line' is not defined"
     ]
    }
   ],
   "source": [
    "results = multi_round_inference(df_line[\"train\"].filter(lambda e: e[\"labels\"] != 3).select(range(2))[\"text\"], model, tokenizer, two_steps_one, two_steps_two, max_new_tokens = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Leo Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"leo-mistral-hessianai-7b-chat\"\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# For these models running with CPU mem of 10GB not enough, with 30GB it works, maybe try 15GB should be enough to fit the largest shard which is about \n",
    "# 9.5 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"leo-hessianai-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before Model is loaded:\n",
      "\n",
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 10.20 GB\n",
      "   Allocated Memory : 0.00 GB\n",
      "   Reserved Memory : 0.00 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433f61b731364eaa9bb05466f5a3042b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory after Model is loaded:\n",
      "\n",
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 5.69 GB\n",
      "   Allocated Memory : 4.35 GB\n",
      "   Reserved Memory : 4.51 GB\n"
     ]
    }
   ],
   "source": [
    "# Low precision config\n",
    "print(\"Memory before Model is loaded:\\n\")\n",
    "check_gpu_memory()\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(paths.MODEL_PATH/model_name, \n",
    "                                             device_map=\"auto\", \n",
    "                                             quantization_config = bnb_config, \n",
    "                                            # attn_implementation=\"flash_attention_2\"\n",
    "                                            )\n",
    "print(\"Memory after Model is loaded:\\n\")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# For mistral\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    paths.MODEL_PATH/model_name,\n",
    "    padding_side=\"left\",\n",
    "    truncation_side = \"left\",\n",
    "    add_eos_token=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_files = {\"train\": \"ms-diag_clean_train.csv\", \"validation\": \"ms-diag_clean_val.csv\", \"test\": \"ms-diag_clean_test.csv\"}\n",
    "df = load_dataset(os.path.join(paths.DATA_PATH_PREPROCESSED,'ms-diag'), data_files = data_files)\n",
    "#df = df.map(preprocess, remove_columns=[\"rid\", \"date\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c82f5352e19470cbf409fbfa0b498db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7cc18e5b25a45bf995696f22ddcde0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppms_example = df[\"train\"].filter(lambda e: e[\"labels\"] == \"primary_progressive_multiple_sclerosis\")[0]\n",
    "spms_example = df[\"train\"].filter(lambda e: e[\"labels\"] == \"secondary_progressive_multiple_sclerosis\")[0]\n",
    "rrms_example = df[\"train\"].filter(lambda e: e[\"labels\"] == \"relapsing_remitting_multiple_sclerosis\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'V.a. primär progrediente Multiple Sklerose, EM 08/2016, ED 10/2018, EDSS 4.5 INDENT aktuell: klinisch: nicht aktiv, radiologisch: unklar, Progression: ja (nach Lublin 2013) INDENT Verlauf:  INDENT 08/2016: Schwäche und Trauma Fuss links mit Fuss, Trauma mit Bimalleolarluxationsfraktur OSG links, postoperativ progrediente Zunahme der Schwäche des linken Fusses INDENT 02/2019: Zunahme der Schwäche und Steifigkeit des linken Beines, seitdem weiter progredienter Verlauf INDENT 10/2020: leichte Schwäche und Steifigkeitsgefühl des rechten Oberarmes INDENT klinisch: INDENT diagnostisch: laborchemisch: INDENT LP vom 27.11.2018 (Spital Bülach): 1 Zelle/ul, Protein normal, keine Schrankenstörung, OKB positiv Bildgebend: INDENT MR BWS-LWS 10/2018 (Spital Bülach): Mehrere T2w hyperintense Signalalteraltionen im posterioren und lateralen Funiculus rechts und singulär im lateralen Funiculus links des thorakalen Myelons, es ergibt sich der Verdacht auf demyelinisierende Plaques. Keine Schrankenstörung als Zeichen einer akuten Entzündungsaktivität. isolierte Diskushernie BWK 2/BWK 3 ohne Nervenwurzelkompression.  INDENT MR Schädel und HWS 11/2018 (Spital Bülach): Gemäss der McDonald Kriterien (Revision 2017): Bsp. Klinische Attacke und räumliche Dissemination erhärtet der intrazerebrale Befund den Verdacht auf eine MS bei demyelinisierenden Plaques in mindestens 6 MS-typischen Lokalisationen (periventrikulär und juxtakortikal. Weiterer Plaque im zervikalen Myelon auf Höhe von HWK 2 links. Kein Nachweis von aktiven Läsionen.  INDENT MR Schädel 04/2019: stationär, keine aktive Läsionen. INDENT MR HWS/BWS/LWS 04/2019: Bekannte Demyelinisierungsherde im zervikalen und thorakalen Myelon. Kein Nachweis neu aufgetretener Plaques. Nach Kontrastmittelapplikation zeigt der Herd auf Höhe BWK 6 eine flaue Anreicherung als Hinweis auf eine gewisse Aktivität. INDENT MRI Schädel 01/2021: Neue, a.e. demyelinisierende Läsion im Gyrus frontalis medius links mit subcorticalen Beteiligung entlang der U-Fasern. Keine sichere Kontrastmittelaufnahme ebenda. Ansonsten konstante Läsionslast supratentoriell und infratentoriell und im zervikalen Myelon, keine Aktivitätszeichen. INDENT MRI HWS 01/2021: Neue kleine demyelinisierende Läsion auf Höhe HWK 6 im Funiculus lateralis rechts sowie auf Höhe HWK 4 im Funiculus lateralis links. Bekannte MS-Läsionen auf Höhe HWK 2/3 etwas regredient. Keine Zeichen einer entzündlichen Aktivität. Gering progredienter medianer Bandscheibenprolaps auf Höhe BWK 2 / 3 mit geringem Umschlagen nach cranial. INDENT therapeutisch: INDENT Methylprednisolon 100mg für 3 Tage am 25.10.2018 INDENT Lioresal 10mg bis 0.5-0.5-2 (Besserung Gangbild, aufgrund starker Müdigkeit gestoppt) INDENT seit 06/2019 Tecfidera 240mg 1-0-1 Status nach Bimalleolarluxationsfraktur OSG links, 08/2016 INDENT Status nach Platten- und Schraubenosteosynthese 11.08.2016'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppms_example[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Dies ist eine Unterhaltung zwischen einem intelligenten, hilfsbereitem KI-Assistenten und einem Nutzer.\n",
    "Der Assistent gibt ausführliche, hilfreiche und ehrliche Antworten.\"\"\"\n",
    "few_shot_prompt = f\"\"\"<|im_start|>system\n",
    "{system_prompt}<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt1}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{reply1}<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt2}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{reply2}<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt3}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{reply3}<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt4}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "zero_shot_prompt = f\"\"\"\n",
    "<|im_start|>system\n",
    "{system_prompt}<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt1}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "one_shot_prompt = f\"\"\"<|im_start|>system\n",
    "{system_prompt}<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt1}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{reply1}<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt2}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "prompt1 = \"Was ist die MS Diagnose in diesem Text: \" + ppms_example[\"text\"][:80]\n",
    "reply1 = \"Primär progrediente Multiple Sklerose\"\n",
    "prompt2 = \"Was ist die MS Diagnose in diesem Text: \" + spms_example[\"text\"][:80]\n",
    "reply2 = \"Sekundär progrediente Multiple Sklerose\"\n",
    "prompt3 = \"Was ist die MS Diagnose in diesem Text: \" + rrms_example[\"text\"][:80]\n",
    "reply3 = \"Schubförmig remittierende Multiple Sklerose\"\n",
    "prompt4 = \"Was ist die MS Diagnose in diesem Text: \" + df[\"train\"][\"text\"][5][:80]\n",
    "def format_few_shot(system_prompt, prompt1, reply1, prompt2, reply2, prompt3, reply3, prompt4):\n",
    "    input = few_shot_prompt.format(system_prompt,\n",
    "                               prompt1,\n",
    "                              reply1,\n",
    "                              prompt2,\n",
    "                              reply2,\n",
    "                              prompt3,\n",
    "                              reply3,\n",
    "                              prompt4)\n",
    "    return tokenizer(input, return_tensors = \"pt\")\n",
    "\n",
    "def format_one_shot(system_prompt, prompt1, reply1, prompt2):\n",
    "    input = one_shot_prompt.format(system_prompt,\n",
    "                               prompt1,\n",
    "                              reply1,\n",
    "                              prompt2,\n",
    "                              )\n",
    "    print(len(input))\n",
    "    print(input)\n",
    "    return tokenizer(input, return_tensors = \"pt\")\n",
    "\n",
    "def format_zero_shot(system_prompt, prompt1):\n",
    "    input = zero_shot_prompt.format(system_prompt, prompt1)\n",
    "    return tokenizer(input, return_tensors = \"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = tokenizer([\"Primär progrediente Multiple Sklerose\", \"Sekundär progrediente Multiple Sklerose\", \"Schubförmig remittierende Multiple Sklerose\"], add_special_tokens=False)[\"input_ids\"]\n",
    "encoded_bad_words = tokenizer([\"user\"], add_special_tokens = False)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.generation.beam_constraints.DisjunctiveConstraint at 0x1466cec758e0>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DisjunctiveConstraint(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 13, 14]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(input) for input in encoded_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system\\nDies ist eine Unterhaltung zwischen einem intelligenten, hilfsbereitem KI-Assistenten und einem Nutzer.\\nDer Assistent gibt ausführliche, hilfreiche und ehrliche Antworten. \\n user\\nWas ist die MS Diagnose in diesem Text: V.a. primär progrediente Multiple Sklerose, EM 08/2016, ED 10/2018, EDSS 4.5 IND \\n assistant\\nPrimär progrediente Multiple Sklerose \\n user\\nWas ist die MS Diagnose in diesem Text: Multiple Sklerose mit sekundär progredientem Verlauf seit ca. 2004 (EM 1983, ED  \\n assistant\\nSekundär progrediente Multiple Sklerose \\n user\\nWas ist die MS Diagnose in diesem Text: Schubförmige Multiple Sklerose (EM 09/2015, ED 11/2015), EDSS 0,0 Anamnestisch I \\n assistant\\nSchubförmig remittierende Multiple Sklerose \\n user\\nWas ist die MS Diagnose in diesem Text: Primär progrediente Multiple Sklerose, EM ca. 2010, ED 06/2016  INDENT EDSS 07/2 \\n assistant\\n assistant\\nWähle A, B, C oder D als deine Lösung.\\\\n\\\\nEine kürzlich durchgeführte Studie mit 10.000 Personen ergab, dass']\n"
     ]
    }
   ],
   "source": [
    "prompt_encoded = format_few_shot(system_prompt, prompt1, reply1, prompt2, reply2, prompt3, reply3, prompt4)\n",
    "# prompt_encoded = format_zero_shot(system_prompt, prompt1)\n",
    "# prompt_encoded = format_one_shot(system_prompt, prompt1, reply1, prompt2)\n",
    "return_tokens = model.generate(**prompt_encoded, max_new_tokens=50, temperature = 0, bad_words_ids = encoded_bad_words, num_beams = 2)\n",
    "print(tokenizer.batch_decode(return_tokens, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [tokenizer(get_classification_llama(t)) for t in df[\"train\"][\"text\"]]\n",
    "\n",
    "# Default collate function \n",
    "collate_fn = DataCollatorWithPadding(tokenizer, padding=True) #padding=True, 'max_length'\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset=tokens, collate_fn=collate_fn, batch_size=2, shuffle = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems as though reserved memory is extremely high when using beam search. If I have longer input sequences this will lead to out of memory issues. I will try to set number of tokens to a lower number and check if beam search works then. I truncate the text directly because if I truncate after the prompt insertion I will loose the end of the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Consumption before loop\n",
      "\n",
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 5.92 GB\n",
      "   Allocated Memory : 4.26 GB\n",
      "   Reserved Memory : 4.27 GB\n",
      "Memory Consumption before Batch:  0\n",
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 5.92 GB\n",
      "   Allocated Memory : 3.77 GB\n",
      "   Reserved Memory : 4.27 GB\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "print(\"Memory Consumption before loop\\n\")\n",
    "check_gpu_memory()\n",
    "for idx, batch in enumerate(dataloader):\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Memory Consumption before Batch: \", idx)\n",
    "    check_gpu_memory()\n",
    "    \n",
    "    input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "    attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "    with torch.inference_mode():\n",
    "        generated_ids = model.generate(input_ids = input_ids, attention_mask = attention_mask, max_new_tokens=20, num_beams=1, do_sample=True, temperature = 0.9, num_return_sequences = 1, top_p = 0.6).to(\"cpu\")\n",
    "    outputs.append(tokenizer.batch_decode(generated_ids, skip_special_tokens=True))\n",
    "    break\n",
    "    outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 22.73 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 8.04 GB\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "   Total Memory: 31.74 GB\n",
      "   Free Memory: 26.85 GB\n",
      "   Allocated Memory : 3.70 GB\n",
      "   Reserved Memory : 3.91 GB\n"
     ]
    }
   ],
   "source": [
    "check_gpu_memory()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "outputs = list(chain.from_iterable(outputs))\n",
    "pd.Series(outputs).to_csv(paths.RESULTS_PATH/'ms_diag-llama2-chat_zero_shot-shortened300_beam2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [out.split(\"\\nBased on the information provided in the text, the most likely diagnosis for the patient is:\")[1] for out in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'primary_progressive_multiple_sclerosis',\n",
       " 'relapsing_remitting_multiple_sclerosis',\n",
       " 'secondary_progressive_multiple_sclerosis'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df[\"train\"][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to map keywords to labels\n",
    "keyword_label_mapping = {\n",
    "    \"RRMS\": 'relapsing_remitting_multiple_sclerosis',\n",
    "    \"SPMS\": 'secondary_progressive_multiple_sclerosis',\n",
    "    \"PPMS\": 'primary_progressive_multiple_sclerosis',\n",
    "}\n",
    "\n",
    "# Function to assign labels based on text content\n",
    "def assign_label(text):\n",
    "    for keyword, label in keyword_label_mapping.items():\n",
    "        if keyword in text:\n",
    "            return label\n",
    "    return \"unknown\"  # Default label if no keyword is found\n",
    "\n",
    "# Assign labels to each text in the list\n",
    "labels = [assign_label(text) for text in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6016260162601627"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == df[\"train\"][\"labels\"][i]:\n",
    "        correct += 1\n",
    "correct/len(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
