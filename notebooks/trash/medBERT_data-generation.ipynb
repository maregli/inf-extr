{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForMaskedLM, AutoTokenizer\n",
    "import random\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "\n",
    "from src import paths\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"GerMedBERT/medbert-512\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GerMedBERT/medbert-512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_files = {\"train\": \"ms-diag_clean_train.csv\", \"validation\": \"ms-diag_clean_val.csv\", \"test\": \"ms-diag_clean_test.csv\"}\n",
    "df = load_dataset(os.path.join(paths.DATA_PATH_PREPROCESSED,'ms-diag'), data_files = data_files)\n",
    "\n",
    "# Create a masked language model pipeline\n",
    "unmasker = pipeline('fill-mask', model=model, tokenizer=tokenizer, device_map=\"auto\", top_k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_replacement(text:str, unmasker:pipeline, n:int=5, n_words=10)->list():\n",
    "    \"\"\"\n",
    "    Replaces n_words random words in text with [MASK] token. Then fills the [MASK] tokens with the most likely words.\n",
    "    Returns list with of length n with the augmented texts.\n",
    "    \n",
    "    Args:\n",
    "        text (str): text to be masked\n",
    "        unmasker (pipeline): HF pipeline to be used for unmasking. Must take a string as input with a single pipeline.tokenizer.mask_token in it. and return a list of dictionaries with the keys 'token_str' and 'score'.\n",
    "        n (int, optional): number of augmented texts to be returned. Defaults to 5.\n",
    "        n_words (int, optional): number of words to be masked. Defaults to 10.\n",
    "        Returns:\n",
    "            list(str): list of augmented texts\n",
    "    \"\"\"\n",
    "\n",
    "    orig_text_list = text.split()\n",
    "\n",
    "    # Initialize augmented text array\n",
    "    augmented_texts = np.tile(orig_text_list, (n,1))\n",
    "    len_input = len(orig_text_list)\n",
    "\n",
    "    # Initialize new words and their positions\n",
    "    replace_ids = []\n",
    "    new_words = []\n",
    "\n",
    "    # Truncate text to max length of model\n",
    "    text = unmasker.tokenizer.encode(text, truncation=True, add_special_tokens=False)\n",
    "    text = unmasker.tokenizer.decode(text)\n",
    "    \n",
    "    # Get random positions for replacement\n",
    "    for _ in tqdm.tqdm(range(n_words)):\n",
    "        replace_id = random.randint(1, len_input-3)\n",
    "        replace_ids.append(replace_id)\n",
    "\n",
    "        # Mask word\n",
    "        masked_text_list = orig_text_list.copy()\n",
    "        masked_text_list[replace_id] = unmasker.tokenizer.mask_token\n",
    "        masked_text = ' '.join(masked_text_list)\n",
    "\n",
    "        # Get most likely words\n",
    "        results = unmasker(masked_text, top_k=n, tokenizer_kwargs={\"truncation\": True})\n",
    "        new_words.append([result['token_str'] for result in results])\n",
    "\n",
    "    new_words = np.stack(new_words, axis=1)\n",
    "\n",
    "    # Replace words in augmented texts\n",
    "    augmented_texts[:,replace_ids] = new_words\n",
    "\n",
    "    # Join words to sentences\n",
    "    augmented_texts = [' '.join(augmented_text) for augmented_text in augmented_texts]\n",
    "\n",
    "    return augmented_texts\n",
    "        \n",
    "        \n",
    "def random_insertion(text:str, unmasker:pipeline, n:int=5, n_words=10)->list():\n",
    "    \"\"\"\n",
    "    Inserts n_words random words in text. Then fills the inserted words with the most likely words.\n",
    "    Returns list with of length n with the augmented texts.\n",
    "    \n",
    "    Args:\n",
    "        text (str): text to be masked\n",
    "        unmasker (pipeline): HF pipeline to be used for unmasking. Must take a string as input with a single pipeline.tokenizer.mask_token in it. and return a list of dictionaries with the keys 'token_str' and 'score'.\n",
    "        n (int, optional): number of augmented texts to be returned. Defaults to 5.\n",
    "        n_words (int, optional): number of words to be inserted. Defaults to 10.\n",
    "        Returns:\n",
    "            list(str): list of augmented texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Truncate text to max length of model\n",
    "    text = unmasker.tokenizer.encode(text, truncation=True, add_special_tokens=False)\n",
    "    text = unmasker.tokenizer.decode(text)\n",
    "    orig_text_list = text.split()\n",
    "\n",
    "    # Initialize augmented text array\n",
    "    augmented_texts = np.tile(orig_text_list, (n,1))\n",
    "    len_input = len(orig_text_list)\n",
    "\n",
    "    # Initialize new words and their positions\n",
    "    insert_ids = []\n",
    "    new_words = []\n",
    "    \n",
    "    # Get random positions for replacement\n",
    "    for _ in tqdm.tqdm(range(n_words)):\n",
    "        insert_id = random.randint(1, len_input-3) # -3 because CLS and SEP tokens are not counted\n",
    "        insert_ids.append(insert_id)\n",
    "\n",
    "        # Mask word\n",
    "        masked_text_list = orig_text_list.copy()\n",
    "        masked_text_list.insert(insert_id, unmasker.tokenizer.mask_token)\n",
    "        masked_text = ' '.join(masked_text_list)\n",
    "\n",
    "        # Get most likely words\n",
    "        results = unmasker(masked_text, top_k=n, tokenizer_kwargs={\"truncation\": True})\n",
    "        new_words.append([result['token_str'] for result in results])\n",
    "\n",
    "    new_words = np.stack(new_words, axis=1)\n",
    "\n",
    "    # Insert columns in augmented texts\n",
    "    augmented_texts = np.insert(augmented_texts, insert_ids, new_words, axis=1)\n",
    "\n",
    "    # Join words to sentences\n",
    "    augmented_texts = [' '.join(augmented_text) for augmented_text in augmented_texts]\n",
    "\n",
    "    return augmented_texts\n",
    "\n",
    "def augment_text(text:str, unmasker:pipeline, n:int=5, n_replacements = 10, n_insertions = 10)->list():\n",
    "    \"\"\"\n",
    "    Augments text by replacing and inserting words. Then fills the [MASK] tokens with the most likely words.\n",
    "    Returns list with of length n with the augmented texts.\n",
    "    \n",
    "    Args:\n",
    "        text (str): text to be masked\n",
    "        unmasker (pipeline): HF pipeline to be used for unmasking. Must take a string as input with a single pipeline.tokenizer.mask_token in it. and return a list of dictionaries with the keys 'token_str' and 'score'.\n",
    "        n (int, optional): number of augmented texts to be returned. Defaults to 5.\n",
    "        n_replacements (int, optional): number of words to be replaced. Defaults to 10.\n",
    "        n_insertions (int, optional): number of words to be inserted. Defaults to 10.\n",
    "        Returns:\n",
    "            list(str): list of augmented texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Truncate text to max length of model\n",
    "    text = unmasker.tokenizer.encode(text, truncation=True, add_special_tokens=False)\n",
    "    text = unmasker.tokenizer.decode(text)\n",
    "    orig_text_list = text.split()\n",
    "\n",
    "    # Initialize augmented text array\n",
    "    augmented_texts = np.tile(orig_text_list, (n,1))\n",
    "    len_input = len(orig_text_list)\n",
    "\n",
    "    # Initialize new words and their positions\n",
    "    replace_ids = []\n",
    "    insert_ids = []\n",
    "    new_replacements = []\n",
    "    new_insertions = []\n",
    "    \n",
    "    # Replacement\n",
    "    for _ in range(n_replacements):\n",
    "        replace_id = random.randint(1, len_input-3) # -3 because CLS and SEP tokens are not counted\n",
    "        replace_ids.append(replace_id)\n",
    "\n",
    "        # Mask word\n",
    "        masked_text_list = orig_text_list.copy()\n",
    "        masked_text_list[replace_id] = unmasker.tokenizer.mask_token\n",
    "        masked_text = ' '.join(masked_text_list)\n",
    "\n",
    "        # Get most likely words. If error just return original word\n",
    "        try:\n",
    "            results = unmasker(masked_text, top_k=n, tokenizer_kwargs={\"truncation\": True})\n",
    "            new_replacements.append([result['token_str'] for result in results])\n",
    "        except:\n",
    "            results = n*orig_text_list[replace_id]\n",
    "            new_replacements.append(results)\n",
    "\n",
    "\n",
    "    new_replacements = np.stack(new_replacements, axis=1)\n",
    "\n",
    "    # Insertion\n",
    "    for _ in range(n_insertions):\n",
    "        insert_id = random.randint(1, len_input-1)\n",
    "        insert_ids.append(insert_id)\n",
    "\n",
    "        # Mask word\n",
    "        masked_text_list = orig_text_list.copy()\n",
    "        masked_text_list.insert(insert_id, unmasker.tokenizer.mask_token)\n",
    "        masked_text = ' '.join(masked_text_list)\n",
    "\n",
    "        # Get most likely words\n",
    "        try:\n",
    "            results = unmasker(masked_text, top_k=n, tokenizer_kwargs={\"truncation\": True})\n",
    "            new_insertions.append([result['token_str'] for result in results])\n",
    "        except:\n",
    "            results = n*[\"\"]\n",
    "            new_insertions.append(results)\n",
    "        \n",
    "\n",
    "    new_insertions = np.stack(new_insertions, axis=1)\n",
    "\n",
    "    # Replace words in augmented texts\n",
    "    augmented_texts[:,replace_ids] = new_replacements\n",
    "\n",
    "    # Insert columns in augmented texts\n",
    "    augmented_texts = np.insert(augmented_texts, insert_ids, new_insertions, axis=1)\n",
    "\n",
    "    # Join words to sentences\n",
    "    augmented_texts = [' '.join(augmented_text) for augmented_text in augmented_texts]\n",
    "\n",
    "    return augmented_texts\n",
    "\n",
    "def augment_df(df, unmasker, n, n_replacement:int=10, n_insertion:int=5, n_replacements_ratio:float=None, n_inserations_ratio:float=None)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Augments a dataframe by replacing and inserting words in the text column.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe to be augmented\n",
    "        n (int): number of augmented texts to be returned per example. Defaults to 5.\n",
    "        n_replacement (int, optional): number of replacements per text. Defaults to 10.\n",
    "        n_insertion (int, optional): number of insertions per text. Defaults to 5.\n",
    "        n_replacements_ratio (float, optional): ratio of replacements compared to text length per text. Defaults to None. Mutually exclusive with n_replacement.\n",
    "        n_inserations_ratio (float, optional): ratio of insertions compared to text length per text. Defaults to None. Mutually exclusive with n_insertion.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: augmented dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # Augment data\n",
    "    augmented_data = []\n",
    "    labels = []\n",
    "    for i in tqdm.tqdm(range(len(df))):\n",
    "\n",
    "        text = df['text'][i]\n",
    "        label = df['labels'][i]\n",
    "        max_len = min(len(text.split()), unmasker.tokenizer.model_max_length-2)\n",
    "\n",
    "        # Check if ratios are used\n",
    "        if n_replacements_ratio is not None:\n",
    "            n_replacement = int(max(1,max_len*n_replacements_ratio))\n",
    "        if n_inserations_ratio is not None:\n",
    "            n_insertion = int(max(1, max_len*n_inserations_ratio))\n",
    "        \n",
    "        # Replace words\n",
    "        augmented_text = augment_text(text, unmasker, n=n, n_replacements=n_replacement, n_insertions=n_insertion)\n",
    "        augmented_data.extend(augmented_text)\n",
    "        labels.extend([label]*len(augmented_text))\n",
    "\n",
    "    # Create augmented dataframe\n",
    "    augmented_df = pd.DataFrame({\"text\": augmented_data, \"labels\": labels})\n",
    "\n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spms = df[\"train\"].filter(lambda x: x['labels'] == 'secondary_progressive_multiple_sclerosis')\n",
    "df_ppms = df[\"train\"].filter(lambda x: x['labels'] == 'primary_progressive_multiple_sclerosis')\n",
    "df_rrms = df[\"train\"].filter(lambda x: x['labels'] == 'relapsing_remitting_multiple_sclerosis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting With Minority Classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [08:53<00:00, 66.75s/it]\n",
      "100%|██████████| 5/5 [04:12<00:00, 50.45s/it]\n",
      "100%|██████████| 8/8 [08:59<00:00, 67.39s/it]\n",
      "100%|██████████| 5/5 [04:04<00:00, 48.92s/it]\n",
      "100%|██████████| 8/8 [08:39<00:00, 64.99s/it]\n",
      " 60%|██████    | 3/5 [04:02<02:41, 80.92s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting With Minority Classes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39m[df_ppms, df_spms]:\n\u001b[0;32m----> 5\u001b[0m     augmented_df \u001b[38;5;241m=\u001b[39m \u001b[43maugment_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munmasker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_replacements_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_inserations_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     augmented_dfs\u001b[38;5;241m.\u001b[39mappend(augmented_df)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# print(\"Starting With Majority Class\")\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# augmented_dfs.append(augment_df(df_rrms, unmasker, n=1, n_replacements_ratio=0.1, n_inserations_ratio=0.05))\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Concatenate augmented dataframes\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 217\u001b[0m, in \u001b[0;36maugment_df\u001b[0;34m(df, unmasker, n, n_replacement, n_insertion, n_replacements_ratio, n_inserations_ratio)\u001b[0m\n\u001b[1;32m    214\u001b[0m     n_insertion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_len\u001b[38;5;241m*\u001b[39mn_inserations_ratio))\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# Replace words\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m augmented_text \u001b[38;5;241m=\u001b[39m \u001b[43maugment_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munmasker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_replacements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_replacement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_insertions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_insertion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m augmented_data\u001b[38;5;241m.\u001b[39mextend(augmented_text)\n\u001b[1;32m    219\u001b[0m labels\u001b[38;5;241m.\u001b[39mextend([label]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(augmented_text))\n",
      "Cell \u001b[0;32mIn[24], line 170\u001b[0m, in \u001b[0;36maugment_text\u001b[0;34m(text, unmasker, n, n_replacements, n_insertions)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m         results \u001b[38;5;241m=\u001b[39m n\u001b[38;5;241m*\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 170\u001b[0m     new_insertions\u001b[38;5;241m.\u001b[39mappend([result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_str\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results])\n\u001b[1;32m    172\u001b[0m new_insertions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(new_insertions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# Replace words in augmented texts\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 170\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m         results \u001b[38;5;241m=\u001b[39m n\u001b[38;5;241m*\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 170\u001b[0m     new_insertions\u001b[38;5;241m.\u001b[39mappend([\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtoken_str\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results])\n\u001b[1;32m    172\u001b[0m new_insertions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(new_insertions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# Replace words in augmented texts\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "# Augment data for ppms, spms and rrms creating even class sizes\n",
    "augmented_dfs = []\n",
    "print(\"Starting With Minority Classes\")\n",
    "for df in (3*[df_ppms] + 4*[df_spms]):\n",
    "    augmented_df = augment_df(df, unmasker, n=5, n_replacements_ratio=0.2, n_inserations_ratio=0.1)\n",
    "    augmented_dfs.append(augmented_df)\n",
    "\n",
    "# print(\"Starting With Majority Class\")\n",
    "# augmented_dfs.append(augment_df(df_rrms, unmasker, n=1, n_replacements_ratio=0.1, n_inserations_ratio=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate augmented dataframes\n",
    "augmented_df = pd.concat(augmented_dfs)\n",
    "augmented_df[\"date\"] = \"\"\n",
    "augmented_df[\"rid\"] = \"\"\n",
    "\n",
    "# Save augmented dataframe\n",
    "augmented_df.to_csv(os.path.join(paths.DATA_PATH_PREPROCESSED, 'ms-diag', 'ms-diag_augmented.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf-extr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
