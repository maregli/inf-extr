{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7c836af-5638-484f-a431-c2ee674b762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "\n",
    "from src import paths\n",
    "from src import paths\n",
    "from src.utils import (load_model_and_tokenizer, \n",
    "                       check_gpu_memory, \n",
    ")\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from peft import get_peft_config, get_peft_model\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import bitsandbytes as bnb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52200719-12f0-4666-b670-5a7b44713110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   Total Memory: 10.75 GB\n",
      "   Free Memory: 10.18 GB\n",
      "   Allocated Memory : 0.00 GB\n",
      "   Reserved Memory : 0.00 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23dfdb3590b24bf29851f79bbe91d20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer pad token ID: 32000\n",
      "Tokenizer special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}\n",
      "Model pad token ID: 32000\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"Llama2-MedTuned-13b\"\n",
    "# NEW_MODEL_NAME = args.new_model_name\n",
    "QUANTIZATION = \"4bit\"\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-05\n",
    "NUM_EPOCHS = 1\n",
    "PEFT_CONFIG = None\n",
    "ATTN_IMPLEMENTATION = None\n",
    "BF16 = False\n",
    "\n",
    "# Check GPU Memory\n",
    "check_gpu_memory()\n",
    "\n",
    "##########################\n",
    "# Model and Tokenizer\n",
    "##########################\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(model_name=MODEL_NAME, \n",
    "                                            quantization=QUANTIZATION, \n",
    "                                            attn_implementation=ATTN_IMPLEMENTATION,\n",
    "                                            task_type=\"clm\")\n",
    "\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1 # recommended for quantized models I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "550068a4-242e-4d75-b219-e160a3652347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32001, 5120)\n",
      "    (layers): ModuleList(\n",
      "      (0-39): 40 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=5120, out_features=32001, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "614068bc-c9d3-45bf-a983-b9b375a3e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/artidoro/qlora/blob/7f4e95a68dc076bea9b3a413d2b512eca6d004e5/qlora.py#L249\n",
    "def find_all_linear_names(args, model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "878b5cf4-4e8b-477f-980e-5fab288fc982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['down_proj', 'o_proj', 'k_proj', 'up_proj', 'v_proj', 'q_proj', 'gate_proj']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_all_linear_names(\"bla\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd1bc65-d4d2-4170-813c-296ed0cc683d",
   "metadata": {},
   "source": [
    "- https://arxiv.org/abs/2012.14913 talks about number of layers. earlier layers are more for shallow patterns while later layers specialize more in semantic understanding. In paper a model with 16 layers they said layers 1-9 were for shallow detection (so half).\n",
    "    - 40 Decoder Layers; target the first 10 should be enough.\n",
    "- Original Lora Paper (https://arxiv.org/abs/2106.09685). \"Note that putting all the parameters in Wq or Wk results in significantly lower performance, while adapting both Wq and Wv yields the best result (so adapting Key and Query).\n",
    "- QLoRa Paper: \"we find that the most critical LoRA hyperparameter is how many LoRA adapters are used in total and that LoRA on all linear transformer block layers are required to match full finetuning performance. Other LoRA hyperparameters, such as the projection dimension r, do not affect performance\" (https://arxiv.org/pdf/2305.14314.pdf)\n",
    "- As the QLoRa Paper is newer I would go with this one (so target all linear layers) and set the rank lower than I originally did.\n",
    "    - Find all linear layers, and target these\n",
    "    - Set Rank to 4\n",
    "- https://medium.com/@fartypantsham/what-rank-r-and-alpha-to-use-in-lora-in-llm-1b4f025fd133 suggests setting rank = alpha if you don't want to completely change your model. In my case I want to make as little changes as possible. So I try this.\n",
    "\n",
    "Thus config:\n",
    "\n",
    "config = {\n",
    "        \"peft_type\": \"LORA\",\n",
    "        \"r\": 4,\n",
    "        \"lora_alpha\": 4,\n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"bias\":\"none\",\n",
    "        \"task_type\":\"CAUSAL_LM\",\n",
    "        \"target_modules\": ['down_proj', 'o_proj', 'k_proj', 'up_proj', 'v_proj', 'q_proj', 'gate_proj'],\n",
    "        \"layers_to_transform\" : list(range(0, 5)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4e3d3fb-a48b-48eb-badf-cb76cb767fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##########################\n",
    "# Data\n",
    "##########################\n",
    "\n",
    "dataset = Dataset.load_from_disk(os.path.join(paths.DATA_PATH_PREPROCESSED, \"text-finetune/kisim_diagnoses\"))\n",
    "\n",
    "##########################\n",
    "# Specifiations\n",
    "##########################\n",
    "\n",
    "# adapted from: https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = os.path.join(paths.MODEL_PATH, \"results\")\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = NUM_EPOCHS\n",
    "\n",
    "# Enable fp16/bf16 training\n",
    "fp16 = False\n",
    "bf16 = BF16\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = BATCH_SIZE\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = LEARNING_RATE\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 50\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 50\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = 128\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = True\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68c02de5-705f-4275-a967-e116a51db9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32001, 5120)\n",
      "    (layers): ModuleList(\n",
      "      (0-39): 40 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=5120, out_features=32001, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31e9a158-ad14-4344-b2b1-b942a7e675ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"peft_type\": \"LORA\",\n",
    "        \"r\": 4,\n",
    "        \"lora_alpha\": 4,\n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"bias\":\"none\",\n",
    "        \"task_type\":\"CAUSAL_LM\",\n",
    "        \"target_modules\": ['down_proj', 'o_proj', 'k_proj', 'up_proj', 'v_proj', 'q_proj', 'gate_proj'],\n",
    "        \"layers_to_transform\" : list(range(5)),\n",
    "    }\n",
    "\n",
    "PEFT_CONFIG = get_peft_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3c41e7f-cdc4-4641-b0a9-f653a29999bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=4, target_modules={'down_proj', 'o_proj', 'k_proj', 'up_proj', 'v_proj', 'q_proj', 'gate_proj'}, lora_alpha=4, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=[0, 1, 2, 3, 4], layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PEFT_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c43065b-6e51-4904-951e-6fb8f0cde6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, PEFT_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62f98cc2-103c-4353-babf-354005e04825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,867,520 || all params: 13,021,742,080 || trainable%: 0.045059408825274476\n"
     ]
    }
   ],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f1a304b-834d-49f8-8fce-79f34b10e26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32001, 5120)\n",
      "        (layers): ModuleList(\n",
      "          (0-9): 10 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=13824, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "          (10-39): 30 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaSdpaAttention(\n",
      "              (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "              (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "              (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "              (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=5120, out_features=32001, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e82d46-f516-4fc4-ad6f-beb693b360f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################\n",
    "# Training\n",
    "##########################\n",
    "\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant':False},\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=PEFT_CONFIG,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "print(\"Starting Training\")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "print(\"Finished Training\")\n",
    "\n",
    "##########################\n",
    "# Saving\n",
    "##########################\n",
    "\n",
    "if NEW_MODEL_NAME is None:\n",
    "    NEW_MODEL_NAME = MODEL_NAME + \"_finetuned\"\n",
    "\n",
    "print(\"Saving Model at:\", paths.MODEL_PATH/NEW_MODEL_NAME)\n",
    "trainer.save_model(paths.MODEL_PATH/NEW_MODEL_NAME)\n",
    "\n",
    "print(\"Saving Tokenizer at:\", paths.MODEL_PATH/NEW_MODEL_NAME)\n",
    "tokenizer.save_pretrained(paths.MODEL_PATH/NEW_MODEL_NAME)\n",
    "\n",
    "print(\"Saving training logs at:\", paths.MODEL_PATH/NEW_MODEL_NAME/\"log_history.pt\")\n",
    "torch.save(trainer.state.log_history, paths.MODEL_PATH/NEW_MODEL_NAME/\"log_history.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
