{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForMaskedLM, AutoTokenizer\n",
    "import random\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "\n",
    "from src import paths\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"GerMedBERT/medbert-512\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GerMedBERT/medbert-512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_files = {\"train\": \"ms-diag_clean_train.csv\", \"validation\": \"ms-diag_clean_val.csv\", \"test\": \"ms-diag_clean_test.csv\"}\n",
    "df = load_dataset(os.path.join(paths.DATA_PATH_PREPROCESSED,'ms-diag'), data_files = data_files)\n",
    "\n",
    "# Create a masked language model pipeline\n",
    "unmasker = pipeline('fill-mask', model=model, tokenizer=tokenizer, device_map=\"auto\", top_k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_replacement(text:str, unmasker:pipeline, n:int=5, n_words=10)->list():\n",
    "    \"\"\"\n",
    "    Replaces n_words random words in text with [MASK] token. Then fills the [MASK] tokens with the most likely words.\n",
    "    Returns list with of length n with the augmented texts.\n",
    "    \n",
    "    Args:\n",
    "        text (str): text to be masked\n",
    "        unmasker (pipeline): HF pipeline to be used for unmasking. Must take a string as input with a single pipeline.tokenizer.mask_token in it. and return a list of dictionaries with the keys 'token_str' and 'score'.\n",
    "        n (int, optional): number of augmented texts to be returned. Defaults to 5.\n",
    "        n_words (int, optional): number of words to be masked. Defaults to 10.\n",
    "        Returns:\n",
    "            list(str): list of augmented texts\n",
    "    \"\"\"\n",
    "\n",
    "    orig_text_list = text.split()\n",
    "\n",
    "    # Initialize augmented text array\n",
    "    augmented_texts = np.tile(orig_text_list, (n,1))\n",
    "    len_input = len(orig_text_list)\n",
    "\n",
    "    # Initialize new words and their positions\n",
    "    replace_ids = []\n",
    "    new_words = []\n",
    "\n",
    "    # Truncate text to max length of model\n",
    "    text = unmasker.tokenizer.encode(text, truncation=True, add_special_tokens=False)\n",
    "    text = unmasker.tokenizer.decode(text)\n",
    "    \n",
    "    # Get random positions for replacement\n",
    "    for _ in tqdm.tqdm(range(n_words)):\n",
    "        replace_id = random.randint(1, len_input-3)\n",
    "        replace_ids.append(replace_id)\n",
    "\n",
    "        # Mask word\n",
    "        masked_text_list = orig_text_list.copy()\n",
    "        masked_text_list[replace_id] = unmasker.tokenizer.mask_token\n",
    "        masked_text = ' '.join(masked_text_list)\n",
    "\n",
    "        # Get most likely words\n",
    "        results = unmasker(masked_text, top_k=n, tokenizer_kwargs={\"truncation\": True})\n",
    "        new_words.append([result['token_str'] for result in results])\n",
    "\n",
    "    new_words = np.stack(new_words, axis=1)\n",
    "\n",
    "    # Replace words in augmented texts\n",
    "    augmented_texts[:,replace_ids] = new_words\n",
    "\n",
    "    # Join words to sentences\n",
    "    augmented_texts = [' '.join(augmented_text) for augmented_text in augmented_texts]\n",
    "\n",
    "    return augmented_texts\n",
    "        \n",
    "        \n",
    "def random_insertion(text:str, unmasker:pipeline, n:int=5, n_words=10)->list():\n",
    "    \"\"\"\n",
    "    Inserts n_words random words in text. Then fills the inserted words with the most likely words.\n",
    "    Returns list with of length n with the augmented texts.\n",
    "    \n",
    "    Args:\n",
    "        text (str): text to be masked\n",
    "        unmasker (pipeline): HF pipeline to be used for unmasking. Must take a string as input with a single pipeline.tokenizer.mask_token in it. and return a list of dictionaries with the keys 'token_str' and 'score'.\n",
    "        n (int, optional): number of augmented texts to be returned. Defaults to 5.\n",
    "        n_words (int, optional): number of words to be inserted. Defaults to 10.\n",
    "        Returns:\n",
    "            list(str): list of augmented texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Truncate text to max length of model\n",
    "    text = unmasker.tokenizer.encode(text, truncation=True, add_special_tokens=False)\n",
    "    text = unmasker.tokenizer.decode(text)\n",
    "    orig_text_list = text.split()\n",
    "\n",
    "    # Initialize augmented text array\n",
    "    augmented_texts = np.tile(orig_text_list, (n,1))\n",
    "    len_input = len(orig_text_list)\n",
    "\n",
    "    # Initialize new words and their positions\n",
    "    insert_ids = []\n",
    "    new_words = []\n",
    "    \n",
    "    # Get random positions for replacement\n",
    "    for _ in tqdm.tqdm(range(n_words)):\n",
    "        insert_id = random.randint(1, len_input-3) # -3 because CLS and SEP tokens are not counted\n",
    "        insert_ids.append(insert_id)\n",
    "\n",
    "        # Mask word\n",
    "        masked_text_list = orig_text_list.copy()\n",
    "        masked_text_list.insert(insert_id, unmasker.tokenizer.mask_token)\n",
    "        masked_text = ' '.join(masked_text_list)\n",
    "\n",
    "        # Get most likely words\n",
    "        results = unmasker(masked_text, top_k=n, tokenizer_kwargs={\"truncation\": True})\n",
    "        new_words.append([result['token_str'] for result in results])\n",
    "\n",
    "    new_words = np.stack(new_words, axis=1)\n",
    "\n",
    "    # Insert columns in augmented texts\n",
    "    augmented_texts = np.insert(augmented_texts, insert_ids, new_words, axis=1)\n",
    "\n",
    "    # Join words to sentences\n",
    "    augmented_texts = [' '.join(augmented_text) for augmented_text in augmented_texts]\n",
    "\n",
    "    return augmented_texts\n",
    "\n",
    "def augment_text(text:str, unmasker:pipeline, n:int=5, n_replacements = 10, n_insertions = 10)->list():\n",
    "    \"\"\"\n",
    "    Augments text by replacing and inserting words. Then fills the [MASK] tokens with the most likely words.\n",
    "    Returns list with of length n with the augmented texts.\n",
    "    \n",
    "    Args:\n",
    "        text (str): text to be masked\n",
    "        unmasker (pipeline): HF pipeline to be used for unmasking. Must take a string as input with a single pipeline.tokenizer.mask_token in it. and return a list of dictionaries with the keys 'token_str' and 'score'.\n",
    "        n (int, optional): number of augmented texts to be returned. Defaults to 5.\n",
    "        n_replacements (int, optional): number of words to be replaced. Defaults to 10.\n",
    "        n_insertions (int, optional): number of words to be inserted. Defaults to 10.\n",
    "        Returns:\n",
    "            list(str): list of augmented texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Truncate text to max length of model\n",
    "    text = unmasker.tokenizer.encode(text, truncation=True, add_special_tokens=False)\n",
    "    text = unmasker.tokenizer.decode(text)\n",
    "    orig_text_list = text.split()\n",
    "\n",
    "    # Initialize augmented text array\n",
    "    augmented_texts = np.tile(orig_text_list, (n,1))\n",
    "    len_input = len(orig_text_list)\n",
    "\n",
    "    # Initialize new words and their positions\n",
    "    replace_ids = []\n",
    "    insert_ids = []\n",
    "    new_replacements = []\n",
    "    new_insertions = []\n",
    "    \n",
    "    # Replacement\n",
    "    for _ in range(n_replacements):\n",
    "        replace_id = random.randint(1, len_input-3) # -3 because CLS and SEP tokens are not counted\n",
    "        replace_ids.append(replace_id)\n",
    "\n",
    "        # Mask word\n",
    "        masked_text_list = orig_text_list.copy()\n",
    "        masked_text_list[replace_id] = unmasker.tokenizer.mask_token\n",
    "        masked_text = ' '.join(masked_text_list)\n",
    "\n",
    "        # Get most likely words. If error just return original word\n",
    "        try:\n",
    "            results = unmasker(masked_text, top_k=n, tokenizer_kwargs={\"truncation\": True})\n",
    "            new_replacements.append([result['token_str'] for result in results])\n",
    "        except:\n",
    "            results = n*orig_text_list[replace_id]\n",
    "            new_replacements.append(results)\n",
    "\n",
    "\n",
    "    new_replacements = np.stack(new_replacements, axis=1)\n",
    "\n",
    "    # Insertion\n",
    "    for _ in range(n_insertions):\n",
    "        insert_id = random.randint(1, len_input-1)\n",
    "        insert_ids.append(insert_id)\n",
    "\n",
    "        # Mask word\n",
    "        masked_text_list = orig_text_list.copy()\n",
    "        masked_text_list.insert(insert_id, unmasker.tokenizer.mask_token)\n",
    "        masked_text = ' '.join(masked_text_list)\n",
    "\n",
    "        # Get most likely words\n",
    "        try:\n",
    "            results = unmasker(masked_text, top_k=n, tokenizer_kwargs={\"truncation\": True})\n",
    "            new_insertions.append([result['token_str'] for result in results])\n",
    "        except:\n",
    "            results = n*[\"\"]\n",
    "            new_insertions.append(results)\n",
    "        \n",
    "\n",
    "    new_insertions = np.stack(new_insertions, axis=1)\n",
    "\n",
    "    # Replace words in augmented texts\n",
    "    augmented_texts[:,replace_ids] = new_replacements\n",
    "\n",
    "    # Insert columns in augmented texts\n",
    "    augmented_texts = np.insert(augmented_texts, insert_ids, new_insertions, axis=1)\n",
    "\n",
    "    # Join words to sentences\n",
    "    augmented_texts = [' '.join(augmented_text) for augmented_text in augmented_texts]\n",
    "\n",
    "    return augmented_texts\n",
    "\n",
    "def augment_df(df, unmasker, n, n_replacement:int=10, n_insertion:int=5, n_replacements_ratio:float=None, n_inserations_ratio:float=None)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Augments a dataframe by replacing and inserting words in the text column.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe to be augmented\n",
    "        n (int): number of augmented texts to be returned per example. Defaults to 5.\n",
    "        n_replacement (int, optional): number of replacements per text. Defaults to 10.\n",
    "        n_insertion (int, optional): number of insertions per text. Defaults to 5.\n",
    "        n_replacements_ratio (float, optional): ratio of replacements compared to text length per text. Defaults to None. Mutually exclusive with n_replacement.\n",
    "        n_inserations_ratio (float, optional): ratio of insertions compared to text length per text. Defaults to None. Mutually exclusive with n_insertion.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: augmented dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # Augment data\n",
    "    augmented_data = []\n",
    "    labels = []\n",
    "    for i in tqdm.tqdm(range(len(df))):\n",
    "\n",
    "        text = df['text'][i]\n",
    "        label = df['labels'][i]\n",
    "        max_len = min(len(text.split()), unmasker.tokenizer.model_max_length-2)\n",
    "\n",
    "        # Check if ratios are used\n",
    "        if n_replacements_ratio is not None:\n",
    "            n_replacement = int(max(1,max_len*n_replacements_ratio))\n",
    "        if n_inserations_ratio is not None:\n",
    "            n_insertion = int(max(1, max_len*n_inserations_ratio))\n",
    "        \n",
    "        # Replace words\n",
    "        augmented_text = augment_text(text, unmasker, n=n, n_replacements=n_replacement, n_insertions=n_insertion)\n",
    "        augmented_data.extend(augmented_text)\n",
    "        labels.extend([label]*len(augmented_text))\n",
    "\n",
    "    # Create augmented dataframe\n",
    "    augmented_df = pd.DataFrame({\"text\": augmented_data, \"labels\": labels})\n",
    "\n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spms = df[\"train\"].filter(lambda x: x['labels'] == 'secondary_progressive_multiple_sclerosis')\n",
    "df_ppms = df[\"train\"].filter(lambda x: x['labels'] == 'primary_progressive_multiple_sclerosis')\n",
    "df_rrms = df[\"train\"].filter(lambda x: x['labels'] == 'relapsing_remitting_multiple_sclerosis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment data for ppms, spms and rrms creating even class sizes\n",
    "augmented_dfs = []\n",
    "print(\"Starting With Minority Classes\")\n",
    "for df in (3*[df_ppms] + 4*[df_spms]):\n",
    "    augmented_df = augment_df(df, unmasker, n=5, n_replacements_ratio=0.2, n_inserations_ratio=0.1)\n",
    "    augmented_dfs.append(augmented_df)\n",
    "\n",
    "# print(\"Starting With Majority Class\")\n",
    "# augmented_dfs.append(augment_df(df_rrms, unmasker, n=1, n_replacements_ratio=0.1, n_inserations_ratio=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate augmented dataframes\n",
    "augmented_df = pd.concat(augmented_dfs)\n",
    "augmented_df[\"date\"] = \"\"\n",
    "augmented_df[\"rid\"] = \"\"\n",
    "\n",
    "# Save augmented dataframe\n",
    "augmented_df.to_csv(os.path.join(paths.DATA_PATH_PREPROCESSED, 'ms-diag', 'ms-diag_augmented.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf-extr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
