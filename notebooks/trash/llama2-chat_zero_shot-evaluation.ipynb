{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "from src import paths\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, ConfusionMatrixDisplay, confusion_matrix, classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from src.utils import load_ms_data, ms_label2id\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting Strategies 13B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = torch.load(paths.RESULTS_PATH/\"ms-diag\"/\"ms-diag_outlines_Llama2-MedTuned-13b_4bit_all_test_few_shot_vanilla_rag.pt\")\n",
    "base_labels = torch.load(paths.RESULTS_PATH/\"ms-diag\"/\"label_encodings_Llama2-MedTuned-13b.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"model_answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'primär progrediente Multiple Sklerose': 0,\n",
    "              'sekundär progrediente Multiple Sklerose': 2,\n",
    "              'schubförmige remittierende Multiple Sklerose': 1,\n",
    "              'other': 3,\n",
    "              'no information found': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(filename:str, labels:str, load_line:bool = False):\n",
    "    results = torch.load(paths.RESULTS_PATH/\"ms-diag\"/filename, map_location=torch.device('cpu'))\n",
    "    labels = torch.load(paths.RESULTS_PATH/\"ms-diag\"/labels, map_location=torch.device('cpu'))\n",
    "    \n",
    "    last_hidden_states = results.pop(\"last_hidden_states\")\n",
    "    last_hidden_states = last_hidden_states.cpu()\n",
    "    \n",
    "    pred_idx = np.argmax(cosine_similarity(last_hidden_states, labels[1]), axis=1)\n",
    "    results[\"preds\"] = [labels[0][i] for i in pred_idx]\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    if load_line:\n",
    "        return convert_line2report(df, filename)\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "def show_results(results:pd.DataFrame):\n",
    "    contains_any = results[\"prediction\"].str.contains('|'.join(base_labels[0]), case=False)\n",
    "    print(\"Percent of correctly formatted labels: \", sum(contains_any)/len(results))\n",
    "    print(classification_report(y_true = results[\"labels\"], y_pred = results[\"preds\"]))\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true = results[\"labels\"], y_pred = results[\"preds\"], display_labels=ms_label2id, xticks_rotation=\"vertical\")\n",
    "\n",
    "def convert_line2report(results: pd.DataFrame, filename: str):\n",
    "    split = filename.split(\"_\")[4]\n",
    "    df = load_ms_data(\"line\")\n",
    "    \n",
    "    if split == \"all\":\n",
    "        df = concatenate_datasets([df[\"train\"], df[\"val\"], df[\"test\"]])\n",
    "    else:\n",
    "        df = df[split]\n",
    "    \n",
    "    results['index_within_rid'] = df['index_within_rid']\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for i, rid_data in results.groupby(\"rid\"):\n",
    "        _df = rid_data[rid_data[\"index_within_rid\"] == 0].copy()  # Create a copy to avoid chained indexing warnings\n",
    "\n",
    "        rid_data_sorted = rid_data.sort_values('index_within_rid')\n",
    "        _df.loc[:, \"report\"] = \"\\n\".join(rid_data_sorted[\"report\"].tolist())\n",
    "\n",
    "        # There should only be one kind label other than 3 (no MS) or just 3\n",
    "        if rid_data[\"labels\"].value_counts().index[0] == 3 and len(rid_data[\"labels\"].value_counts()) > 1:\n",
    "            _df.loc[:, \"labels\"] = rid_data[\"labels\"].value_counts().index[1]\n",
    "        else:\n",
    "            _df.loc[:, \"labels\"] = rid_data[\"labels\"].value_counts().index[0]\n",
    "\n",
    "        _df.loc[:, \"rid\"] = i\n",
    "        df_list.append(_df)\n",
    "\n",
    "    df_agg = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    return df_agg\n",
    "\n",
    "def summarize_performance(files: list[str]):\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        splitted_filename = file.split(\"_\")\n",
    "        model_name = splitted_filename[1]\n",
    "        level = splitted_filename[3]\n",
    "        split = splitted_filename[4]\n",
    "        strategy = \" \".join(splitted_filename[5:])[:-3]\n",
    "\n",
    "        results = load_results(file)\n",
    "\n",
    "        metric_dict = classification_report(y_true=results[\"labels\"], y_pred=results[\"preds\"], output_dict=True,\n",
    "                                            target_names=target_names)\n",
    "\n",
    "        # Create a dictionary with flattened keys\n",
    "        _df = pd.json_normalize(metric_dict)\n",
    "        \n",
    "\n",
    "        # Add additional information\n",
    "        _df[\"strategy\"] = strategy\n",
    "        _df[\"level\"] = level\n",
    "        _df[\"split\"] = split\n",
    "        contains_any = results[\"prediction\"].str.contains('|'.join(base_labels[0]), case=False)\n",
    "        _df[\"valid_label\"] = sum(contains_any)/len(results)\n",
    "\n",
    "        # Reorder columns\n",
    "        reordered_cols = _df.columns[-4:].append(_df.columns[:-4])\n",
    "        _df = _df[reordered_cols]\n",
    "\n",
    "        dfs.append(_df)\n",
    "\n",
    "    return pd.concat(dfs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "base_labels = torch.load(paths.RESULTS_PATH/\"ms-diag\"/\"label_encodings_Llama2-MedTuned-13b.pt\")\n",
    "base_labels[0], ms_label2id\n",
    "# Reformat base labels such that dim 0 of tensor corresponds to ms_label2id index\n",
    "labels_encoded = base_labels[1][[2,0,1,3],:].cpu()\n",
    "summarize_performance([\"ms-diag_Llama2-MedTuned-13b_4bit_all_all_zero_shot_vanilla.pt\",\n",
    "                      \"ms-diag_Llama2-MedTuned-13b_4bit_all_all_zero_shot_instruction.pt\",\n",
    "                      \"ms-diag_Llama2-MedTuned-13b_4bit_all_all_few_shot_vanilla.pt\",\n",
    "                      \"ms-diag_Llama2-MedTuned-13b_4bit_all_all_few_shot_instruction.pt\",\n",
    "                       \"ms-diag_Llama2-MedTuned-13b_4bit_line_all_zero_shot_vanilla.pt\",\n",
    "                      \"ms-diag_Llama2-MedTuned-13b_4bit_line_all_zero_shot_instruction.pt\",\n",
    "                      \"ms-diag_Llama2-MedTuned-13b_4bit_line_all_few_shot_vanilla.pt\",\n",
    "                      \"ms-diag_Llama2-MedTuned-13b_4bit_line_all_few_shot_instruction.pt\"\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Improvement from zero shot vanilla to few shot vanilla. This helps especially with valid_label output (so model knows better how to structure output)\n",
    "- Improvement from vanilla to instruct (leveraging pretraining)\n",
    "- Combining few shot with instruct has mixed results. Unclear how pretrain objective and few shot interact maybe model is confused because of mixed input format\n",
    "- Zero-Shot Instruct seems to perform best. Is in line with their fine-tune objective\n",
    "- Improvement from all of report to just line. Prompt gets less washed out. Model can focus on relevant input. Especially recall for actual ms classes is really good, indicating that when just presenting relevant info to the model it is quite adept at filtering out. Precision is often less because model has trouble with not enough info (no_ms) recall, it is prone to hallucinations. Because of imbalancedness, just a few falsely classified no_ms classes can have huge impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole report\n",
    "results = load_results(\"ms-diag_Llama2-MedTuned-13b_4bit_all_all_zero_shot_vanilla.pt\")\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line report\n",
    "results = load_results(\"ms-diag_Llama2-MedTuned-13b_4bit_line_all_zero_shot_vanilla.pt\")\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole Report\n",
    "results = load_results(\"ms-diag_Llama2-MedTuned-13b_4bit_all_all_zero_shot_instruction.pt\")\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line Result\n",
    "results = load_results(\"ms-diag_Llama2-MedTuned-13b_4bit_line_all_zero_shot_instruction.pt\")\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results(\"ms-diag_Llama2-MedTuned-13b_4bit_all_all_few_shot_vanilla.pt\")\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results(\"ms-diag_Llama2-MedTuned-13b_4bit_line_all_few_shot_vanilla.pt\")\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results(\"ms-diag_Llama2-MedTuned-13b_4bit_all_all_few_shot_instruction.pt\")\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results(\"ms-diag_Llama2-MedTuned-13b_4bit_line_all_few_shot_instruction.pt\")\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results(\"ms-diag_Llama2-MedTuned-13b_4bit_all_test_two_steps.pt\")\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results(\"ms-diag_Llama2-MedTuned-13b_4bit_line_all_zero_shot_instruction.pt\", load_line = True)\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Prompting Strategies 7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_labels = torch.load(paths.RESULTS_PATH/\"ms-diag\"/\"label_encodings_Llama2-MedTuned-7b.pt\")\n",
    "base_labels[0], ms_label2id\n",
    "# Reformat base labels such that dim 0 of tensor corresponds to ms_label2id index\n",
    "labels_encoded = base_labels[1][[2,0,1,3],:].cpu()\n",
    "summarize_performance([\"ms-diag_Llama2-MedTuned-7b_4bit_all_all_zero_shot_vanilla.pt\",\n",
    "                      \"ms-diag_Llama2-MedTuned-7b_4bit_all_all_zero_shot_instruction.pt\",\n",
    "                      \"ms-diag_Llama2-MedTuned-7b_4bit_all_all_few_shot_vanilla.pt\",\n",
    "                      \"ms-diag_Llama2-MedTuned-7b_4bit_all_all_few_shot_instruction.pt\",\n",
    "                       \"ms-diag_Llama2-MedTuned-7b_4bit_line_all_zero_shot_vanilla.pt\",\n",
    "                      \"ms-diag_Llama2-MedTuned-7b_4bit_line_all_zero_shot_instruction.pt\",\n",
    "                      \"ms-diag_Llama2-MedTuned-7b_4bit_line_all_few_shot_vanilla.pt\",\n",
    "                      \"ms-diag_Llama2-MedTuned-7b_4bit_line_all_few_shot_instruction.pt\"\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Improvement from zero shot vanilla to few shot vanilla to . This helps especially with valid_label output (so model knows better how to structure output)\n",
    "- Improvement from vanilla to instruct (leveraging pretraining)\n",
    "- Few Shot Vanilla and Zero Shot instruct sometimes seem to perform equally well.\n",
    "- Combining few shot with instruct has mixed results. Unclear how pretrain objective and few shot interact maybe model is confused because of mixed input format\n",
    "- Improvement from all of report to just line. Prompt gets less washed out. Model can focus on relevant input. Especially recall for actual ms classes is really good, indicating that when just presenting relevant info to the model it is quite adept at filtering out. Precision is often less because model has trouble with not enough info (no_ms) recall, it is prone to hallucinations. Because of imbalancedness, just a few falsely classified no_ms classes can have huge impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole report\n",
    "results = load_results(\"ms-diag_Llama2-MedTuned-7b_4bit_all_all_zero_shot_vanilla.pt\")\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line report\n",
    "results = load_results(\"ms-diag_Llama2-MedTuned-7b_4bit_line_all_zero_shot_vanilla.pt\")\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results(\"ms-diag_Llama2-MedTuned-7b_4bit_all_all_zero_shot_instruction.pt\")\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line Result\n",
    "results = load_results(\"ms-diag_Llama2-MedTuned-7b_4bit_line_all_zero_shot_instruction.pt\")\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results(\"ms-diag_Llama2-MedTuned-7b_4bit_all_all_few_shot_vanilla.pt\")\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results(\"ms-diag_Llama2-MedTuned-7b_4bit_line_all_few_shot_vanilla.pt\")\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results(\"ms-diag_Llama2-MedTuned-7b_4bit_all_all_few_shot_instruction.pt\")\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results(\"ms-diag_Llama2-MedTuned-7b_4bit_line_all_few_shot_instruction.pt\")\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results(\"ms-diag_Llama2-MedTuned-7b_4bit_all_all_two_steps.pt\")\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = 3592235\n",
    "output = pd.read_csv(paths.RESULTS_PATH/'ms-diag/ms_diag-llama2-chat_zero-shot_generation-strats_3595180.csv')\n",
    "# Map from output to label\n",
    "# Dictionary to map keywords to labels\n",
    "keyword_label_mapping = {\n",
    "    \"rrms\": 'relapsing_remitting_multiple_sclerosis',\n",
    "    \"spms\": 'secondary_progressive_multiple_sclerosis',\n",
    "    \"ppms\": 'primary_progressive_multiple_sclerosis',\n",
    "    \"remittierend\": 'relapsing_remitting_multiple_sclerosis',\n",
    "    \"schubförmig\": 'relapsing_remitting_multiple_sclerosis',\n",
    "    \"sekundär\": 'secondary_progressive_multiple_sclerosis',\n",
    "    \"primär\": 'primary_progressive_multiple_sclerosis',\n",
    "}\n",
    "\n",
    "# Number of columns in the results dataframe\n",
    "n_cols = len(output.columns)\n",
    "\n",
    "# Function to assign labels based on text content\n",
    "def assign_label(text):\n",
    "    for keyword, label in keyword_label_mapping.items():\n",
    "        if keyword in text.lower():\n",
    "            return label\n",
    "    return \"unknown\"  # Default label if no keyword is found\n",
    "\n",
    "# Assign labels to each text in the list\n",
    "for col in output.columns[1:]:\n",
    "    output[f'{col}_r'] = output[col].apply(assign_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count \"unknown\" labels per column\n",
    "for col in output.columns[n_cols:]:\n",
    "    print(f'{col}: {output[col].value_counts()[\"unknown\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(cm:np.ndarray):\n",
    "    \"\"\"Calculate metrics from a confusion matrix. Even if matrix is not square, it will calculate metrics for each class.\"\"\"\n",
    "    num_classes = len(cm)\n",
    "\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_score = []\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        # True Positive, False Positive, False Negative\n",
    "        TP = cm[i, i]\n",
    "        FP = np.sum(cm[:, i]) - TP\n",
    "        FN = np.sum(cm[i, :]) - TP\n",
    "\n",
    "        # Precision, Recall, F1 Score\n",
    "        precision_i = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall_i = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1_score_i = 2 * precision_i * recall_i / (precision_i + recall_i) if (precision_i + recall_i) > 0 else 0\n",
    "\n",
    "        precision.append(precision_i)\n",
    "        recall.append(recall_i)\n",
    "        f1_score.append(f1_score_i)\n",
    "\n",
    "    weighted_precision = np.sum(np.array(precision) * np.sum(cm, axis=1)) / np.sum(cm)\n",
    "    weighted_recall = np.sum(np.array(recall) * np.sum(cm, axis=1)) / np.sum(cm)\n",
    "    weighted_f1_score = np.sum(np.array(f1_score) * np.sum(cm, axis=1)) / np.sum(cm)\n",
    "\n",
    "    macro_precision = np.mean(precision)\n",
    "    macro_recall = np.mean(recall)\n",
    "    macro_f1_score = np.mean(f1_score)\n",
    "\n",
    "    micro_precision = np.sum(cm.diagonal()) / np.sum(cm)\n",
    "    micro_recall = np.sum(cm.diagonal()) / np.sum(cm)\n",
    "    micro_f1_score = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'weighted_precision': weighted_precision,\n",
    "        'weighted_recall': weighted_recall,\n",
    "        'weighted_f1_score': weighted_f1_score,\n",
    "        'macro_precision': macro_precision,\n",
    "        'macro_recall': macro_recall,\n",
    "        'macro_f1_score': macro_f1_score,\n",
    "        'micro_precision': micro_precision,\n",
    "        'micro_recall': micro_recall,\n",
    "        'micro_f1_score': micro_f1_score\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "labels = list(set(output[\"labels\"])) + [\"unknown\"]\n",
    "results = []\n",
    "for col in output.columns[n_cols:]:\n",
    "    print(f'{col}:')\n",
    "    conf_mat = confusion_matrix(y_true=output['labels'], y_pred=output[col], labels=labels)\n",
    "    sns.heatmap(conf_mat, annot=True)\n",
    "    results.append(calculate_metrics(conf_mat))\n",
    "    print()\n",
    "    break\n",
    "\n",
    "pd.DataFrame(results, index=output.columns[n_cols:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['primary_progressive_multiple_sclerosis', 'relapsing_remitting_multiple_sclerosis', 'secondary_progressive_multiple_sclerosis', 'unknown']\n",
    "label_names = ['PPMS', 'RRMS', 'SPMS', 'unknown']\n",
    "results = []\n",
    "\n",
    "# Set the number of columns in the grid\n",
    "grid_cols = 2\n",
    "grid_rows = 6\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(20, 10))\n",
    "\n",
    "for idx, col in enumerate(output.columns[n_cols:]):\n",
    "    row_idx = idx // grid_cols\n",
    "    col_idx = idx % grid_cols\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_mat = confusion_matrix(y_true=output['labels'], y_pred=output[col], labels=labels)\n",
    "\n",
    "    # Plot heatmap in the corresponding subplot\n",
    "    sns.heatmap(conf_mat[:-1,:], ax=axes[row_idx, col_idx], annot=True, fmt='d', cmap='Blues')\n",
    "    axes[row_idx, col_idx].set_title(col)\n",
    "    axes[row_idx, col_idx].set(yticklabels=[])\n",
    "    axes[row_idx, col_idx].set(xticklabels=[])\n",
    "    axes[row_idx, col_idx].tick_params(left=False, bottom=False)\n",
    "\n",
    "    if col_idx == 0:\n",
    "        axes[row_idx, col_idx].set(yticklabels=label_names[:-1])\n",
    "        plt.setp(axes[row_idx, col_idx].get_yticklabels(), rotation=0, horizontalalignment='right')\n",
    "        axes[row_idx, col_idx].collections[0].colorbar.remove()\n",
    "    \n",
    "    if row_idx == grid_rows - 1:\n",
    "        axes[row_idx, col_idx].set(xticklabels=label_names)\n",
    "\n",
    "    \n",
    "    # Calculate metrics and append to the results\n",
    "    results.append(calculate_metrics(conf_mat))\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.add_subplot(111, frameon=False)\n",
    "# hide tick and tick label of the big axis\n",
    "plt.tick_params(labelcolor='none', which='both', top=False, bottom=False, left=False, right=False)\n",
    "plt.xlabel(\"Predicted Class\", labelpad=20, fontsize=16)\n",
    "plt.ylabel(\"True Class\", labelpad=20, fontsize=16)\n",
    "\n",
    "# Display the grid of heatmaps\n",
    "plt.show()\n",
    "\n",
    "# Display the results in a DataFrame\n",
    "results_df = pd.DataFrame(results, index=output.columns[n_cols:]).round(2)\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latex Table\n",
    "num_strategies = 4\n",
    "latex_string_first_row = r\"\\multirow{{4}}{{*}}{{{truncation_size}}} & {{{strategy}}} & {{{precision}}} & {{{recall}}} & {{{f1}}} \\\\\"\n",
    "latex_string_other_rows = r\" & {{{strategy}}} & {{{precision}}} & {{{recall}}} & {{{f1}}} \\\\\"\n",
    "\n",
    "for idx, index in enumerate(results_df.index):\n",
    "    truncation_size = index.split('_')[1]\n",
    "    strategy = index.split('_')[3].capitalize()\n",
    "    if idx %num_strategies == 0:\n",
    "        print(r\"\\midrule\")\n",
    "        print(latex_string_first_row.format(truncation_size=truncation_size, strategy=strategy, precision=results_df.iloc[idx, 0], recall=results_df.iloc[idx, 1], f1=results_df.iloc[idx, 2]))\n",
    "    else:\n",
    "        print(latex_string_other_rows.format(strategy=strategy, precision=results_df.iloc[idx, 0], recall=results_df.iloc[idx, 1], f1=results_df.iloc[idx, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
