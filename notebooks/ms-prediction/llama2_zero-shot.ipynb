{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from datasets import DatasetDict, Features, Sequence, Value, load_dataset\n",
    "\n",
    "import transformers\n",
    "\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "from src import paths\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download model\n",
    "# checkpoint = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# # Save model\n",
    "# model.save_pretrained(paths.MODEL_PATH/'llama2')\n",
    "\n",
    "# # Save tokenizer\n",
    "# tokenizer.save_pretrained(paths.MODEL_PATH/'llama2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a913c94d043d438ab6dc39a51fad23b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(paths.MODEL_PATH/'llama2', padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(paths.MODEL_PATH/'llama2', device_map=\"auto\", load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device of model.embed_tokens.weight:  cuda:0\n",
      "Device of model.layers.0.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.0.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.0.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.0.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.0.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.0.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.0.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.0.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.0.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.1.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.1.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.1.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.1.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.1.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.1.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.1.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.1.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.1.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.2.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.2.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.2.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.2.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.2.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.2.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.2.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.2.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.2.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.3.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.3.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.3.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.3.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.3.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.3.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.3.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.3.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.3.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.4.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.4.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.4.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.4.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.4.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.4.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.4.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.4.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.4.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.5.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.5.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.5.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.5.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.5.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.5.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.5.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.5.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.5.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.6.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.6.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.6.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.6.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.6.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.6.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.6.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.6.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.6.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.7.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.7.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.7.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.7.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.7.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.7.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.7.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.7.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.7.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.8.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.8.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.8.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.8.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.8.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.8.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.8.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.8.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.8.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.9.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.9.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.9.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.9.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.9.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.9.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.9.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.9.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.9.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.10.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.10.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.10.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.10.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.10.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.10.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.10.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.10.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.10.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.11.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.11.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.11.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.11.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.11.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.11.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.11.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.11.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.11.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.12.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.12.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.12.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.12.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.12.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.12.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.12.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.12.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.12.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.13.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.13.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.13.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.13.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.13.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.13.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.13.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.13.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.13.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.14.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.14.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.14.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.14.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.14.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.14.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.14.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.14.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.14.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.15.self_attn.q_proj.weight:  cuda:0\n",
      "Device of model.layers.15.self_attn.k_proj.weight:  cuda:0\n",
      "Device of model.layers.15.self_attn.v_proj.weight:  cuda:0\n",
      "Device of model.layers.15.self_attn.o_proj.weight:  cuda:0\n",
      "Device of model.layers.15.mlp.gate_proj.weight:  cuda:0\n",
      "Device of model.layers.15.mlp.up_proj.weight:  cuda:0\n",
      "Device of model.layers.15.mlp.down_proj.weight:  cuda:0\n",
      "Device of model.layers.15.input_layernorm.weight:  cuda:0\n",
      "Device of model.layers.15.post_attention_layernorm.weight:  cuda:0\n",
      "Device of model.layers.16.self_attn.q_proj.weight:  cuda:1\n",
      "Device of model.layers.16.self_attn.k_proj.weight:  cuda:1\n",
      "Device of model.layers.16.self_attn.v_proj.weight:  cuda:1\n",
      "Device of model.layers.16.self_attn.o_proj.weight:  cuda:1\n",
      "Device of model.layers.16.mlp.gate_proj.weight:  cuda:1\n",
      "Device of model.layers.16.mlp.up_proj.weight:  cuda:1\n",
      "Device of model.layers.16.mlp.down_proj.weight:  cuda:1\n",
      "Device of model.layers.16.input_layernorm.weight:  cuda:1\n",
      "Device of model.layers.16.post_attention_layernorm.weight:  cuda:1\n",
      "Device of model.layers.17.self_attn.q_proj.weight:  cuda:1\n",
      "Device of model.layers.17.self_attn.k_proj.weight:  cuda:1\n",
      "Device of model.layers.17.self_attn.v_proj.weight:  cuda:1\n",
      "Device of model.layers.17.self_attn.o_proj.weight:  cuda:1\n",
      "Device of model.layers.17.mlp.gate_proj.weight:  cuda:1\n",
      "Device of model.layers.17.mlp.up_proj.weight:  cuda:1\n",
      "Device of model.layers.17.mlp.down_proj.weight:  cuda:1\n",
      "Device of model.layers.17.input_layernorm.weight:  cuda:1\n",
      "Device of model.layers.17.post_attention_layernorm.weight:  cuda:1\n",
      "Device of model.layers.18.self_attn.q_proj.weight:  cuda:1\n",
      "Device of model.layers.18.self_attn.k_proj.weight:  cuda:1\n",
      "Device of model.layers.18.self_attn.v_proj.weight:  cuda:1\n",
      "Device of model.layers.18.self_attn.o_proj.weight:  cuda:1\n",
      "Device of model.layers.18.mlp.gate_proj.weight:  cuda:1\n",
      "Device of model.layers.18.mlp.up_proj.weight:  cuda:1\n",
      "Device of model.layers.18.mlp.down_proj.weight:  cuda:1\n",
      "Device of model.layers.18.input_layernorm.weight:  cuda:1\n",
      "Device of model.layers.18.post_attention_layernorm.weight:  cuda:1\n",
      "Device of model.layers.19.self_attn.q_proj.weight:  cuda:1\n",
      "Device of model.layers.19.self_attn.k_proj.weight:  cuda:1\n",
      "Device of model.layers.19.self_attn.v_proj.weight:  cuda:1\n",
      "Device of model.layers.19.self_attn.o_proj.weight:  cuda:1\n",
      "Device of model.layers.19.mlp.gate_proj.weight:  cuda:1\n",
      "Device of model.layers.19.mlp.up_proj.weight:  cuda:1\n",
      "Device of model.layers.19.mlp.down_proj.weight:  cuda:1\n",
      "Device of model.layers.19.input_layernorm.weight:  cuda:1\n",
      "Device of model.layers.19.post_attention_layernorm.weight:  cuda:1\n",
      "Device of model.layers.20.self_attn.q_proj.weight:  cuda:1\n",
      "Device of model.layers.20.self_attn.k_proj.weight:  cuda:1\n",
      "Device of model.layers.20.self_attn.v_proj.weight:  cuda:1\n",
      "Device of model.layers.20.self_attn.o_proj.weight:  cuda:1\n",
      "Device of model.layers.20.mlp.gate_proj.weight:  cuda:1\n",
      "Device of model.layers.20.mlp.up_proj.weight:  cuda:1\n",
      "Device of model.layers.20.mlp.down_proj.weight:  cuda:1\n",
      "Device of model.layers.20.input_layernorm.weight:  cuda:1\n",
      "Device of model.layers.20.post_attention_layernorm.weight:  cuda:1\n",
      "Device of model.layers.21.self_attn.q_proj.weight:  cuda:1\n",
      "Device of model.layers.21.self_attn.k_proj.weight:  cuda:1\n",
      "Device of model.layers.21.self_attn.v_proj.weight:  cuda:1\n",
      "Device of model.layers.21.self_attn.o_proj.weight:  cuda:1\n",
      "Device of model.layers.21.mlp.gate_proj.weight:  cuda:1\n",
      "Device of model.layers.21.mlp.up_proj.weight:  cuda:1\n",
      "Device of model.layers.21.mlp.down_proj.weight:  cuda:1\n",
      "Device of model.layers.21.input_layernorm.weight:  cuda:1\n",
      "Device of model.layers.21.post_attention_layernorm.weight:  cuda:1\n",
      "Device of model.layers.22.self_attn.q_proj.weight:  cuda:1\n",
      "Device of model.layers.22.self_attn.k_proj.weight:  cuda:1\n",
      "Device of model.layers.22.self_attn.v_proj.weight:  cuda:1\n",
      "Device of model.layers.22.self_attn.o_proj.weight:  cuda:1\n",
      "Device of model.layers.22.mlp.gate_proj.weight:  cuda:1\n",
      "Device of model.layers.22.mlp.up_proj.weight:  cuda:1\n",
      "Device of model.layers.22.mlp.down_proj.weight:  cuda:1\n",
      "Device of model.layers.22.input_layernorm.weight:  cuda:1\n",
      "Device of model.layers.22.post_attention_layernorm.weight:  cuda:1\n",
      "Device of model.layers.23.self_attn.q_proj.weight:  cuda:1\n",
      "Device of model.layers.23.self_attn.k_proj.weight:  cuda:1\n",
      "Device of model.layers.23.self_attn.v_proj.weight:  cuda:1\n",
      "Device of model.layers.23.self_attn.o_proj.weight:  cuda:1\n",
      "Device of model.layers.23.mlp.gate_proj.weight:  cuda:1\n",
      "Device of model.layers.23.mlp.up_proj.weight:  cuda:1\n",
      "Device of model.layers.23.mlp.down_proj.weight:  cuda:1\n",
      "Device of model.layers.23.input_layernorm.weight:  cuda:1\n",
      "Device of model.layers.23.post_attention_layernorm.weight:  cuda:1\n",
      "Device of model.layers.24.self_attn.q_proj.weight:  cuda:1\n",
      "Device of model.layers.24.self_attn.k_proj.weight:  cuda:1\n",
      "Device of model.layers.24.self_attn.v_proj.weight:  cuda:1\n",
      "Device of model.layers.24.self_attn.o_proj.weight:  cuda:1\n",
      "Device of model.layers.24.mlp.gate_proj.weight:  cuda:1\n",
      "Device of model.layers.24.mlp.up_proj.weight:  cuda:1\n",
      "Device of model.layers.24.mlp.down_proj.weight:  cuda:1\n",
      "Device of model.layers.24.input_layernorm.weight:  cuda:1\n",
      "Device of model.layers.24.post_attention_layernorm.weight:  cuda:1\n",
      "Device of model.layers.25.self_attn.q_proj.weight:  cuda:1\n",
      "Device of model.layers.25.self_attn.k_proj.weight:  cuda:1\n",
      "Device of model.layers.25.self_attn.v_proj.weight:  cuda:1\n",
      "Device of model.layers.25.self_attn.o_proj.weight:  cuda:1\n",
      "Device of model.layers.25.mlp.gate_proj.weight:  cuda:1\n",
      "Device of model.layers.25.mlp.up_proj.weight:  cuda:1\n",
      "Device of model.layers.25.mlp.down_proj.weight:  cuda:1\n",
      "Device of model.layers.25.input_layernorm.weight:  cuda:1\n",
      "Device of model.layers.25.post_attention_layernorm.weight:  cuda:1\n",
      "Device of model.layers.26.self_attn.q_proj.weight:  cuda:1\n",
      "Device of model.layers.26.self_attn.k_proj.weight:  cuda:1\n",
      "Device of model.layers.26.self_attn.v_proj.weight:  cuda:1\n",
      "Device of model.layers.26.self_attn.o_proj.weight:  cuda:1\n",
      "Device of model.layers.26.mlp.gate_proj.weight:  cuda:1\n",
      "Device of model.layers.26.mlp.up_proj.weight:  cuda:1\n",
      "Device of model.layers.26.mlp.down_proj.weight:  cuda:1\n",
      "Device of model.layers.26.input_layernorm.weight:  cuda:1\n",
      "Device of model.layers.26.post_attention_layernorm.weight:  cuda:1\n",
      "Device of model.layers.27.self_attn.q_proj.weight:  cuda:1\n",
      "Device of model.layers.27.self_attn.k_proj.weight:  cuda:1\n",
      "Device of model.layers.27.self_attn.v_proj.weight:  cuda:1\n",
      "Device of model.layers.27.self_attn.o_proj.weight:  cuda:1\n",
      "Device of model.layers.27.mlp.gate_proj.weight:  cuda:1\n",
      "Device of model.layers.27.mlp.up_proj.weight:  cuda:1\n",
      "Device of model.layers.27.mlp.down_proj.weight:  cuda:1\n",
      "Device of model.layers.27.input_layernorm.weight:  cuda:1\n",
      "Device of model.layers.27.post_attention_layernorm.weight:  cuda:1\n",
      "Device of model.layers.28.self_attn.q_proj.weight:  cuda:1\n",
      "Device of model.layers.28.self_attn.k_proj.weight:  cuda:1\n",
      "Device of model.layers.28.self_attn.v_proj.weight:  cuda:1\n",
      "Device of model.layers.28.self_attn.o_proj.weight:  cuda:1\n",
      "Device of model.layers.28.mlp.gate_proj.weight:  cuda:1\n",
      "Device of model.layers.28.mlp.up_proj.weight:  cuda:1\n",
      "Device of model.layers.28.mlp.down_proj.weight:  cuda:1\n",
      "Device of model.layers.28.input_layernorm.weight:  cuda:1\n",
      "Device of model.layers.28.post_attention_layernorm.weight:  cuda:1\n",
      "Device of model.layers.29.self_attn.q_proj.weight:  cuda:1\n",
      "Device of model.layers.29.self_attn.k_proj.weight:  cuda:1\n",
      "Device of model.layers.29.self_attn.v_proj.weight:  cuda:1\n",
      "Device of model.layers.29.self_attn.o_proj.weight:  cuda:1\n",
      "Device of model.layers.29.mlp.gate_proj.weight:  cuda:1\n",
      "Device of model.layers.29.mlp.up_proj.weight:  cuda:1\n",
      "Device of model.layers.29.mlp.down_proj.weight:  cuda:1\n",
      "Device of model.layers.29.input_layernorm.weight:  cuda:1\n",
      "Device of model.layers.29.post_attention_layernorm.weight:  cuda:1\n",
      "Device of model.layers.30.self_attn.q_proj.weight:  cuda:1\n",
      "Device of model.layers.30.self_attn.k_proj.weight:  cuda:1\n",
      "Device of model.layers.30.self_attn.v_proj.weight:  cuda:1\n",
      "Device of model.layers.30.self_attn.o_proj.weight:  cuda:1\n",
      "Device of model.layers.30.mlp.gate_proj.weight:  cuda:1\n",
      "Device of model.layers.30.mlp.up_proj.weight:  cuda:1\n",
      "Device of model.layers.30.mlp.down_proj.weight:  cuda:1\n",
      "Device of model.layers.30.input_layernorm.weight:  cuda:1\n",
      "Device of model.layers.30.post_attention_layernorm.weight:  cuda:1\n",
      "Device of model.layers.31.self_attn.q_proj.weight:  cuda:1\n",
      "Device of model.layers.31.self_attn.k_proj.weight:  cuda:1\n",
      "Device of model.layers.31.self_attn.v_proj.weight:  cuda:1\n",
      "Device of model.layers.31.self_attn.o_proj.weight:  cuda:1\n",
      "Device of model.layers.31.mlp.gate_proj.weight:  cuda:1\n",
      "Device of model.layers.31.mlp.up_proj.weight:  cuda:1\n",
      "Device of model.layers.31.mlp.down_proj.weight:  cuda:1\n",
      "Device of model.layers.31.input_layernorm.weight:  cuda:1\n",
      "Device of model.layers.31.post_attention_layernorm.weight:  cuda:1\n",
      "Device of model.norm.weight:  cuda:1\n",
      "Device of lm_head.weight:  cuda:1\n"
     ]
    }
   ],
   "source": [
    "# Check device allocation\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Device of {name}: \", param.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_files = {\"train\": \"ms-diag_clean_train.csv\", \"validation\": \"ms-diag_clean_val.csv\", \"test\": \"ms-diag_clean_test.csv\"}\n",
    "df = load_dataset(os.path.join(paths.DATA_PATH_PREPROCESSED,'ms-diag'), data_files = data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2efc36b70b445ddb2073d140d051c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935ded16fe8e491c861bd53195c1d913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585c1110a481420c8f3b0c13d06fb352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[\"###Classify the following text as 'primary_progressive_multiple_sclerosis', 'secondary_progressive_multiple_sclerosis', or'relapsing_remitting_multiple_sclerosis'.###\\nText: \\n\\nErstmanifestation einer schubförmig-remittierenden Multiple Sklerose\\r\\n··anamnestisch: vor 2 Wochen Beginn mit Übelkeit und Erbrechen, langsam aufgetretener Schwindel ohne Richtungskomponente, Verstärkung bei körperlicher und psychischer Anstrengung, im Verlauf zusätzlich beinbetonte Schwäche des gesamten Körpers, Kribbelparästhesien am Rumpf, Gesichtsfeldeinfschränkung und Visusminderung des linken Auges, Extremitätenataxie bds., Babinski bds positiv\\r\\n·klinisch: Visusminderung linkes Auge, fingerperimetrisch eingeschränktes Gesichtsfeld des linken Auges insbesondere temporal, verlangsamte direkte und konsensuelle Lichtreaktion links, skandierende Sprache, distalbetonte Hemiparese links ohne faziale Beteiligung, Dysdiadochokinese bds und Extremitätenataxie, Reflexsprung C6-C7, Muskeleigenreflexe an den Beinen linksbetont gesteigert, fehlende Bauchhautreflexe, bds Babinski positiv,  Gangataxie\\r\\n·diagnostisch: \\r\\n··cMRI vom 23.05.16: Neuritis nervi optici links. Multiple Läsionen periventrikulär, paraventrikulär, juxtakortikal gelegen, z.T supra- infratentoriell und teils abgebildet spinal\\r\\n··MRI WS vom 24.05.16: zahlreiche T2-hyperintense Läsionen mit KM-Anreicherung und teilweise ödematöser Auftreibung zwischen HWK 1 bis BWK 12\\r\\n·LP vom 23.05.16: ZZ 34/ul, Protein 0.571 g/l, Laktat 1.5 mmol/l, IgG-Index 0.77, OKB im Liquor positiv\\r\\n·Serologie im Liquor und Serum Borrelien negativ\\r\\n·Vaskulitis-Screening (IgG, IgA, IgM Serum, Kompl Faktor C3c, C4, RF, ANA, anti-ds-DNA, ANCA, MPO-ANCA, PR3-ANCA) negativ, HIV negativ, s-IL2-Rezeptor normwertig\\r\\n·Virologie CMV, EBV, HSV, VZV, Masern, Röteln Serologie ausstehend\\r\\n·ophthalmologisches Konsil vom 24.05.16: ausgeprägte linksseitige Gesichtsfeldeinschränkung mit restlicher sichelförmiger Insel \\r\\n·VEP vom 26.05.16: beidseits und deutlich linksbetonte pathologische visuell evozierte Potentiale.\\r\\n··therapeutisch: Methylprednisolon 1g/d i.v. für insgesamt 5 Tage (bis zum 28.05.16)\\r\\n\\n Classification: \\n\\nPrimary_progressive_multiple_sclerosis\\nSecondary_progressive_\"]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "data_files = {\"train\": \"ms-diag_clean_train.csv\", \"validation\": \"ms-diag_clean_val.csv\", \"test\": \"ms-diag_clean_test.csv\"}\n",
    "df = load_dataset(os.path.join(paths.DATA_PATH_PREPROCESSED,'ms-diag'), data_files = data_files)\n",
    "# Prompt\n",
    "prompt = \"\"\"###Classify the following text as 'primary_progressive_multiple_sclerosis', 'secondary_progressive_multiple_sclerosis', or'relapsing_remitting_multiple_sclerosis'.###\n",
    "Text: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Prompt construction function\n",
    "def construct_prompt(text):\n",
    "    return prompt + text + '\\n Classification: '\n",
    "\n",
    "def preprocess(examples):\n",
    "    examples['text'] = construct_prompt(examples['text'])\n",
    "    return examples\n",
    "df = df.map(preprocess)\n",
    "model_inputs = tokenizer(df[\"train\"][\"text\"][0], return_tensors=\"pt\").to(\"cuda\")\n",
    "model_inputs = {k: v.to(torch.int64).to(\"cuda\") for k, v in model_inputs.items()}\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=20, num_beams=4, do_sample=True, num_return_sequences = 1)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of class labels: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[12, 14, 12]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Length of class labels: \")\n",
    "[len(label) for label in tokenizer(list(set(df[\"train\"][\"labels\"])))[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7943032105a64de7a917ae7ad36db247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9f75298a1a45c2aa7692268ce8b1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd2d5f56a8f4e678b218e6c810c1109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize dataset\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, return_tensors = \"pt\", padding = True, max_length = 1024)\n",
    "\n",
    "df = df.map(tokenize, batched=True, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m test_sample \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m:df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m4\u001b[39m],\n\u001b[1;32m      2\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m:df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m4\u001b[39m]}\n\u001b[0;32m----> 3\u001b[0m test_sample \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m test_sample\u001b[38;5;241m.\u001b[39mitems()}\n",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m test_sample \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m:df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m4\u001b[39m],\n\u001b[1;32m      2\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m:df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m4\u001b[39m]}\n\u001b[0;32m----> 3\u001b[0m test_sample \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(torch\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m test_sample\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "test_sample = {\"input_ids\":df[\"train\"][\"input_ids\"][:4],\n",
    "                \"attention_mask\":df[\"train\"][\"attention_mask\"][:4]}\n",
    "test_sample = {k: v.to(torch.int32).to(\"cuda\") for k, v in test_sample.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[173, 183]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess as sp\n",
    "import os\n",
    "\n",
    "def get_gpu_memory():\n",
    "    command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    return memory_free_values\n",
    "\n",
    "get_gpu_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
