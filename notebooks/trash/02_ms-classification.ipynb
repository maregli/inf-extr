{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS Prediction Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd()+\"/../..\")\n",
    "from src import paths\n",
    "\n",
    "from dateutil import parser\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnoses labels\n",
    "\n",
    "Found in `seantis/diagnoses.csv` in the `disease` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(paths.DATA_PATH_SEANTIS/'diagnoses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(labels.head())\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Label distribution:\")\n",
    "print(labels['disease'].value_counts())\n",
    "print(\"None: \", labels['disease'].isna().sum(), \"\\n\")\n",
    "print(\"Unique patient IDs: \", len(labels['research_id'].unique()))\n",
    "print(\"Duplicate patient IDs: \", len(labels['research_id']) - len(labels['research_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Diagnosis reliability:\")\n",
    "print(labels['diagnosis_reliability'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print missing dates\n",
    "print(\"Missing dates:\")\n",
    "print(labels[\"diagnosis_date\"].isna().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Date column formats\n",
    "print(\"Date column formats:\")\n",
    "print(labels[\"diagnosis_date\"].apply(lambda x: len(x)).value_counts())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print a 10 character date\n",
    "print(\"Example date:\")\n",
    "print(labels[\"diagnosis_date\"][labels[\"diagnosis_date\"].apply(lambda x: len(x)) == 10].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guessing the date format from the length and example. Seems to follow english format with YYYY-MM-DD.\n",
    "date_map = {\n",
    "    7: \"YYYY-MM\",\n",
    "    10: \"YYYY-MM-DD\",\n",
    "    4: \"YYYY\",\n",
    "}\n",
    "print(labels[\"diagnosis_date\"].apply(lambda x: len(x))\n",
    "      .value_counts()\n",
    "      .reset_index()\n",
    "      .replace({\"diagnosis_date\": date_map})\n",
    "      .rename(columns={\"diagnosis_date\": \"format\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the entries for the patients with multiple labels\n",
    "print(\"Examples Patients with multiple labels:\")\n",
    "display((labels[labels[\"research_id\"].duplicated(keep=False)].sort_values(by=[\"research_id\", \"disease_onset_date\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems\n",
    "\n",
    "- Mix of german and english labels.\n",
    "- Irrelevant labels\n",
    "- Class imbalance\n",
    "- There could be multiple diagnoses per patient\n",
    "- The date column contains multiple formats, YYYY-MM, YYYY, YYYY-MM-DD\n",
    "- Some diagnoses are not confirmed, could be detrimental to model training. In old approach they excluded these.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- In the old project they mention, that only english labels are reliable. And they only use confirmed diagnoses for training. Manually check if the german labels are accurate.\n",
    "- Exclude irrelevant labels (like clinically_isolated_syndrome). Keep relevant labels (relapsing_remitting_multiple_sclerosis (RRMS), secondary_progressive_multiple_sclerosis (SPMS), primary_progressive_multiple_sclerosis (PPMS))\n",
    "- Do dataset-splits stratified\n",
    "- Match with text from `kisim_diagnosis` based on research_id, check again how much remains.\n",
    "- Multiple diagnoses per patient are reasonable, but problem is automatically solved by excluding non-confirmed diagnoses as all of the RRMS diagnoses were made status_post which shouldn't be info in the text.\n",
    "- Clean date column by using YYYY-MM-DD format, mapping YYYY to the YYYY-01-01 and YYYY-MM to YYYY-MM-01\n",
    "- Exclude non-confirmed diagnoses like in old approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosis text\n",
    "\n",
    "The diagnosis text is in `kisim_diagnoses.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv(paths.DATA_PATH_SEANTIS/'kisim_diagnoses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Description of text dataset: \\n\")\n",
    "print(text.describe().iloc[:2], \"\\n\")\n",
    "\n",
    "print(\"Shape of text dataset: \\n\")\n",
    "print(text.shape, \"\\n\")\n",
    "\n",
    "print(\"None values: \\n\")\n",
    "print(text.isna().sum(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique patient IDs\n",
    "print(\"Unique patient IDs: \", len(text['research_id'].unique()))\n",
    "\n",
    "# Is there at least one date per research_id?\n",
    "print(\"There is at least one date per rid: \", text.groupby(\"research_id\")[\"diagnosis_date\"].apply(lambda x: x.notna().sum().all()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date column formats\n",
    "print(\"Date column formats:\")\n",
    "print(text[\"diagnosis_date\"].dropna().apply(lambda x: len(x)).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example date:\")\n",
    "print(text[\"diagnosis_date\"].dropna().iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems\n",
    "- There are multiple text entries per rid (diagnosis_label column). This is because doctors mostly just appended to existing reports, meaning the longest one should be the newest, or fullest report. \n",
    "- The LastUpdateDate is not useful for the dates, as different rows, that correspond to the same entries, share this date\n",
    "- The diagnosis_date column has a lot of missing values. Because of missing values matching is hard to match using date.\n",
    "- The date column is formated in the YYYY-MM-DD hh:mm:ss.ms format, could reformat\n",
    "\n",
    "### Conclusions\n",
    "- Multiple entries per rid, I will use the longest. This was already done for the old project. The entries are stored in /midatams/preprocessed_nlp/midata-text-extraction/data/diagnoses/diags_seantis_kisim_longest.json\n",
    "- Date column might not be used for analysis but might prove useful for time-series follow-up. After join, this column should be cleaned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Datasets\n",
    "\n",
    "We can join the datasets based on the rid. For rids with multiple entries we will use the date column to match them exactly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlap of rid between labels and text\n",
    "print(\"Unique patient IDs in labels: \", len(labels['research_id'].unique()))\n",
    "print(\"Unique patient IDs in text: \", len(text['research_id'].unique()))\n",
    "print(\"Overlap of research_id between labels and text: \", len(set(labels[\"research_id\"]).intersection(set(text[\"research_id\"]))))\n",
    "\n",
    "# Double labels\n",
    "# Join labels and text on rid\n",
    "double_labeled = labels[\"research_id\"][labels[\"research_id\"].duplicated(keep=\"first\")]\n",
    "print(\"Number of double labeled patients in both datasets: \", len(set(double_labeled).intersection(set(text[\"research_id\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing steps\n",
    "Steps: \n",
    "1. Merge, rename, drop non-confirmed diagnoses and irrelevant columns\n",
    "2. German labels could be unreliable. Check manually, then relabel\n",
    "3. Drop all entries that are not one of RRMS, PPMS or SPMS. \n",
    "4. primary progressive and secondary progressive have low counts. Very important that text matches these. Check manually.\n",
    "5. Try to mine more SPMS or PPMS by checking entries that might have been wrongly labelled as RRMS and remap them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Merge, rename, drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data. Text data is loaded from json file using the same data as in old approach\n",
    "import json\n",
    "labels = pd.read_csv(paths.DATA_PATH_SEANTIS/'diagnoses.csv').rename(columns={'research_id': 'rid'})\n",
    "text = json.load(open(paths.DATA_PATH_PREPROCESSED/'midatams/diags_seantis_kisim_longest.json', 'r'))\n",
    "text = {k.split('_')[0]: ' '.join(v) for k, v in text.items()} # File name was rid_something_date\n",
    "text = pd.DataFrame.from_dict(text, orient='index').reset_index().rename(columns={0: 'text', 'index': 'rid'})\n",
    "\n",
    "# Merge on rid, drop irrelevant columns, rename columns\n",
    "df = pd.merge(labels, text, on='rid', how='inner')\n",
    "\n",
    "# Only use confirmed diagnosis\n",
    "df = df[df[\"diagnosis_reliability\"] == \"confirmed\"]\n",
    "\n",
    "# Remove and rename columns\n",
    "df = df[[\"rid\", \"diagnosis_date\", \"disease\", \"text\"]].rename(columns={\"diagnosis_date\": \"date\", \"disease\": \"labels\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. German labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check special cases where I rewrite the label\n",
    "map_dict = {\n",
    "    \"Schubförmig remittierende Multiple Sklerose (RRMS)\": \"relapsing_remitting_multiple_sclerosis\",\n",
    "    \"Schubförmig remittierende Multiple Sklerose\": \"relapsing_remitting_multiple_sclerosis\",\n",
    "    \"Schubförmig remittierende Multiple Sklerose \": \"relapsing_remitting_multiple_sclerosis\",\n",
    "    \"Multiple Sklerose mit V.a. sekundär chronisch-progredienten Verlauf\": \"secondary_progressive_multiple_sclerosis\",\n",
    "    \"Multiple Sklerose a.e. primär progredient\": \"primary_progressive_multiple_sclerosis\",\n",
    "    \"Multiple Sklerose mit a.e. primär-progredientem Verlauf\": \"primary_progressive_multiple_sclerosis\",\n",
    "}\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "df[df[\"labels\"].isin(map_dict.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seems text entry of 252 does not contain enough information to be useful The rest of the entries are fine and can be remapped\n",
    "df.drop(252, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Remapping, drop non interesting labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remap labels\n",
    "df = df.replace(map_dict)\n",
    "\n",
    "# Only use confirmed diagnosis and the relevant labels\n",
    "labels_list = [\"relapsing_remitting_multiple_sclerosis\", \"secondary_progressive_multiple_sclerosis\", \"primary_progressive_multiple_sclerosis\"]\n",
    "df = df[df[\"labels\"].isin(labels_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. SPMS and PPMS manual check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because mapping was done manually, check if label matches text for classes with low counts\n",
    "display(df[df[\"labels\"] == \"secondary_progressive_multiple_sclerosis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For entries (47 (RRMS), 210 (unclear), 211 (RRMS), 218 (unclear)) text is not consistent with label. \n",
    "# I will remap entries 47 and 211 to RRMS and drop the other two entries.\n",
    "df.drop([210, 218], inplace=True)\n",
    "df.loc[df.index == 47, \"labels\"] = \"relapsing_remitting_multiple_sclerosis\"\n",
    "df.loc[df.index == 211, \"labels\"] = \"relapsing_remitting_multiple_sclerosis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check primary_progressive_multiple_sclerosis\n",
    "display(df[df[\"labels\"] == \"primary_progressive_multiple_sclerosis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything seems fine here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Manual Mining of SPMS and PPMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find entries that contain \"SPMS\" or sekundär and have label relapsing_remitting_multiple_sclerosis\n",
    "df[(df[\"labels\"] == \"relapsing_remitting_multiple_sclerosis\") & (df[\"text\"].str.contains(\"SPMS|sekundär\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are entries where a RRMS is diagnosed, but SPMS is suspected. Keep in mind when looking at results. I won't remap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find entries that contain \"PPMS\" or primär and have label relapsing_remitting_multiple_sclerosis\n",
    "df[(df[\"labels\"] == \"relapsing_remitting_multiple_sclerosis\") & (df[\"text\"].str.contains(\"PPMS|primär\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Date reformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat date column\n",
    "df[\"date\"] = df[\"date\"].apply(lambda x: parser.parse(x, default=datetime.datetime(2023, 1, 1)).strftime(\"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of cleaned dataset\n",
    "print(\"Summary of cleaned dataset: \\n\")\n",
    "print(df.info(), \"\\n\")\n",
    "print(\"shape: \", df.shape, \"\\n\")\n",
    "print(\"label distribution: \\n\", df[\"labels\"].value_counts(), \"\\n\")\n",
    "print(\"missing: \\n\", df.isna().sum(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "train, test = train_test_split(df, test_size=0.3, random_state=42, stratify=df[\"labels\"])\n",
    "train, val = train_test_split(train, test_size=0.1, random_state=42, stratify=train[\"labels\"])\n",
    "\n",
    "# Save to csv\n",
    "train.to_csv(paths.DATA_PATH_PREPROCESSED/'ms-diag/ms-diag_clean_train.csv', index=False)\n",
    "val.to_csv(paths.DATA_PATH_PREPROCESSED/'ms-diag/ms-diag_clean_val.csv', index=False)\n",
    "test.to_csv(paths.DATA_PATH_PREPROCESSED/'ms-diag/ms-diag_clean_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train label distribution: \\n\", train[\"labels\"].value_counts(), \"\\n\")\n",
    "print(\"Val label distribution: \\n\", val[\"labels\"].value_counts(), \"\\n\")\n",
    "print(\"Test label distribution: \\n\", test[\"labels\"].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Project Preprocessing\n",
    "To get comparability to old project I will use parts of their data preprocessing. Specifically I use the seantis_kisim.csv that they have prepared. Their approach:\n",
    "\n",
    "1. Extract the longest diagnosis per rid (most lines) from the csv and if the rid had a manually line labelled text, they used this instead.\n",
    "2. Results in dataset consisting of text lines per row with a label for the line.\n",
    "\n",
    "Further processing:\n",
    "3. I can use this to create a df with joined text per rid and determined what content will be in the text. Later I can filter useful examples based on this.\n",
    "4. If a content entry is False, we know that there is no such information in the text. This is useful to estimate the accuracy of a model to correctly determine the lack of information. If the entry is NaN we don't know if the information is present.\n",
    "5. As there are very few entries for SPMS and PPMS I will manually check their lines for dm content. If I make any changes to line labellings I will report this in the \"line_label_origin\" column as \"manual\", while original labels are stored as \"original\". Later I also add a third option for \"classifier1\" where a Model automatically labels the lines.\n",
    "6. As there are very few entries for SPMS and PPMS I will manually check their labels for correctness. If I change any of their labels I will remark this in the \"label_origin\" column as \"manual\", while original labels will be stored as \"original\". Later I also add a third option for \"classifier2\" where a Model automatically labels the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(paths.DATA_PATH_PREPROCESSED,\"line_labelling\", \"seantis_kisim.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if rid text contains a certain content\n",
    "contains_variable_dict = {line_class: f\"contains_{line_class}\" for line_class in df[\"class\"].unique()}\n",
    "\n",
    "# Create columns\n",
    "df_clean = pd.DataFrame(columns=[\"rid\", \"text\"] + list(contains_variable_dict.values()) + [\"line_label_origin\"])\n",
    "line_labelled_reports = 0\n",
    "\n",
    "# Fill df_clean\n",
    "for rid, rid_data in df.groupby(\"research_id\"):\n",
    "\n",
    "    # Concatenate text lines\n",
    "    text = rid_data[\"text\"].dropna().str.cat(sep=\"\\n\")\n",
    "\n",
    "     # If all line classes are nan fill with nan\n",
    "    if rid_data[\"class\"].isna().all():\n",
    "        _df = pd.DataFrame({\"rid\": rid, \"text\": text, **{k: np.nan for k in contains_variable_dict.values()}, \"line_label_origin\": np.nan}, index=[0])\n",
    "        df_clean = pd.concat([df_clean, _df], ignore_index=True)\n",
    "    \n",
    "    # If value is contained fill with True, else False\n",
    "    else:\n",
    "        text_contains = rid_data[\"class\"].unique()\n",
    "        text_contains_dict = {v: False for k, v in contains_variable_dict.items()}\n",
    "        text_contains_dict.update({contains_variable_dict[v]: True for v in text_contains})\n",
    "        _df = pd.DataFrame({\"rid\": rid, \"text\": text, **text_contains_dict, \"line_label_origin\": \"original\"}, index=[0])\n",
    "        df_clean = pd.concat([df_clean, _df], ignore_index=True)\n",
    "        line_labelled_reports += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of line labelled reports: \", line_labelled_reports)\n",
    "print(\"Number of reports without line labels: \", len(df_clean) - line_labelled_reports)\n",
    "print(\"Number of reports in df_clean: \", len(df_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data. Text data is loaded from json file using the same data as in old approach\n",
    "labels = pd.read_csv(paths.DATA_PATH_SEANTIS/'diagnoses.csv').rename(columns={'research_id': 'rid'}).drop(columns=[\"diagnosis_date\", \"disease_onset_date\"])\n",
    "\n",
    "# Add label_origin and fill with \"original\"\n",
    "labels[\"label_origin\"] = \"original\"\n",
    "\n",
    "# Merge on rid, drop irrelevant columns, rename columns\n",
    "df_merged = pd.merge(labels, df_clean, on='rid', how='inner')\n",
    "\n",
    "# Only use confirmed diagnosis\n",
    "df_merged = df_merged[df_merged[\"diagnosis_reliability\"] == \"confirmed\"].drop(columns=[\"diagnosis_reliability\"])\n",
    "\n",
    "# Remove and rename columns\n",
    "df_merged = df_merged.rename(columns={\"disease\": \"labels\"})\n",
    "\n",
    "# Check special cases where I rewrite the label\n",
    "map_dict = {\n",
    "    \"Schubförmig remittierende Multiple Sklerose (RRMS)\": \"relapsing_remitting_multiple_sclerosis\",\n",
    "    \"Schubförmig remittierende Multiple Sklerose\": \"relapsing_remitting_multiple_sclerosis\",\n",
    "    \"Schubförmig remittierende Multiple Sklerose \": \"relapsing_remitting_multiple_sclerosis\",\n",
    "    \"Multiple Sklerose mit V.a. sekundär chronisch-progredienten Verlauf\": \"secondary_progressive_multiple_sclerosis\",\n",
    "    \"Multiple Sklerose a.e. primär progredient\": \"primary_progressive_multiple_sclerosis\",\n",
    "    \"Multiple Sklerose mit a.e. primär-progredientem Verlauf\": \"primary_progressive_multiple_sclerosis\",\n",
    "}\n",
    "\n",
    "# Remap labels\n",
    "df_merged = df_merged.replace(map_dict)\n",
    "\n",
    "# Only use confirmed diagnosis and the relevant labels\n",
    "labels_list = [\"relapsing_remitting_multiple_sclerosis\", \"secondary_progressive_multiple_sclerosis\", \"primary_progressive_multiple_sclerosis\"]\n",
    "df_merged = df_merged[df_merged[\"labels\"].isin(labels_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[df_merged.contains_dm == True].labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[df_merged.labels == \"secondary_progressive_multiple_sclerosis\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For PPMS and SPMS there are not enough line labels. I will check myself for ms and set to true if there is MS in report and False elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[df_merged.labels == \"secondary_progressive_multiple_sclerosis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From inspection it seems entries 210 and 218 do not contain dm, the rest does. Also entries 47 and 211 are relapsing_remitting_multiple_sclerosis and contain dm.\n",
    "# Set contains_dm to True for all secondary_progressive_multiple_sclerosis entries\n",
    "df_merged.loc[df_merged.labels == \"secondary_progressive_multiple_sclerosis\", \"contains_dm\"] = True\n",
    "df_merged.loc[df_merged.labels == \"secondary_progressive_multiple_sclerosis\", \"line_label_origin\"] = \"manual\"\n",
    "df_merged.loc[df_merged.index == 210, \"contains_dm\"] = False\n",
    "df_merged.loc[df_merged.index == 218, \"contains_dm\"] = False\n",
    "\n",
    "# Set label of entries 47, 50 and 211 to relapsing_remitting_multiple_sclerosis\n",
    "df_merged.loc[df_merged.index == 47, \"labels\"] = \"relapsing_remitting_multiple_sclerosis\"\n",
    "df_merged.loc[df_merged.index == 47, \"label_origin\"] = \"manual\"\n",
    "df_merged.loc[df_merged.index == 211, \"labels\"] = \"relapsing_remitting_multiple_sclerosis\"\n",
    "df_merged.loc[df_merged.index == 211, \"label_origin\"] = \"manual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check primary_progressive_multiple_sclerosis\n",
    "df_merged[df_merged.labels == \"primary_progressive_multiple_sclerosis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems everything is fine with PPMS all contain dm\n",
    "df_merged.loc[df_merged.labels == \"primary_progressive_multiple_sclerosis\", \"contains_dm\"] = True\n",
    "df_merged.loc[df_merged.labels == \"primary_progressive_multiple_sclerosis\", \"line_label_origin\"] = \"manual\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution\n",
    "print(\"Label distribution all:\")\n",
    "print(df_merged.labels.value_counts(), \"\\n\\n\")\n",
    "\n",
    "# Texts that contain diagnosis label\n",
    "print(\"Label distribution contains_dm:\")\n",
    "print(df_merged[df_merged.contains_dm == True].labels.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "df_merged.to_csv(paths.DATA_PATH_PREPROCESSED/'ms-diag/ms-diag_line_annotated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[df_merged.contains_dm == False]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
